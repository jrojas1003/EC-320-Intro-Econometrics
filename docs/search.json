[
  {
    "objectID": "lectures/index.html",
    "href": "lectures/index.html",
    "title": "Lectures",
    "section": "",
    "text": "Expand\n\n\n\n\n\n\n\n  \n\n\n   Expand\n\n\n\n\n\n  \n\n\n   Expand"
  },
  {
    "objectID": "lectures/index.html#theory",
    "href": "lectures/index.html#theory",
    "title": "Lectures",
    "section": "",
    "text": "Expand\n\n\n\n\n\n\n\n  \n\n\n   Expand\n\n\n\n\n\n  \n\n\n   Expand"
  },
  {
    "objectID": "lectures/04-Estimators-02/041-ols-properties.html",
    "href": "lectures/04-Estimators-02/041-ols-properties.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "There are three important OLS properties\n\n\nThe point \\((\\bar{x}, \\bar{y})\\) is always on the regression line\n\n\n\nResiduals sum to zero: \\(\\sum_{i}^{n} \\hat{u}_{i} = 0\\)\n\n\n\nThe sample covariance between the independent variable and the residuals is zero: \\(\\sum_{i}^{n} x_{i}\\hat{u}_{i} = 0\\)"
  },
  {
    "objectID": "lectures/04-Estimators-02/041-ols-properties.html#important-properties",
    "href": "lectures/04-Estimators-02/041-ols-properties.html#important-properties",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "There are three important OLS properties\n\n\nThe point \\((\\bar{x}, \\bar{y})\\) is always on the regression line\n\n\n\nResiduals sum to zero: \\(\\sum_{i}^{n} \\hat{u}_{i} = 0\\)\n\n\n\nThe sample covariance between the independent variable and the residuals is zero: \\(\\sum_{i}^{n} x_{i}\\hat{u}_{i} = 0\\)"
  },
  {
    "objectID": "lectures/04-Estimators-02/041-ols-properties.html#property-1---proof",
    "href": "lectures/04-Estimators-02/041-ols-properties.html#property-1---proof",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Property 1 - Proof",
    "text": "Property 1 - Proof\nThe point \\((\\bar{x}, \\bar{y})\\) is always on the regression line\n\nStart with the regression line: \\(\\hat{y}_{i} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1} x_{i}\\)\nRecall that \\(\\hat{\\beta}_{0} = \\bar{y} - \\hat{\\beta}_{1}\\bar{x}\\)\nPlug that in \\(\\hat{\\beta}_{0}\\) and substitute \\(\\bar{x}\\) for \\(x_{i}\\):\n\n\\[\\begin{align*}\n    \\hat{y}_{i} &= \\bar{y} - \\hat{\\beta}_{1}\\bar{x} + \\hat{\\beta}_{1} \\bar{x} \\\\\n    \\hat{y}_{i} &= \\bar{y}\n\\end{align*}\\]"
  },
  {
    "objectID": "lectures/04-Estimators-02/041-ols-properties.html#property-2---proof",
    "href": "lectures/04-Estimators-02/041-ols-properties.html#property-2---proof",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Property 2 - Proof",
    "text": "Property 2 - Proof\nResiduals sum to zero: \\(\\sum_{i}^{n} \\hat{u}_{i} = 0\\)\n\nRecall a couple of things we have derived:\n\n\\[\n    \\hat{y}_{i} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1}x_{i} \\;\\; \\text{and} \\;\\; \\hat{u}_{i} = y_{i} - \\hat{y}_{i}\n\\]\n\nThe sum of residuals is:\n\n\\[\n    \\sum_{i} \\hat{u}_{i} = \\sum_{i} (y_{i} - \\hat{y}_{i}) = \\sum_{i} y_{i} - \\sum \\hat{y}_{i}\n\\]\n\nRecall the fact that \\(\\sum_{i} y_{i} = n\\bar{y}\\) and also:\n\n\\[\\begin{align*}\n    \\sum_{i} \\hat{y}_{i} &= \\sum_{i} (\\hat{\\beta}_{0} + \\hat{\\beta}_{1}x_{i})\n    = n \\hat{\\beta}_{0} + \\hat{\\beta}_{1} \\sum_{i} x_{i} \\\\\n    &= n (\\bar{y}_{i} - \\hat{\\beta}_{1}\\bar{x}) + \\hat{\\beta}_{1} n\\bar{x} = n\\bar{y}_{i}\n\\end{align*}\\]\n\nSo:\n\n\\[\n    \\sum_{i} \\hat{u}_{i} = n\\bar{y}_{i} - n\\bar{y}_{i} = 0\n\\]"
  },
  {
    "objectID": "lectures/04-Estimators-02/041-ols-properties.html#property-3---proof",
    "href": "lectures/04-Estimators-02/041-ols-properties.html#property-3---proof",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Property 3 - Proof",
    "text": "Property 3 - Proof\nThe sample covariance between the independent variable and the residuals is zero: \\(\\sum_{i}^{n} x_{i}\\hat{u}_{i} = 0\\)\n\nStart with our residuals: \\(\\hat{u}_{i} = y_{i} - \\hat{\\beta}_{0} - \\hat{\\beta}_{1}x_{i}\\)\nMultiply both sides by \\(x_{i}\\) and sum them:\n\n\\[\n    \\sum_{i} x_{i}\\hat{u}_{i} = \\sum_{i} x_{i}y_{i} - \\hat{\\beta}_{0}\\sum_{i} x_{i} - \\hat{\\beta}_{1}\\sum_{i} x_{i}^{2}\n\\]\n\nRecall from our \\(\\hat{\\beta}_{1}\\) derivation that \\(\\sum_{i} x_{i}y_{i} = \\hat{\\beta}_{0}\\sum_{i} x_{i} + \\hat{\\beta}_{1}\\sum_{i} x_{i}^{2}\\)\n\nSo: \\(\\sum_{i}^{n} x_{i}\\hat{u}_{i} = \\hat{\\beta}_{0}\\sum_{i} x_{i} + \\hat{\\beta}_{1}\\sum_{i} x_{i}^{2} - \\hat{\\beta}_{0}\\sum_{i} x_{i} - \\hat{\\beta}_{1}\\sum_{i} x_{i}^{2} = 0\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/035-ols-interpretation.html",
    "href": "lectures/03-Estimators-01/035-ols-interpretation.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "There are two stages of interpretation of a regression equation\n\nInterpret regression estimates into words\nDeciding whether this interpretation should be taken at face value\n\n\nBoth stages are important, but for now, we will focus on the first\nLet’s revisit our crime example"
  },
  {
    "objectID": "lectures/03-Estimators-01/035-ols-interpretation.html#interpretation",
    "href": "lectures/03-Estimators-01/035-ols-interpretation.html#interpretation",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "There are two stages of interpretation of a regression equation\n\nInterpret regression estimates into words\nDeciding whether this interpretation should be taken at face value\n\n\nBoth stages are important, but for now, we will focus on the first\nLet’s revisit our crime example"
  },
  {
    "objectID": "lectures/03-Estimators-01/035-ols-interpretation.html#ex-effect-of-police-on-crime",
    "href": "lectures/03-Estimators-01/035-ols-interpretation.html#ex-effect-of-police-on-crime",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Ex: Effect of Police on Crime",
    "text": "Ex: Effect of Police on Crime\nUsing the OLS formulas, we get \\(\\hat{\\beta}_{0} = 18.41\\) and \\(\\hat{\\beta}_{1} = 1.76\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/035-ols-interpretation.html#coefficient-interpretation",
    "href": "lectures/03-Estimators-01/035-ols-interpretation.html#coefficient-interpretation",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Coefficient Interpretation",
    "text": "Coefficient Interpretation\nHow do I interpret \\(\\hat{\\beta}_{0} = 18.41\\) and \\(\\hat{\\beta}_{1} = 1.76\\)?\nThe general interpration of the intercept is the estimated value of \\(y_{i}\\) when \\(x_{i} = 0\\)\nAnd the general interpretation of the slope parameter is the estimated change \\(y_{i}\\) for the marginal increase \\(x_{i}\\)\n. . .\nFirst, it is important to understand the units:\n\n\\(\\widehat{\\text{Crime}}_{i}\\) is measured as a crime rate, the number of crimes per 1,000 students on campus\n\\(\\text{Police}_{i}\\) is also measured as a rate, the number of police officers per 1,000 students on campus"
  },
  {
    "objectID": "lectures/03-Estimators-01/035-ols-interpretation.html#coefficient-interpretation-1",
    "href": "lectures/03-Estimators-01/035-ols-interpretation.html#coefficient-interpretation-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Coefficient Interpretation",
    "text": "Coefficient Interpretation\nUsing OLS gives us the fitted line\n\\[\n\\widehat{\\text{Crime}_i} = \\hat{\\beta}_1 + \\hat{\\beta}_2\\text{Police}_i.\n\\]\nWhat does \\(\\hat{\\beta_0}\\) = \\(18.41\\) tell us? Without any police on campus, the crime rate is \\(18.41\\) per 1,000 people on campus\n. . .\nWhat does \\(\\hat{\\beta_1}\\) = \\(1.76\\) tell us? For each additional police officer per 1,000, there is an associated increase in the crime rate by \\(1.76\\) crimes per 1,000 people on campus.\n. . .\nDoes this mean that police cause crime? Probably not.\nThis is where deciding if the interpretation should be taken at face value. It now becomes your job to bring reason to the values."
  },
  {
    "objectID": "lectures/03-Estimators-01/033-linear-ex.html#ex.-effect-of-police-on-crime",
    "href": "lectures/03-Estimators-01/033-linear-ex.html#ex.-effect-of-police-on-crime",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Ex. Effect of police on crime",
    "text": "Ex. Effect of police on crime\n\nEmpirical question:\n\nDoes the number of on-campus police officers affect campus crime rates? If so, by how much?\n\n\n. . .\n\nAlways plot your data first"
  },
  {
    "objectID": "lectures/03-Estimators-01/033-linear-ex.html#ex.-effect-of-police-on-crime-1",
    "href": "lectures/03-Estimators-01/033-linear-ex.html#ex.-effect-of-police-on-crime-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Ex. Effect of police on crime",
    "text": "Ex. Effect of police on crime\nThe scatter plot suggest that a weak positive relationship exists\n\nA sample correlation of 0.14 confirms this\n\n\n. . .\nBut correlation does not imply causation\n. . .\n\nLets estimate a statistical model"
  },
  {
    "objectID": "lectures/03-Estimators-01/033-linear-ex.html#ex.-effect-of-police-on-crime-2",
    "href": "lectures/03-Estimators-01/033-linear-ex.html#ex.-effect-of-police-on-crime-2",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Ex. Effect of police on crime",
    "text": "Ex. Effect of police on crime\nWe express the relationship between a dependent variable and an independent variable as linear:\n\\[\n{\\text{Crime}_i} = \\beta_0 + \\beta_1 \\text{Police}_i + u_i.\n\\]\n\n\\(\\beta_0\\) is the intercept or constant.\n\\(\\beta_1\\) is the slope coefficient.\n\\(u_i\\) is an error term or disturbance term."
  },
  {
    "objectID": "lectures/03-Estimators-01/033-linear-ex.html#ex.-effect-of-police-on-crime-3",
    "href": "lectures/03-Estimators-01/033-linear-ex.html#ex.-effect-of-police-on-crime-3",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Ex. Effect of police on crime",
    "text": "Ex. Effect of police on crime\nThe intercept tells us the expected value of \\(\\text{Crime}_i\\) when \\(\\text{Police}_i = 0\\).\n\\[\n\\text{Crime}_i = {\\color{#BF616A} \\beta_{0}} + \\beta_1\\text{Police}_i + u_i\n\\]\nUsually not the focus of an analysis."
  },
  {
    "objectID": "lectures/03-Estimators-01/033-linear-ex.html#ex.-effect-of-police-on-crime-4",
    "href": "lectures/03-Estimators-01/033-linear-ex.html#ex.-effect-of-police-on-crime-4",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Ex. Effect of police on crime",
    "text": "Ex. Effect of police on crime\nThe slope coefficient tells us the expected change in \\(\\text{Crime}_i\\) when \\(\\text{Police}_i\\) increases by one.\n\\[\n\\text{Crime}_i = \\beta_0 + {\\color{#BF616A} \\beta_1} \\text{Police}_i + u_i\n\\]\n“A one-unit increase in \\(\\text{Police}_i\\) is associated with a \\(\\color{#BF616A}{\\beta_1}\\)-unit increase in \\(\\text{Crime}_i\\).”\n. . .\nInterpretation of this parameter is crucial\n. . .\nUnder certain (strong) assumptions1, \\(\\color{#BF616A}{\\beta_1}\\) is the effect of \\(X_i\\) on \\(Y_i\\).\n\nOtherwise, it’s the association of \\(X_i\\) with \\(Y_i\\)."
  },
  {
    "objectID": "lectures/03-Estimators-01/033-linear-ex.html#ex.-effect-of-police-on-crime-5",
    "href": "lectures/03-Estimators-01/033-linear-ex.html#ex.-effect-of-police-on-crime-5",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Ex. Effect of police on crime",
    "text": "Ex. Effect of police on crime\nThe error term reminds us that \\(\\text{Police}_i\\) does not perfectly explain \\(Y_i\\).\n\\[\n\\text{Crime}_i = \\beta_0 + \\beta_1\\text{Police}_i + {\\color{#BF616A} u_i}\n\\]\nRepresents all other factors that explain \\(\\text{Crime}_i\\).\n\nUseful mnemonic: pretend that \\(u\\) stands for “unobserved” or “unexplained.”"
  },
  {
    "objectID": "lectures/03-Estimators-01/033-linear-ex.html#ex.-effect-of-police-on-crime-6",
    "href": "lectures/03-Estimators-01/033-linear-ex.html#ex.-effect-of-police-on-crime-6",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Ex. Effect of police on crime",
    "text": "Ex. Effect of police on crime\nHow might we apply the simple linear regression model to our question about the effect of on-campus police on campus crime?\n\\[\n\\text{Crime}_i = \\beta_0 + \\beta_1\\text{Police}_i + u_i.\n\\]\n\n\\(\\beta_0\\) is the crime rate for colleges without police.\n\\(\\beta_1\\) is the increase in the crime rate for an additional police officer per 1000 students."
  },
  {
    "objectID": "lectures/03-Estimators-01/033-linear-ex.html#ex.-effect-of-police-on-crime-7",
    "href": "lectures/03-Estimators-01/033-linear-ex.html#ex.-effect-of-police-on-crime-7",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Ex. Effect of police on crime",
    "text": "Ex. Effect of police on crime\nHow might we apply the simple linear regression model to our question?\n\\[\n\\text{Crime}_i = \\beta_0 + \\beta_1\\text{Police}_i + u_i\n\\]\n\\(\\beta_0\\) and \\(\\beta_1\\) are the unobserved population parameters we want\n. . .\n\nWe estimate\n\n\\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) generate predictions of \\(\\text{Crime}_i\\) called \\(\\widehat{\\text{Crime}_i}\\).\nWe call the predictions of the dependent variable fitted values.\n\n. . .\n\nTogether, these trace a line: \\(\\widehat{\\text{Crime}_i} = \\hat{\\beta_0} + \\hat{\\beta_1}\\text{Police}_i\\)."
  },
  {
    "objectID": "lectures/03-Estimators-01/033-linear-ex.html#section-1",
    "href": "lectures/03-Estimators-01/033-linear-ex.html#section-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "So, the question becomes, how do I pick \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/033-linear-ex.html#section-2",
    "href": "lectures/03-Estimators-01/033-linear-ex.html#section-2",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Let’s take some guesses: \\(\\hat{\\beta_0} = 60\\) and \\(\\hat{\\beta}_{1} = -7\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/033-linear-ex.html#section-3",
    "href": "lectures/03-Estimators-01/033-linear-ex.html#section-3",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Let’s take some guesses: \\(\\hat{\\beta_0} = 30\\) and \\(\\hat{\\beta}_{1} = 0\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/033-linear-ex.html#section-4",
    "href": "lectures/03-Estimators-01/033-linear-ex.html#section-4",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Let’s take some guesses: \\(\\hat{\\beta_0} = 15.6\\) and \\(\\hat{\\beta}_{1} = 7.94\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/033-linear-ex.html#residuals",
    "href": "lectures/03-Estimators-01/033-linear-ex.html#residuals",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Residuals",
    "text": "Residuals\nUsing \\(\\hat{\\beta}_{0}\\) and \\(\\hat{\\beta}_{1}\\) to make \\(\\hat{y}_{i}\\) generates misses.\n\n\n\n \\(\\hat{\\beta_0} = 60 \\;\\) Guess\n\n \\(\\hat{\\beta_0} = 30 \\;\\) Guess\n\n \\(\\hat{\\beta_0} = 15 \\;\\) Guess"
  },
  {
    "objectID": "lectures/03-Estimators-01/033-linear-ex.html#residuals-sum-of-squares-rss",
    "href": "lectures/03-Estimators-01/033-linear-ex.html#residuals-sum-of-squares-rss",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Residuals Sum of Squares (RSS)",
    "text": "Residuals Sum of Squares (RSS)\nWhat if we picked an estimator that minimizes the residuals?\nWhy do we not minimize:\n\\[\n    \\sum_{i=1}^{n} \\hat{u}_{i}^{2}\n\\]\nso that the estimator makes fewer big misses?\nThis estimator, the residual sum of squares (RSS), is convenient because squared numbers are never negative so we can minimze an absolut sum of the residuals\nRSS will give bigger penalties to bigger residuals"
  },
  {
    "objectID": "lectures/03-Estimators-01/033-linear-ex.html#minimizing-rss",
    "href": "lectures/03-Estimators-01/033-linear-ex.html#minimizing-rss",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Minimizing RSS",
    "text": "Minimizing RSS\nWe could test thousands of guesses of \\(\\beta_0\\) and \\(\\beta_1\\) an pick the pair the has the smallest RSS\nWe could painstakingly do that, and eventually figure out which one fits best.\nOr… We could just do a little math"
  },
  {
    "objectID": "lectures/03-Estimators-01/033-linear-ex.html#footnotes",
    "href": "lectures/03-Estimators-01/033-linear-ex.html#footnotes",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAssumptions regarding the error term↩︎"
  },
  {
    "objectID": "lectures/03-Estimators-01/031-estimators.html",
    "href": "lectures/03-Estimators-01/031-estimators.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Let’s define some concepts first:\nEstimand\n\nQuantity that is to be estimated in a statistical analysis\n\nEstimator\n\nA rule (or formula) for estimating an unknown population parameter given a sample of data\n\nEstimate\n\nA specific numerical value that we obtain from the smaple data by applying the estimator"
  },
  {
    "objectID": "lectures/03-Estimators-01/031-estimators.html#estimators",
    "href": "lectures/03-Estimators-01/031-estimators.html#estimators",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Let’s define some concepts first:\nEstimand\n\nQuantity that is to be estimated in a statistical analysis\n\nEstimator\n\nA rule (or formula) for estimating an unknown population parameter given a sample of data\n\nEstimate\n\nA specific numerical value that we obtain from the smaple data by applying the estimator"
  },
  {
    "objectID": "lectures/03-Estimators-01/031-estimators.html#estimators-example",
    "href": "lectures/03-Estimators-01/031-estimators.html#estimators-example",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Estimators Example",
    "text": "Estimators Example\nSuppose we want to know the average height of the population in the US\n\nWe have a sample of 1 million Americans\n\nSo then we can identify our Estimand, Estimator, and Estimate\n\nEstimand: The population mean \\((\\mu)\\)\nEstimator: The sample mean \\((\\bar{X})\\)\n\n\\[\n    \\bar{X} = \\dfrac{1}{n} \\sum_{i=1}^{n} X_{i}\n\\]\n\nEstimate: The sample mean \\((\\hat{\\mu} = 5'6'')\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/031-estimators.html#properties-of-estimators",
    "href": "lectures/03-Estimators-01/031-estimators.html#properties-of-estimators",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Properties of Estimators",
    "text": "Properties of Estimators\nThere are many ways to estimate things and they all have their benefits and costs.\nImagine we want to estimate an unknown parameter \\(\\mu\\), and we know the distributions of three competing estimators.\nWhich one should we use?"
  },
  {
    "objectID": "lectures/03-Estimators-01/031-estimators.html#properties-of-estimators---unbiasedness",
    "href": "lectures/03-Estimators-01/031-estimators.html#properties-of-estimators---unbiasedness",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Properties of Estimators - Unbiasedness",
    "text": "Properties of Estimators - Unbiasedness\nWe ask: What properties make an estimator reliable?\nAnswer (1): Unbiasedness\nOn average, does the estimator tend toward the correct value?\n\nFormally: Does the mean of the estimator’s distribution equal the parameter it estimates?\n\\[\n    \\text{Bias}_{\\mu} (\\hat{\\mu}) = E[\\hat{\\mu}] - \\mu\n\\]"
  },
  {
    "objectID": "lectures/03-Estimators-01/031-estimators.html#properties-of-estimators-1",
    "href": "lectures/03-Estimators-01/031-estimators.html#properties-of-estimators-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Properties of estimators",
    "text": "Properties of estimators\nQuestion What properties make an estimator reliable?\nA01: Unbiasedness\n\n\nUnbiased estimator: \\(E\\left[ \\hat{\\mu} \\right] = \\mu\\)\n\n\n\n\n\n\n\n\n\n\nBiased estimator \\(E\\left[ \\hat{\\mu} \\right] \\neq \\mu\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/031-estimators.html#properties-of-estimators---efficiency",
    "href": "lectures/03-Estimators-01/031-estimators.html#properties-of-estimators---efficiency",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Properties of Estimators - Efficiency",
    "text": "Properties of Estimators - Efficiency\nWe ask: What properties make an estimator reliable?\nAnswer (1): Efficiency (Low Variance)\nThe central tendencies (means) of competing distribution are not the only things that matter. We also care about the variance of an estimator.\n\\[\n    Var(\\hat{\\mu}) = E \\left[ (\\hat{\\mu} - E[\\hat{\\mu}])^{2} \\right]\n\\]\nLower variance estimators estimate closer to the mean in each sample"
  },
  {
    "objectID": "lectures/03-Estimators-01/031-estimators.html#properties-of-estimators---efficiency-1",
    "href": "lectures/03-Estimators-01/031-estimators.html#properties-of-estimators---efficiency-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Properties of Estimators - Efficiency",
    "text": "Properties of Estimators - Efficiency\nImagine low variance to be similar to accuracy \\(\\rightarrow\\) tighter estimates"
  },
  {
    "objectID": "lectures/03-Estimators-01/031-estimators.html#the-bias-variance-tradeoff",
    "href": "lectures/03-Estimators-01/031-estimators.html#the-bias-variance-tradeoff",
    "title": "EC 320 - Intro. Econometrics",
    "section": "The Bias-Variance Tradeoff",
    "text": "The Bias-Variance Tradeoff\nMuch like everything, there are tradeoffs from gaining one thing over another.\nShould we be willing to take a bit of bias to reduce the variance?\nIn economics/causal inference, we emphasize unbiasedness"
  },
  {
    "objectID": "lectures/03-Estimators-01/031-estimators.html#unbiased-estimators",
    "href": "lectures/03-Estimators-01/031-estimators.html#unbiased-estimators",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Unbiased estimators",
    "text": "Unbiased estimators\nIn addition to the sample mean, there are other unbiased estimators we will often use\n\n\nSample variance estimates the variance \\(\\sigma^{2}\\)\n\n\n\nSample covariance setimates the covariance \\(\\sigma_{XY}\\)\n\n\n\nSample correlation estimates the pop. correlation coefficient \\(\\rho_{XY}\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/031-estimators.html#sample-variance",
    "href": "lectures/03-Estimators-01/031-estimators.html#sample-variance",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Sample Variance",
    "text": "Sample Variance\nThe sample variance, \\(S_{X}^{2}\\), is an unbiased estimator of the population variance\n\n\\[\n    S_{X}^{2} = \\dfrac{1}{n - 1} \\sum_{i=1}^{n} (X_{i} - \\bar{X})^{2}.\n\\]"
  },
  {
    "objectID": "lectures/03-Estimators-01/031-estimators.html#sample-covariance",
    "href": "lectures/03-Estimators-01/031-estimators.html#sample-covariance",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Sample Covariance",
    "text": "Sample Covariance\nThe sample covariance, \\(S_{XY}\\), is an unbiaed estimator of the population covariance\n\n\\[\n    S_{XY} = \\dfrac{1}{n-1} \\sum_{i=1}^{n} (X_{i} - \\bar{X})(Y_{i} - \\bar{Y}).\n\\]"
  },
  {
    "objectID": "lectures/03-Estimators-01/031-estimators.html#sample-correlation",
    "href": "lectures/03-Estimators-01/031-estimators.html#sample-correlation",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Sample Correlation",
    "text": "Sample Correlation\nSample correlation, \\(r_{XY}\\), is an unbiased estimator of the population correlation coefficient\n\n\\[\n    r_{XY} = \\dfrac{S_{XY}}{\\sqrt{S_{X}^{2}}\\sqrt{S_{Y}^{2}}}.\n\\]"
  },
  {
    "objectID": "lectures/01-Random-Variables/015-correlation.html",
    "href": "lectures/01-Random-Variables/015-correlation.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "An issue with covariance is that the covariance between two random variables depends on the units those variables are measured in. That’s where correlation comes in:\nCorrelation is another measure of linear association that has the benefit of being dimensionless because the units in the numerator cancel with the units in the denominator.\nIt is also the case that the correlation between two variables is always between -1 and 1. Where correlation = 1, the two variables have a perfect positive linear relationsihp, and when correlation = -1, the two variables have a perfect negative linear relationship.\nWe will use the greek letter \\(\\rho\\) (“rho”) to refer to the correlation between two RVs. The formula is:\n\\[\\begin{align*}\n    \\rho_{XY} =\n    \\dfrac{\n        \\sigma_{XY}\n    }{\n        \\sqrt{\\sigma_{X}^{2}\\sigma_{Y}^{2}}\n    }\n\\end{align*}\\]"
  },
  {
    "objectID": "lectures/01-Random-Variables/015-correlation.html#definition",
    "href": "lectures/01-Random-Variables/015-correlation.html#definition",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "An issue with covariance is that the covariance between two random variables depends on the units those variables are measured in. That’s where correlation comes in:\nCorrelation is another measure of linear association that has the benefit of being dimensionless because the units in the numerator cancel with the units in the denominator.\nIt is also the case that the correlation between two variables is always between -1 and 1. Where correlation = 1, the two variables have a perfect positive linear relationsihp, and when correlation = -1, the two variables have a perfect negative linear relationship.\nWe will use the greek letter \\(\\rho\\) (“rho”) to refer to the correlation between two RVs. The formula is:\n\\[\\begin{align*}\n    \\rho_{XY} =\n    \\dfrac{\n        \\sigma_{XY}\n    }{\n        \\sqrt{\\sigma_{X}^{2}\\sigma_{Y}^{2}}\n    }\n\\end{align*}\\]"
  },
  {
    "objectID": "lectures/01-Random-Variables/013-variance.html",
    "href": "lectures/01-Random-Variables/013-variance.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "The variance of a random variable measures its dispersion. It asks “on average, how far is the variable from its average”? Differences are squared to get rid of the negative sign and punish large deviances a little more. We will use the greek letter \\(\\sigma\\) (“sigma”) for variance \\((\\sigma^{2})\\) and standard deviation \\((\\sigma)\\)\nThe formula is:\n\\[\\begin{align}\n    Var(X) = \\sigma_{X}^{2}\n    &= E[(X - \\mu_{X})^{2}] \\\\\n    &= (x_{1} - \\mu_{X})^{2}p_{1} + (x_{2} - \\mu_{X})^{2}p_{2} + \\cdots + (x_{n} - \\mu_{X})^{2}p_{n} \\\\\n    &= \\sum_{i = 1}^{n} (x_{i} - \\mu_{X})^{2}p_{i}\n\\end{align}\\]\nNote that because of the square and the fact that probabilities \\(p_{i}\\) are never negative, the variance of a RV can never be a negative number"
  },
  {
    "objectID": "lectures/01-Random-Variables/013-variance.html#definition",
    "href": "lectures/01-Random-Variables/013-variance.html#definition",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "The variance of a random variable measures its dispersion. It asks “on average, how far is the variable from its average”? Differences are squared to get rid of the negative sign and punish large deviances a little more. We will use the greek letter \\(\\sigma\\) (“sigma”) for variance \\((\\sigma^{2})\\) and standard deviation \\((\\sigma)\\)\nThe formula is:\n\\[\\begin{align}\n    Var(X) = \\sigma_{X}^{2}\n    &= E[(X - \\mu_{X})^{2}] \\\\\n    &= (x_{1} - \\mu_{X})^{2}p_{1} + (x_{2} - \\mu_{X})^{2}p_{2} + \\cdots + (x_{n} - \\mu_{X})^{2}p_{n} \\\\\n    &= \\sum_{i = 1}^{n} (x_{i} - \\mu_{X})^{2}p_{i}\n\\end{align}\\]\nNote that because of the square and the fact that probabilities \\(p_{i}\\) are never negative, the variance of a RV can never be a negative number"
  },
  {
    "objectID": "lectures/01-Random-Variables/013-variance.html#rules",
    "href": "lectures/01-Random-Variables/013-variance.html#rules",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Rules",
    "text": "Rules\nSome important rules about the way variance works. Let \\(X\\) and \\(Y\\) be random variables and let \\(b\\) be a constant.\n\nThe variance of the sum of two RVs is the sum of their variances plus two times their covariance: \\[\nVar(X + Y) = Var(X) + Var(Y) + 2Cov(X,Y)\n\\]\nConstants can pass outside of a variance if you square them: \\[\nVar(bX) = b^{2}Var(X)\n\\]\nThe variance of a constant is 0: \\[\nVar(b) = 0\n\\]\nThe variance of a RV plus a constant is the variance of that random variable: \\[\nVar(X + b) = Var(X)\n\\]"
  },
  {
    "objectID": "lectures/01-Random-Variables/011-probs.html",
    "href": "lectures/01-Random-Variables/011-probs.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "A Random Variable is any variable whose value cannot be predicted exactly. For example:\n\nThe message you get in a fortune cookie\nThe amount of time spent searching for your keys\nThe number of likes you get on a social media post\nThe number of customers that enter a store in a day\n\nAll of these are random variables.\nSome random variables are discrete and some are continuous"
  },
  {
    "objectID": "lectures/01-Random-Variables/011-probs.html#random-variables",
    "href": "lectures/01-Random-Variables/011-probs.html#random-variables",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "A Random Variable is any variable whose value cannot be predicted exactly. For example:\n\nThe message you get in a fortune cookie\nThe amount of time spent searching for your keys\nThe number of likes you get on a social media post\nThe number of customers that enter a store in a day\n\nAll of these are random variables.\nSome random variables are discrete and some are continuous"
  },
  {
    "objectID": "lectures/01-Random-Variables/011-probs.html#discrete-and-continuous-rv",
    "href": "lectures/01-Random-Variables/011-probs.html#discrete-and-continuous-rv",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Discrete and Continuous RV",
    "text": "Discrete and Continuous RV\nWhat’s the difference?\n\n\nDiscrete\n\nCounted\nTake on a small number of possible values\nEx: Number of M&Ms in your bag\n\n\nContinuous\n\nMeasured\nCan take on an infinite number of possible values\nEx: How heavy your bag is\n\n\n\nVariables can also be categorical instead of numeric. They may represent qualitative data that can be divided into categories or groups. For now, we will lump them in with discrete variables"
  },
  {
    "objectID": "lectures/01-Random-Variables/011-probs.html#discrete-probability-distributions",
    "href": "lectures/01-Random-Variables/011-probs.html#discrete-probability-distributions",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Discrete Probability Distributions",
    "text": "Discrete Probability Distributions\nConsider the event of a dice roll. This action produces a discrete random variable.\nIt could take on values 1 to 6 and, if it is a fair die, it takes on each of those values with equali probability \\(1/6\\).\nOur notation will be:\n\n\\(X\\) is the random variable, \\(x_{i}\\) is a potential outcome for \\(X\\), and each potential outcome \\(x_{i}\\) happens with probability \\(p_{i}\\)\n\n\n\n\n\\(x_{i}\\)\n1\n2\n3\n4\n5\n6\n\n\n\n\n\n\\(p_{i}\\)\n1/6\n1/6\n1/6\n1/6\n1/6\n1/6"
  },
  {
    "objectID": "lectures/01-Random-Variables/011-probs.html#discrete-probability-distributions-1",
    "href": "lectures/01-Random-Variables/011-probs.html#discrete-probability-distributions-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Discrete Probability Distributions",
    "text": "Discrete Probability Distributions\nConsider another random variable \\(X\\) to be the sum of two dice rolls. In the table below, the first row represents the potential outcomes for the first roll and the first column represents the potential outcomes for the second roll. The values inside the table represent the potential outcomes for \\(X\\) (the sum)\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n2\n3\n4\n5\n6\n7\n8\n\n\n3\n4\n5\n6\n7\n8\n9\n\n\n4\n5\n6\n7\n8\n9\n10\n\n\n5\n6\n7\n8\n9\n10\n11\n\n\n6\n7\n8\n9\n10\n11\n12\n\n\n\n\nEach of the cells occur with equal probability. So that X = 2 has probability 1/36. X = 3 has probability 2/36, as it can occur in two ways."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "EC320 - Introduction to Econometrics",
    "section": "",
    "text": "Hello! Welcome to EC 320. I am Jose Rojas-Fallas, a 4th Year PhD student in the Econ department. And welcome to the course website! I have begun to use a website as a main method of course delivery as it is more intuitive (and easier to manage) than Canvas. For this course, Canvas is a glorified assignment submission site and gradebook. You will find everything you need on this site by navigating the tabs above.\nThat said: as you can clearly see, it is still largely incomplete. I aim to be as transparent as possible with students and their course so allow me to quickly explain why and how I intend for this course to run:\n\nThis is my first time teaching this course so I am still putting all the content together in a way that I like to teach it.\nI will have a complete “Week 01” content by Tuesday evening. I am currently traveling for a conference so my time has unfortunately been drawn away. But you will not be negatively impacted.\nWe will be doing alternate weeks of “theory/math” and “R”. The goal is to understand the reason why things work and then learn how to make them work.\nA lot, and I mean a lot, of econometrics is about learning by doing. What I mean by that is that you should always attempt the math or to code things even if you do not have a clear picture of how to do things. R may become frustrating but it is a worthwhile skill to pick up. To this day I still have to refer to older code I’ve used for simple things such as making a graph.\nLLMs, like ChatGPT, are a great tool that can assist you but it still requires you to understand the output and be able to properly implement it. Do not over rely on it."
  },
  {
    "objectID": "index.html#hello",
    "href": "index.html#hello",
    "title": "EC320 - Introduction to Econometrics",
    "section": "",
    "text": "Hello! Welcome to EC 320. I am Jose Rojas-Fallas, a 4th Year PhD student in the Econ department. And welcome to the course website! I have begun to use a website as a main method of course delivery as it is more intuitive (and easier to manage) than Canvas. For this course, Canvas is a glorified assignment submission site and gradebook. You will find everything you need on this site by navigating the tabs above.\nThat said: as you can clearly see, it is still largely incomplete. I aim to be as transparent as possible with students and their course so allow me to quickly explain why and how I intend for this course to run:\n\nThis is my first time teaching this course so I am still putting all the content together in a way that I like to teach it.\nI will have a complete “Week 01” content by Tuesday evening. I am currently traveling for a conference so my time has unfortunately been drawn away. But you will not be negatively impacted.\nWe will be doing alternate weeks of “theory/math” and “R”. The goal is to understand the reason why things work and then learn how to make them work.\nA lot, and I mean a lot, of econometrics is about learning by doing. What I mean by that is that you should always attempt the math or to code things even if you do not have a clear picture of how to do things. R may become frustrating but it is a worthwhile skill to pick up. To this day I still have to refer to older code I’ve used for simple things such as making a graph.\nLLMs, like ChatGPT, are a great tool that can assist you but it still requires you to understand the output and be able to properly implement it. Do not over rely on it."
  },
  {
    "objectID": "index.html#syllabus",
    "href": "index.html#syllabus",
    "title": "EC320 - Introduction to Econometrics",
    "section": "Syllabus",
    "text": "Syllabus\n\n  Summer 2025 Syllabus PDF\n  \n     Download"
  },
  {
    "objectID": "index.html#summer-2025-class-details",
    "href": "index.html#summer-2025-class-details",
    "title": "EC320 - Introduction to Econometrics",
    "section": "Summer 2025 Class Details",
    "text": "Summer 2025 Class Details\n\n\nInstructor: Jose Rojas-Fallas\nOffice Hours: Tues/Thurs 02:00 to 04:00 pm\n\nEmail: jrojas2@uoregon.edu\nZoom Room\n\n\n\nCourse Grade Breakdown\nFor more details, read the Syllabus above\n\n\n\nAssignment\nGrade Weight\n\n\n\n\nProblem Sets (x4)\n20%\n\n\nQuizzes (x4)\n20%\n\n\nMidterm Exam\n30%\n\n\nFinal Exam\n30%\n\n\n\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "documents/problem-sets/index.html",
    "href": "documents/problem-sets/index.html",
    "title": "Assignments",
    "section": "",
    "text": "Test"
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#preview",
    "href": "lectures/01-Random-Variables/010-compile.html#preview",
    "title": "Random Variables",
    "section": "Preview",
    "text": "Preview\nIn this chapter we will:\n\nLearn what discrete and continuous random variables are\nHow to use the probability distribution of a discrete random variable to obtain the expected value and variance of the random variable\nHow to use the probability density function (PDF) of a continuous random variable to obtain the expected value and variance of the random variable\nHow to obtain the covariance and correlation between two random variables"
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#notation",
    "href": "lectures/01-Random-Variables/010-compile.html#notation",
    "title": "Random Variables",
    "section": "Notation",
    "text": "Notation\nSome important notation we need to introduce:\n\n\n\n\nSymbol\nMeaning\n\n\n\n\n\\(X\\)\nRandom Variable (RV)\n\n\n\\(x_i\\)\nA potential outcome for the RV \\(X\\)\n\n\n\\(p_i\\)\nThe probability a certain outcome will occur (discrete RVs)\n\n\n\\(\\mu_X\\)\nThe expected value of \\(X\\), also known as \\(E[X]\\)\n\n\n\\(\\sigma_X^2\\)\nThe variance of \\(X\\)\n\n\n\\(\\sigma_X\\)\nThe standard deviation of \\(X\\)\n\n\n\\(\\sigma_{XY}\\)\nThe covariance of \\(X\\) and \\(Y\\)\n\n\n\\(\\rho_{XY}\\)\nThe correlation between \\(X\\) and \\(Y\\)"
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#random-variables-1",
    "href": "lectures/01-Random-Variables/010-compile.html#random-variables-1",
    "title": "Random Variables",
    "section": "Random Variables",
    "text": "Random Variables\nA Random Variable is any variable whose value cannot be predicted exactly. For example:\n\nThe message you get in a fortune cookie\nThe amount of time spent searching for your keys\nThe number of likes you get on a social media post\nThe number of customers that enter a store in a day\n\nAll of these are random variables.\nSome random variables are discrete and some are continuous"
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#discrete-and-continuous-rv",
    "href": "lectures/01-Random-Variables/010-compile.html#discrete-and-continuous-rv",
    "title": "Random Variables",
    "section": "Discrete and Continuous RV",
    "text": "Discrete and Continuous RV\nWhat’s the difference?\n\n\nDiscrete\n\nCounted\nTake on a small number of possible values\nEx: Number of M&Ms in your bag\n\n\nContinuous\n\nMeasured\nCan take on an infinite number of possible values\nEx: How heavy your bag is\n\n\nVariables can also be categorical instead of numeric. They may represent qualitative data that can be divided into categories or groups. For now, we will lump them in with discrete variables"
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#discrete-probability-distributions",
    "href": "lectures/01-Random-Variables/010-compile.html#discrete-probability-distributions",
    "title": "Random Variables",
    "section": "Discrete Probability Distributions",
    "text": "Discrete Probability Distributions\nConsider the event of a dice roll. This action produces a discrete random variable.\nIt could take on values 1 to 6 and, if it is a fair die, it takes on each of those values with equali probability \\(1/6\\).\nOur notation will be:\n\n\\(X\\) is the random variable, \\(x_{i}\\) is a potential outcome for \\(X\\), and each potential outcome \\(x_{i}\\) happens with probability \\(p_{i}\\)\n\n\n\n\n\\(x_{i}\\)\n1\n2\n3\n4\n5\n6\n\n\n\n\n\n\\(p_{i}\\)\n1/6\n1/6\n1/6\n1/6\n1/6\n1/6"
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#discrete-probability-distributions-1",
    "href": "lectures/01-Random-Variables/010-compile.html#discrete-probability-distributions-1",
    "title": "Random Variables",
    "section": "Discrete Probability Distributions",
    "text": "Discrete Probability Distributions\nConsider another random variable \\(X\\) to be the sum of two dice rolls. In the table below, the first row represents the potential outcomes for the first roll and the first column represents the potential outcomes for the second roll. The values inside the table represent the potential outcomes for \\(X\\) (the sum)\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n2\n3\n4\n5\n6\n7\n8\n\n\n3\n4\n5\n6\n7\n8\n9\n\n\n4\n5\n6\n7\n8\n9\n10\n\n\n5\n6\n7\n8\n9\n10\n11\n\n\n6\n7\n8\n9\n10\n11\n12\n\n\n\n\nEach of the cells occur with equal probability. So that X = 2 has probability 1/36. X = 3 has probability 2/36, as it can occur in two ways."
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#expected-values-of-discrete-random-variables",
    "href": "lectures/01-Random-Variables/010-compile.html#expected-values-of-discrete-random-variables",
    "title": "Random Variables",
    "section": "Expected Values of Discrete Random Variables",
    "text": "Expected Values of Discrete Random Variables\nThe expected value of a random variable is its long-term average.\nWe will use the greek letter \\(\\mu\\) (“mew”) to refer to expected values. That is, we will say that the expected value of \\(X\\) is \\(\\mu_{X}\\), or equivalently, \\(E[X] = \\mu_{X}\\).\nIf the variable is discrete, you can calculate its expectation by taking the sum of all possible values of the random variable, each multiplied by their corresponding probabilities.\nWe write this as:\n\\[\n    E[X] = \\sum_{i} x_{i}p_{i}\n\\]\nWhere \\(x_{i}\\) is a potential outcome for \\(X\\) and \\(p_{i}\\) is the probability that outcome occurs"
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#expected-value-rules",
    "href": "lectures/01-Random-Variables/010-compile.html#expected-value-rules",
    "title": "Random Variables",
    "section": "Expected Value Rules",
    "text": "Expected Value Rules\n\nHere are some very important math rules to know about the way expected values work. Let \\(X\\),\\(Y\\), and \\(Z\\) be random variables and let \\(b\\) be a constant.\n\n\nThe expectation of the sum of several RVs is the sum of their expectation: \\[\nE[X + Y + Z] = E[X] + E[Y] + E[Z]\n\\]\nConstants can pass outside of an expectation: \\[\nE[bX] = bE[X]\n\\]\nThe expected value of a constant is that constant: \\[\nE[b] = b\n\\]"
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#definition",
    "href": "lectures/01-Random-Variables/010-compile.html#definition",
    "title": "Random Variables",
    "section": "Definition",
    "text": "Definition\nThe variance of a random variable measures its dispersion. It asks “on average, how far is the variable from its average”? Differences are squared to get rid of the negative sign and punish large deviances a little more. We will use the greek letter \\(\\sigma\\) (“sigma”) for variance \\((\\sigma^{2})\\) and standard deviation \\((\\sigma)\\)\nThe formula is:\n\\[\\begin{align}\n    Var(X) = \\sigma_{X}^{2}\n    &= E[(X - \\mu_{X})^{2}] \\\\\n    &= (x_{1} - \\mu_{X})^{2}p_{1} + (x_{2} - \\mu_{X})^{2}p_{2} + \\cdots + (x_{n} - \\mu_{X})^{2}p_{n} \\\\\n    &= \\sum_{i = 1}^{n} (x_{i} - \\mu_{X})^{2}p_{i}\n\\end{align}\\]\nNote that because of the square and the fact that probabilities \\(p_{i}\\) are never negative, the variance of a RV can never be a negative number"
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#rules",
    "href": "lectures/01-Random-Variables/010-compile.html#rules",
    "title": "Random Variables",
    "section": "Rules",
    "text": "Rules\nSome important rules about the way variance works. Let \\(X\\) and \\(Y\\) be random variables and let \\(b\\) be a constant.\n\nThe variance of the sum of two RVs is the sum of their variances plus two times their covariance: \\[\nVar(X + Y) = Var(X) + Var(Y) + 2Cov(X,Y)\n\\]\nConstants can pass outside of a variance if you square them: \\[\nVar(bX) = b^{2}Var(X)\n\\]\nThe variance of a constant is 0: \\[\nVar(b) = 0\n\\]\nThe variance of a RV plus a constant is the variance of that random variable: \\[\nVar(X + b) = Var(X)\n\\]"
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#definition-1",
    "href": "lectures/01-Random-Variables/010-compile.html#definition-1",
    "title": "Random Variables",
    "section": "Definition",
    "text": "Definition\nThe covariance of two random variables \\((\\sigma_{XY})\\) is a measure of the linear association between those variables. For example, since people who are taller are generally heavier, we would say that the random variables height and weight have a positive covariance. On the other hand, if large values for one random variable tend to correspond to small values in the other, we would say the two variables have a negative covariance. Two variables are independent have a covariance of 0.\nThe formula is:\n\\[\n    Cov(X,Y) = \\sigma_{XY} = E[(X - \\mu_{X})(Y - \\mu_{Y})]\n\\]\nNotice that the covariance of a random variable \\(X\\) with itself is the variance of \\(X\\)"
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#rules-1",
    "href": "lectures/01-Random-Variables/010-compile.html#rules-1",
    "title": "Random Variables",
    "section": "Rules",
    "text": "Rules\nSome important rules about the way variance works. Let \\(X\\),\\(Y\\), and \\(Z\\) be random variables and let \\(b\\) be a constant.\n\nThe covariance of a random variable with a constant is 0 \\[\nCov(X,b) = 0\n\\]\nThe covariance of a random variable with itself is its variance: \\[\nCov(X,X) = Var(X)\n\\]\nConstants can come outside of the covariance: \\[\nCov(X,bY) = bCov(X,Y)\n\\]\nIf \\(Z\\) is a third random variable, we write: \\[\nCov(X,Y + Z) = Cov(X,Y) + Cov(X,Z)\n\\]"
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#definition-2",
    "href": "lectures/01-Random-Variables/010-compile.html#definition-2",
    "title": "Random Variables",
    "section": "Definition",
    "text": "Definition\nAn issue with covariance is that the covariance between two random variables depends on the units those variables are measured in. That’s where correlation comes in:\nCorrelation is another measure of linear association that has the benefit of being dimensionless because the units in the numerator cancel with the units in the denominator.\nIt is also the case that the correlation between two variables is always between -1 and 1. Where correlation = 1, the two variables have a perfect positive linear relationsihp, and when correlation = -1, the two variables have a perfect negative linear relationship.\nWe will use the greek letter \\(\\rho\\) (“rho”) to refer to the correlation between two RVs. The formula is:\n\\[\\begin{align*}\n    \\rho_{XY} =\n    \\dfrac{\n        \\sigma_{XY}\n    }{\n        \\sqrt{\\sigma_{X}^{2}\\sigma_{Y}^{2}}\n    }\n\\end{align*}\\]"
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#probabilities-of-continuous-rvs",
    "href": "lectures/01-Random-Variables/010-compile.html#probabilities-of-continuous-rvs",
    "title": "Random Variables",
    "section": "Probabilities of Continuous RVs",
    "text": "Probabilities of Continuous RVs\nWhen the variable can take on an infinite number of possible values, the probability it takes on any given value must be zero.\nThe variable takes so many values that we cannot count all possibilities, so the probability of any one particular value is zero.\nWe can use probability density functions (PDFs) to help describe continuous RVs of which there are many but we will give emphasis to two:\n\nUniform Distribution\nNormal Distribution"
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#distributions",
    "href": "lectures/01-Random-Variables/010-compile.html#distributions",
    "title": "Random Variables",
    "section": "Distributions",
    "text": "Distributions\nA distribution is a function that represents all outcomes of a random variable and the corresponding probabilities. It is:\n\nA summary that describes the spread of data points in a set\nEssential for making inferences and assumptions from data\n\nKey Takeaway: The shape of a distribution provides valuable information of the data"
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#uniform-distribution",
    "href": "lectures/01-Random-Variables/010-compile.html#uniform-distribution",
    "title": "Random Variables",
    "section": "Uniform Distribution",
    "text": "Uniform Distribution\nThe probability density function of a variable uniformly distributed between 0 and 2 is\n\\[\\begin{align*}\n    f(x) =\n        \\begin{cases}\n        \\dfrac{1}{2} & \\text{if } 0 \\leq x \\leq 2 \\\\\n        0   & \\text{otherwise }\n        \\end{cases}\n\\end{align*}\\]"
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#uniform-distribution-1",
    "href": "lectures/01-Random-Variables/010-compile.html#uniform-distribution-1",
    "title": "Random Variables",
    "section": "Uniform Distribution",
    "text": "Uniform Distribution\nBy definition, the area under \\(f(x)\\) is equal to 1.\nThe shaded area illustrates the probability of the event \\(1 \\leq X \\leq 1.5\\).\n\\[\n    P(1 \\leq X \\leq 1.5) = (1.5 - 1) \\times 0.5 = 0.25\n\\]"
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#normal-distribution",
    "href": "lectures/01-Random-Variables/010-compile.html#normal-distribution",
    "title": "Random Variables",
    "section": "Normal Distribution",
    "text": "Normal Distribution\nThis is commonly called a “bell curve”. It is:\n\nSymmetric: Mean and median occur at the same point (i.e. no skew)\nLow-probability events are in the tails\nHigh-probability events are near the center"
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#normal-distribution-1",
    "href": "lectures/01-Random-Variables/010-compile.html#normal-distribution-1",
    "title": "Random Variables",
    "section": "Normal Distribution",
    "text": "Normal Distribution\nThe shaded area illustrates the probability of the event \\(-2 \\leq X \\leq 2\\) occurring\n\nTo “find the area under the curve” we use integral calculus (or, in practice ).\n\n\\[\n    P(-2 \\leq X \\leq 2) \\approx 0.95\n\\]"
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#normal-distribution-2",
    "href": "lectures/01-Random-Variables/010-compile.html#normal-distribution-2",
    "title": "Random Variables",
    "section": "Normal Distribution",
    "text": "Normal Distribution\nContinuous distribution where \\(x_{i}\\) takes the value of any real number \\((\\mathbb{R})\\)\n\nThe domain spans the entire real line\nCentered on the distribution mean \\(\\mu\\)\n\nA couple of important rules to recall:\n\nThe probability that the random variable takes a value \\(x_{i}\\) is 0 for any \\(x_{i} \\in \\mathbb{R}\\)\nThe probability that the random variable falls between \\([x_{i},x_{j}]\\) range, where \\(x_{i} \\neq x_{j}\\), is the area under \\(p(x)\\) between those two values.\n\nThe area highlighted in the previous graph represents \\(p(x) = 0.95\\). The values \\({-1.96,1.95}\\) represent the 95% confidence interval for \\(\\mu\\)"
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#primary-differences-in-expected-values-by-rv-type",
    "href": "lectures/01-Random-Variables/010-compile.html#primary-differences-in-expected-values-by-rv-type",
    "title": "Random Variables",
    "section": "Primary Differences in Expected Values by RV Type",
    "text": "Primary Differences in Expected Values by RV Type\nTo find the expected value or variance of a continuous random variable instead of a discrete random variable, we just swap integrals for sums and the PDF \\(f(X)\\) for \\(p_{i}\\):\n\n\n\n\n\n\n\n\n\n\\(E[X]\\)\n\\(Var(X) = E[(X - \\mu_{X})^{2}]\\)\n\n\n\n\nDiscrete\n\\(\\sum_{i=1}^{n} x_{i}p_{i}\\)\n\\(\\sum_{i=1}^{n} (x_{i} - \\mu_{X})^{2} p_{i}\\)\n\n\nContinuous\n\\(\\int X f(X) dX\\)\n\\(\\int (X - \\mu_{x})^{2} f(X) dX\\)"
  },
  {
    "objectID": "lectures/01-Random-Variables/012-expected-values.html",
    "href": "lectures/01-Random-Variables/012-expected-values.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "The expected value of a random variable is its long-term average.\nWe will use the greek letter \\(\\mu\\) (“mew”) to refer to expected values. That is, we will say that the expected value of \\(X\\) is \\(\\mu_{X}\\), or equivalently, \\(E[X] = \\mu_{X}\\).\nIf the variable is discrete, you can calculate its expectation by taking the sum of all possible values of the random variable, each multiplied by their corresponding probabilities.\nWe write this as:\n\\[\n    E[X] = \\sum_{i} x_{i}p_{i}\n\\]\nWhere \\(x_{i}\\) is a potential outcome for \\(X\\) and \\(p_{i}\\) is the probability that outcome occurs"
  },
  {
    "objectID": "lectures/01-Random-Variables/012-expected-values.html#expected-values-of-discrete-random-variables",
    "href": "lectures/01-Random-Variables/012-expected-values.html#expected-values-of-discrete-random-variables",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "The expected value of a random variable is its long-term average.\nWe will use the greek letter \\(\\mu\\) (“mew”) to refer to expected values. That is, we will say that the expected value of \\(X\\) is \\(\\mu_{X}\\), or equivalently, \\(E[X] = \\mu_{X}\\).\nIf the variable is discrete, you can calculate its expectation by taking the sum of all possible values of the random variable, each multiplied by their corresponding probabilities.\nWe write this as:\n\\[\n    E[X] = \\sum_{i} x_{i}p_{i}\n\\]\nWhere \\(x_{i}\\) is a potential outcome for \\(X\\) and \\(p_{i}\\) is the probability that outcome occurs"
  },
  {
    "objectID": "lectures/01-Random-Variables/012-expected-values.html#expected-value-rules",
    "href": "lectures/01-Random-Variables/012-expected-values.html#expected-value-rules",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Expected Value Rules",
    "text": "Expected Value Rules\n\nHere are some very important math rules to know about the way expected values work. Let \\(X\\),\\(Y\\), and \\(Z\\) be random variables and let \\(b\\) be a constant.\n\n\nThe expectation of the sum of several RVs is the sum of their expectation: \\[\nE[X + Y + Z] = E[X] + E[Y] + E[Z]\n\\]\nConstants can pass outside of an expectation: \\[\nE[bX] = bE[X]\n\\]\nThe expected value of a constant is that constant: \\[\nE[b] = b\n\\]"
  },
  {
    "objectID": "lectures/01-Random-Variables/014-covariance.html",
    "href": "lectures/01-Random-Variables/014-covariance.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "The covariance of two random variables \\((\\sigma_{XY})\\) is a measure of the linear association between those variables. For example, since people who are taller are generally heavier, we would say that the random variables height and weight have a positive covariance. On the other hand, if large values for one random variable tend to correspond to small values in the other, we would say the two variables have a negative covariance. Two variables are independent have a covariance of 0.\nThe formula is:\n\\[\n    Cov(X,Y) = \\sigma_{XY} = E[(X - \\mu_{X})(Y - \\mu_{Y})]\n\\]\nNotice that the covariance of a random variable \\(X\\) with itself is the variance of \\(X\\)"
  },
  {
    "objectID": "lectures/01-Random-Variables/014-covariance.html#definition",
    "href": "lectures/01-Random-Variables/014-covariance.html#definition",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "The covariance of two random variables \\((\\sigma_{XY})\\) is a measure of the linear association between those variables. For example, since people who are taller are generally heavier, we would say that the random variables height and weight have a positive covariance. On the other hand, if large values for one random variable tend to correspond to small values in the other, we would say the two variables have a negative covariance. Two variables are independent have a covariance of 0.\nThe formula is:\n\\[\n    Cov(X,Y) = \\sigma_{XY} = E[(X - \\mu_{X})(Y - \\mu_{Y})]\n\\]\nNotice that the covariance of a random variable \\(X\\) with itself is the variance of \\(X\\)"
  },
  {
    "objectID": "lectures/01-Random-Variables/014-covariance.html#rules",
    "href": "lectures/01-Random-Variables/014-covariance.html#rules",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Rules",
    "text": "Rules\nSome important rules about the way variance works. Let \\(X\\),\\(Y\\), and \\(Z\\) be random variables and let \\(b\\) be a constant.\n\nThe covariance of a random variable with a constant is 0 \\[\nCov(X,b) = 0\n\\]\nThe covariance of a random variable with itself is its variance: \\[\nCov(X,X) = Var(X)\n\\]\nConstants can come outside of the covariance: \\[\nCov(X,bY) = bCov(X,Y)\n\\]\nIf \\(Z\\) is a third random variable, we write: \\[\nCov(X,Y + Z) = Cov(X,Y) + Cov(X,Z)\n\\]"
  },
  {
    "objectID": "lectures/01-Random-Variables/016-continuous-rvs.html",
    "href": "lectures/01-Random-Variables/016-continuous-rvs.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "When the variable can take on an infinite number of possible values, the probability it takes on any given value must be zero.\nThe variable takes so many values that we cannot count all possibilities, so the probability of any one particular value is zero.\nWe can use probability density functions (PDFs) to help describe continuous RVs of which there are many but we will give emphasis to two:\n\nUniform Distribution\nNormal Distribution"
  },
  {
    "objectID": "lectures/01-Random-Variables/016-continuous-rvs.html#probabilities-of-continuous-rvs",
    "href": "lectures/01-Random-Variables/016-continuous-rvs.html#probabilities-of-continuous-rvs",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "When the variable can take on an infinite number of possible values, the probability it takes on any given value must be zero.\nThe variable takes so many values that we cannot count all possibilities, so the probability of any one particular value is zero.\nWe can use probability density functions (PDFs) to help describe continuous RVs of which there are many but we will give emphasis to two:\n\nUniform Distribution\nNormal Distribution"
  },
  {
    "objectID": "lectures/01-Random-Variables/016-continuous-rvs.html#distributions",
    "href": "lectures/01-Random-Variables/016-continuous-rvs.html#distributions",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Distributions",
    "text": "Distributions\nA distribution is a function that represents all outcomes of a random variable and the corresponding probabilities. It is:\n\nA summary that describes the spread of data points in a set\nEssential for making inferences and assumptions from data\n\nKey Takeaway: The shape of a distribution provides valuable information of the data"
  },
  {
    "objectID": "lectures/01-Random-Variables/016-continuous-rvs.html#uniform-distribution",
    "href": "lectures/01-Random-Variables/016-continuous-rvs.html#uniform-distribution",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Uniform Distribution",
    "text": "Uniform Distribution\nThe probability density function of a variable uniformly distributed between 0 and 2 is\n\\[\\begin{align*}\n    f(x) =\n        \\begin{cases}\n        \\dfrac{1}{2} & \\text{if } 0 \\leq x \\leq 2 \\\\\n        0   & \\text{otherwise }\n        \\end{cases}\n\\end{align*}\\]"
  },
  {
    "objectID": "lectures/01-Random-Variables/016-continuous-rvs.html#uniform-distribution-1",
    "href": "lectures/01-Random-Variables/016-continuous-rvs.html#uniform-distribution-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Uniform Distribution",
    "text": "Uniform Distribution\nBy definition, the area under \\(f(x)\\) is equal to 1.\nThe shaded area illustrates the probability of the event \\(1 \\leq X \\leq 1.5\\).\n\\[\n    P(1 \\leq X \\leq 1.5) = (1.5 - 1) \\times 0.5 = 0.25\n\\]\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead."
  },
  {
    "objectID": "lectures/01-Random-Variables/016-continuous-rvs.html#normal-distribution",
    "href": "lectures/01-Random-Variables/016-continuous-rvs.html#normal-distribution",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Normal Distribution",
    "text": "Normal Distribution\nThis is commonly called a “bell curve”. It is:\n\nSymmetric: Mean and median occur at the same point (i.e. no skew)\nLow-probability events are in the tails\nHigh-probability events are near the center\n\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union"
  },
  {
    "objectID": "lectures/01-Random-Variables/016-continuous-rvs.html#normal-distribution-1",
    "href": "lectures/01-Random-Variables/016-continuous-rvs.html#normal-distribution-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Normal Distribution",
    "text": "Normal Distribution\nThe shaded area illustrates the probability of the event \\(-2 \\leq X \\leq 2\\) occurring\n\nTo “find the area under the curve” we use integral calculus (or, in practice ).\n\n\\[\n    P(-2 \\leq X \\leq 2) \\approx 0.95\n\\]"
  },
  {
    "objectID": "lectures/01-Random-Variables/016-continuous-rvs.html#normal-distribution-2",
    "href": "lectures/01-Random-Variables/016-continuous-rvs.html#normal-distribution-2",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Normal Distribution",
    "text": "Normal Distribution\nContinuous distribution where \\(x_{i}\\) takes the value of any real number \\((\\mathbb{R})\\)\n\nThe domain spans the entire real line\nCentered on the distribution mean \\(\\mu\\)\n\nA couple of important rules to recall:\n\nThe probability that the random variable takes a value \\(x_{i}\\) is 0 for any \\(x_{i} \\in \\mathbb{R}\\)\nThe probability that the random variable falls between \\([x_{i},x_{j}]\\) range, where \\(x_{i} \\neq x_{j}\\), is the area under \\(p(x)\\) between those two values.\n\nThe area highlighted in the previous graph represents \\(p(x) = 0.95\\). The values \\({-1.96,1.95}\\) represent the 95% confidence interval for \\(\\mu\\)"
  },
  {
    "objectID": "lectures/01-Random-Variables/016-continuous-rvs.html#primary-differences-in-expected-values-by-rv-type",
    "href": "lectures/01-Random-Variables/016-continuous-rvs.html#primary-differences-in-expected-values-by-rv-type",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Primary Differences in Expected Values by RV Type",
    "text": "Primary Differences in Expected Values by RV Type\nTo find the expected value or variance of a continuous random variable instead of a discrete random variable, we just swap integrals for sums and the PDF \\(f(X)\\) for \\(p_{i}\\):\n\n\n\n\n\n\n\n\n\n\\(E[X]\\)\n\\(Var(X) = E[(X - \\mu_{X})^{2}]\\)\n\n\n\n\nDiscrete\n\\(\\sum_{i=1}^{n} x_{i}p_{i}\\)\n\\(\\sum_{i=1}^{n} (x_{i} - \\mu_{X})^{2} p_{i}\\)\n\n\nContinuous\n\\(\\int X f(X) dX\\)\n\\(\\int (X - \\mu_{x})^{2} f(X) dX\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#why-estimate-things",
    "href": "lectures/03-Estimators-01/030-compile.html#why-estimate-things",
    "title": "Estimators",
    "section": "Why Estimate Things?",
    "text": "Why Estimate Things?\nWe estimate because we cannot measure everything\nSuppose we want to know the average height of the US population.\n\nWe only have a sample of 1 million Americans\n\nHow can we use these data to estimate the height of the population?\nWe will learn what we can do"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#estimators-1",
    "href": "lectures/03-Estimators-01/030-compile.html#estimators-1",
    "title": "Estimators",
    "section": "Estimators",
    "text": "Estimators\nLet’s define some concepts first:\nEstimand\n\nQuantity that is to be estimated in a statistical analysis\n\nEstimator\n\nA rule (or formula) for estimating an unknown population parameter given a sample of data\n\nEstimate\n\nA specific numerical value that we obtain from the smaple data by applying the estimator"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#estimators-example",
    "href": "lectures/03-Estimators-01/030-compile.html#estimators-example",
    "title": "Estimators",
    "section": "Estimators Example",
    "text": "Estimators Example\nSuppose we want to know the average height of the population in the US\n\nWe have a sample of 1 million Americans\n\nSo then we can identify our Estimand, Estimator, and Estimate\n\nEstimand: The population mean \\((\\mu)\\)\nEstimator: The sample mean \\((\\bar{X})\\)\n\n\\[\n    \\bar{X} = \\dfrac{1}{n} \\sum_{i=1}^{n} X_{i}\n\\]\n\nEstimate: The sample mean \\((\\hat{\\mu} = 5'6'')\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#properties-of-estimators",
    "href": "lectures/03-Estimators-01/030-compile.html#properties-of-estimators",
    "title": "Estimators",
    "section": "Properties of Estimators",
    "text": "Properties of Estimators\nThere are many ways to estimate things and they all have their benefits and costs.\nImagine we want to estimate an unknown parameter \\(\\mu\\), and we know the distributions of three competing estimators.\nWhich one should we use?"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#properties-of-estimators---unbiasedness",
    "href": "lectures/03-Estimators-01/030-compile.html#properties-of-estimators---unbiasedness",
    "title": "Estimators",
    "section": "Properties of Estimators - Unbiasedness",
    "text": "Properties of Estimators - Unbiasedness\nWe ask: What properties make an estimator reliable?\nAnswer (1): Unbiasedness\nOn average, does the estimator tend toward the correct value?\n\nFormally: Does the mean of the estimator’s distribution equal the parameter it estimates?\n\\[\n    \\text{Bias}_{\\mu} (\\hat{\\mu}) = E[\\hat{\\mu}] - \\mu\n\\]"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#properties-of-estimators-1",
    "href": "lectures/03-Estimators-01/030-compile.html#properties-of-estimators-1",
    "title": "Estimators",
    "section": "Properties of estimators",
    "text": "Properties of estimators\nQuestion What properties make an estimator reliable?\nA01: Unbiasedness\n\n\nUnbiased estimator: \\(E\\left[ \\hat{\\mu} \\right] = \\mu\\)\n\n\n\n\n\n\n\n\n\n\nBiased estimator \\(E\\left[ \\hat{\\mu} \\right] \\neq \\mu\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#properties-of-estimators---efficiency",
    "href": "lectures/03-Estimators-01/030-compile.html#properties-of-estimators---efficiency",
    "title": "Estimators",
    "section": "Properties of Estimators - Efficiency",
    "text": "Properties of Estimators - Efficiency\nWe ask: What properties make an estimator reliable?\nAnswer (1): Efficiency (Low Variance)\nThe central tendencies (means) of competing distribution are not the only things that matter. We also care about the variance of an estimator.\n\\[\n    Var(\\hat{\\mu}) = E \\left[ (\\hat{\\mu} - E[\\hat{\\mu}])^{2} \\right]\n\\]\nLower variance estimators estimate closer to the mean in each sample"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#properties-of-estimators---efficiency-1",
    "href": "lectures/03-Estimators-01/030-compile.html#properties-of-estimators---efficiency-1",
    "title": "Estimators",
    "section": "Properties of Estimators - Efficiency",
    "text": "Properties of Estimators - Efficiency\nImagine low variance to be similar to accuracy \\(\\rightarrow\\) tighter estimates"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#the-bias-variance-tradeoff",
    "href": "lectures/03-Estimators-01/030-compile.html#the-bias-variance-tradeoff",
    "title": "Estimators",
    "section": "The Bias-Variance Tradeoff",
    "text": "The Bias-Variance Tradeoff\nMuch like everything, there are tradeoffs from gaining one thing over another.\nShould we be willing to take a bit of bias to reduce the variance?\nIn economics/causal inference, we emphasize unbiasedness"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#unbiased-estimators",
    "href": "lectures/03-Estimators-01/030-compile.html#unbiased-estimators",
    "title": "Estimators",
    "section": "Unbiased estimators",
    "text": "Unbiased estimators\nIn addition to the sample mean, there are other unbiased estimators we will often use\n\n\nSample variance estimates the variance \\(\\sigma^{2}\\)\n\n\n\nSample covariance setimates the covariance \\(\\sigma_{XY}\\)\n\n\n\nSample correlation estimates the pop. correlation coefficient \\(\\rho_{XY}\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#sample-variance",
    "href": "lectures/03-Estimators-01/030-compile.html#sample-variance",
    "title": "Estimators",
    "section": "Sample Variance",
    "text": "Sample Variance\nThe sample variance, \\(S_{X}^{2}\\), is an unbiased estimator of the population variance\n\n\\[\n    S_{X}^{2} = \\dfrac{1}{n - 1} \\sum_{i=1}^{n} (X_{i} - \\bar{X})^{2}.\n\\]"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#sample-covariance",
    "href": "lectures/03-Estimators-01/030-compile.html#sample-covariance",
    "title": "Estimators",
    "section": "Sample Covariance",
    "text": "Sample Covariance\nThe sample covariance, \\(S_{XY}\\), is an unbiaed estimator of the population covariance\n\n\\[\n    S_{XY} = \\dfrac{1}{n-1} \\sum_{i=1}^{n} (X_{i} - \\bar{X})(Y_{i} - \\bar{Y}).\n\\]"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#sample-correlation",
    "href": "lectures/03-Estimators-01/030-compile.html#sample-correlation",
    "title": "Estimators",
    "section": "Sample Correlation",
    "text": "Sample Correlation\nSample correlation, \\(r_{XY}\\), is an unbiased estimator of the population correlation coefficient\n\n\\[\n    r_{XY} = \\dfrac{S_{XY}}{\\sqrt{S_{X}^{2}}\\sqrt{S_{Y}^{2}}}.\n\\]"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#sidebar-summation-rules",
    "href": "lectures/03-Estimators-01/030-compile.html#sidebar-summation-rules",
    "title": "Estimators",
    "section": "Sidebar: Summation Rules",
    "text": "Sidebar: Summation Rules\nBefore we continue, let’s cover some important rules we will need to derive some OLS things in the near future:\nSummations \\((\\sum)\\) have certain rules that we cannot violate and are important to hold in mind:\n\n\n\\(\\sum_{i=1}^{n} x_{i} = x_{1} + x_{2} + \\cdots + x_{n}\\)\n\n\n\n\\(\\sum_{i} x_{i} + y_{i} = \\sum_{i} x_{i} + \\sum_{i} y_{i}\\)\n\n\n\n\\(\\sum_{i} x_{i} y_{i} \\neq \\sum_{i} x_{i} \\sum_{i} y_{i}\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#summation-rules",
    "href": "lectures/03-Estimators-01/030-compile.html#summation-rules",
    "title": "Estimators",
    "section": "Summation Rules",
    "text": "Summation Rules\n\\[\\sum_{i=1}^{n} x_{i} = x_{1} + x_{2} + \\cdots + x_{n}\\]\nLet \\(x\\) be the set of \\({1,5,2}\\) \\((x: \\{1,5,2\\})\\) then using our summation rule we have:\n\\[\n    \\sum_{i} x_{i} = 1 + 5 + 2 = 8.\n\\]"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#summation-rules-1",
    "href": "lectures/03-Estimators-01/030-compile.html#summation-rules-1",
    "title": "Estimators",
    "section": "Summation Rules",
    "text": "Summation Rules\n\\[\\sum_{i} x_{i} + y_{i} = \\sum_{i} x_{i} + \\sum_{i} y_{i}\\]\nLet \\(x: \\{1,5,2\\}\\) and \\(y: \\{1,2,1\\}\\), then using our summation rule we have:\n\\[\\begin{align*}\n    \\sum_{i} x_{i} + y_{i} &= x_{1} + y_{1} + x_{2} + y_{2} + x_{3} + y_{3} \\\\\n                           &= x_{1} + x_{2} + x_{3} + y_{1} + y_{2} + y_{3} \\\\\n                           &= 1 + 5 + 2 + 1 + 2 + 1 \\\\\n                           &= 12\n\\end{align*}\\]"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#summation-rules-2",
    "href": "lectures/03-Estimators-01/030-compile.html#summation-rules-2",
    "title": "Estimators",
    "section": "Summation Rules",
    "text": "Summation Rules\n\\[\\sum_{i} x_{i} y_{i} \\neq \\sum_{i} x_{i} \\sum_{i} y_{i}\\]\nIf we expand \\(\\sum_{i} x_{i} y_{i} \\neq \\sum_{i} x_{i} \\sum_{i} y_{i}\\), we get:\n\n\\[\\begin{align*}\n    x_{1}y_{1} + x_{2}y_{2} + x_{3}y_{3} \\neq (x_{1} + x_{2} + x_{3})(y_{1} + y_{2} + y_{3})\n\\end{align*}\\]\n\nI’ll leave it to you to use the above numbers to show this holds"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#linear-model-estimators",
    "href": "lectures/03-Estimators-01/030-compile.html#linear-model-estimators",
    "title": "Estimators",
    "section": "Linear Model Estimators",
    "text": "Linear Model Estimators\nWe will spend the rest of the course exploring how to use Ordinary Least Squares (OLS) to fit a linear model like:\n\\[\n    y_{i} = \\beta_{0} + \\beta_{1}x_{i} + u_{i},\n\\]\nThat is, if we wanted to hypothesize that some random variable \\(Y\\) depends on another random variable \\(X\\) and that there is a linear relationship between then, \\(\\beta_{0}\\) and \\(\\beta_{1}\\) are the parameters which describe the nature of that relationship.\nGiven a sample of \\(X\\) and \\(Y\\), we will derive unbiased estimators for the intercept \\(\\beta_{0}\\) and slope \\(\\beta_{1}\\). Those estimators help us combine observations of \\(X\\) and \\(Y\\) to estimate underlying relationships between them."
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#the-linear-regression-model",
    "href": "lectures/03-Estimators-01/030-compile.html#the-linear-regression-model",
    "title": "Estimators",
    "section": "The Linear Regression Model",
    "text": "The Linear Regression Model\nWe can estimate the effect of \\(X\\) on \\(Y\\) by estimating the model:\n\\[\n    y_{i} = \\beta_{0} + \\beta_{1}x_{i} + u_{i},\n\\]\n\n\\(y_i\\) is the dependent variable\n\\(x_i\\) is the independent variable (continuous)\n\\(\\beta_0\\) is the intercept parameter. \\(E\\left[ {y_i | x_i=0} \\right] = \\beta_0\\)\n\\(\\beta_1\\) is the slope parameter, which under the correct causal setting represents marginal change in \\(x_i\\)’s effect on \\(y_i\\). \\(\\frac{\\partial y_i}{\\partial x_i} = \\beta_1\\)\n\\(u_i\\) is an Error Term including all other (omitted) factors affecting \\(y_i\\)."
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#the-error-term-u_i",
    "href": "lectures/03-Estimators-01/030-compile.html#the-error-term-u_i",
    "title": "Estimators",
    "section": "The Error Term \\(u_{i}\\)",
    "text": "The Error Term \\(u_{i}\\)\n\\(u_{i}\\) is quite special\nConsider the data generating process of variable \\(y_{i}\\),\n\n\\(u_{i}\\) captures all unobserved variables that explain variation in \\(y_{i}\\)\n\n\nSome error will exist in all models, no model is perfect.\n\nOur aim is to minimize error under a set of constraints\n\n\nError is the price we are willing to accept for a simplified model"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#the-error-term",
    "href": "lectures/03-Estimators-01/030-compile.html#the-error-term",
    "title": "Estimators",
    "section": "The Error Term",
    "text": "The Error Term\nFive items contribute to the existence of the disturbance term:\n\n1. Omission of independent variables\n\n\n\nOur description (model) of the relationship between \\(Y\\) and \\(X\\) is a simplification\nOther variables have been left out (omitted)"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#the-error-term-1",
    "href": "lectures/03-Estimators-01/030-compile.html#the-error-term-1",
    "title": "Estimators",
    "section": "The Error Term",
    "text": "The Error Term\nFive items contribute to the existence of the disturbance term:\n1. Omission of independent variables\n2. Aggregation of Variables\n\n\nMicroeconomic relationships are often summarized\nEx. Housing prices (\\(X\\)) are described by county-level median home value data"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#the-error-term-2",
    "href": "lectures/03-Estimators-01/030-compile.html#the-error-term-2",
    "title": "Estimators",
    "section": "The Error Term",
    "text": "The Error Term\nFive items contribute to the existence of the disturbance term:\n1. Omission of independent variables\n2. Aggregation of Variables\n3. Model misspecificiation\n\n\nModel structure is incorrectly specified\nEx. \\(Y\\) depends on the anticipated value of \\(X\\) in the previous period, not \\(X\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#the-error-term-3",
    "href": "lectures/03-Estimators-01/030-compile.html#the-error-term-3",
    "title": "Estimators",
    "section": "The Error Term",
    "text": "The Error Term\nFive items contribute to the existence of the disturbance term:\n1. Omission of independent variables\n2. Aggregation of Variables\n3. Model misspecificiation\n4. Functional misspecificiation\n\n\nThe functional relationship is specified incorrectly\nTrue relationship is nonlinear, not linear"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#the-error-term-4",
    "href": "lectures/03-Estimators-01/030-compile.html#the-error-term-4",
    "title": "Estimators",
    "section": "The Error Term",
    "text": "The Error Term\nFive items contribute to the existence of the disturbance term:\n1. Omission of independent variables\n2. Aggregation of Variables\n3. Model misspecificiation\n4. Functional misspecificiation\n5. Measurement error\n\n\nMeasurement of the variables in the data is just wrong\n\\(Y\\) or \\(X\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#the-error-term-5",
    "href": "lectures/03-Estimators-01/030-compile.html#the-error-term-5",
    "title": "Estimators",
    "section": "The Error Term",
    "text": "The Error Term\nFive items contribute to the existence of the disturbance term:\n1. Omission of independent variables\n2. Aggregation of Variables\n3. Model misspecificiation\n4. Functional misspecificiation\n5. Measurement error"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#running-a-regression-model",
    "href": "lectures/03-Estimators-01/030-compile.html#running-a-regression-model",
    "title": "Estimators",
    "section": "Running a Regression Model",
    "text": "Running a Regression Model\nUsing an estimator with data on \\(x_{i}\\) and \\(y_{i}\\), we can estimate a fitted regression line:\n\\[\n    \\hat{y}_{i} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1}x_{i}\n\\]\n\n\\(\\hat{y}_{i}\\) is the fitted value of \\(y_{i}\\)\n\\(\\hat{\\beta}_{0}\\) is the estimated intercept\n\\(\\hat{\\beta}_{1}\\) is the estimated slope\n\nThis procedure produces misses, known as residuals \\(y_{i} - \\hat{y_{i}}\\)\nLet’s look at an example of how this works"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#ex.-effect-of-police-on-crime",
    "href": "lectures/03-Estimators-01/030-compile.html#ex.-effect-of-police-on-crime",
    "title": "Estimators",
    "section": "Ex. Effect of police on crime",
    "text": "Ex. Effect of police on crime\n\nEmpirical question:\n\nDoes the number of on-campus police officers affect campus crime rates? If so, by how much?\n\n\n\n\nAlways plot your data first"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#ex.-effect-of-police-on-crime-1",
    "href": "lectures/03-Estimators-01/030-compile.html#ex.-effect-of-police-on-crime-1",
    "title": "Estimators",
    "section": "Ex. Effect of police on crime",
    "text": "Ex. Effect of police on crime\nThe scatter plot suggest that a weak positive relationship exists\n\nA sample correlation of 0.14 confirms this\n\n\n\nBut correlation does not imply causation\n\n\n\nLets estimate a statistical model"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#ex.-effect-of-police-on-crime-2",
    "href": "lectures/03-Estimators-01/030-compile.html#ex.-effect-of-police-on-crime-2",
    "title": "Estimators",
    "section": "Ex. Effect of police on crime",
    "text": "Ex. Effect of police on crime\nWe express the relationship between a dependent variable and an independent variable as linear:\n\\[\n{\\text{Crime}_i} = \\beta_0 + \\beta_1 \\text{Police}_i + u_i.\n\\]\n\n\\(\\beta_0\\) is the intercept or constant.\n\\(\\beta_1\\) is the slope coefficient.\n\\(u_i\\) is an error term or disturbance term."
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#ex.-effect-of-police-on-crime-3",
    "href": "lectures/03-Estimators-01/030-compile.html#ex.-effect-of-police-on-crime-3",
    "title": "Estimators",
    "section": "Ex. Effect of police on crime",
    "text": "Ex. Effect of police on crime\nThe intercept tells us the expected value of \\(\\text{Crime}_i\\) when \\(\\text{Police}_i = 0\\).\n\\[\n\\text{Crime}_i = {\\color{#BF616A} \\beta_{0}} + \\beta_1\\text{Police}_i + u_i\n\\]\nUsually not the focus of an analysis."
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#ex.-effect-of-police-on-crime-4",
    "href": "lectures/03-Estimators-01/030-compile.html#ex.-effect-of-police-on-crime-4",
    "title": "Estimators",
    "section": "Ex. Effect of police on crime",
    "text": "Ex. Effect of police on crime\nThe slope coefficient tells us the expected change in \\(\\text{Crime}_i\\) when \\(\\text{Police}_i\\) increases by one.\n\\[\n\\text{Crime}_i = \\beta_0 + {\\color{#BF616A} \\beta_1} \\text{Police}_i + u_i\n\\]\n“A one-unit increase in \\(\\text{Police}_i\\) is associated with a \\(\\color{#BF616A}{\\beta_1}\\)-unit increase in \\(\\text{Crime}_i\\).”\n\nInterpretation of this parameter is crucial\n\n\nUnder certain (strong) assumptions1, \\(\\color{#BF616A}{\\beta_1}\\) is the effect of \\(X_i\\) on \\(Y_i\\).\n\nOtherwise, it’s the association of \\(X_i\\) with \\(Y_i\\).\n\n\nAssumptions regarding the error term"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#ex.-effect-of-police-on-crime-5",
    "href": "lectures/03-Estimators-01/030-compile.html#ex.-effect-of-police-on-crime-5",
    "title": "Estimators",
    "section": "Ex. Effect of police on crime",
    "text": "Ex. Effect of police on crime\nThe error term reminds us that \\(\\text{Police}_i\\) does not perfectly explain \\(Y_i\\).\n\\[\n\\text{Crime}_i = \\beta_0 + \\beta_1\\text{Police}_i + {\\color{#BF616A} u_i}\n\\]\nRepresents all other factors that explain \\(\\text{Crime}_i\\).\n\nUseful mnemonic: pretend that \\(u\\) stands for “unobserved” or “unexplained.”"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#ex.-effect-of-police-on-crime-6",
    "href": "lectures/03-Estimators-01/030-compile.html#ex.-effect-of-police-on-crime-6",
    "title": "Estimators",
    "section": "Ex. Effect of police on crime",
    "text": "Ex. Effect of police on crime\nHow might we apply the simple linear regression model to our question about the effect of on-campus police on campus crime?\n\\[\n\\text{Crime}_i = \\beta_0 + \\beta_1\\text{Police}_i + u_i.\n\\]\n\n\\(\\beta_0\\) is the crime rate for colleges without police.\n\\(\\beta_1\\) is the increase in the crime rate for an additional police officer per 1000 students."
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#ex.-effect-of-police-on-crime-7",
    "href": "lectures/03-Estimators-01/030-compile.html#ex.-effect-of-police-on-crime-7",
    "title": "Estimators",
    "section": "Ex. Effect of police on crime",
    "text": "Ex. Effect of police on crime\nHow might we apply the simple linear regression model to our question?\n\\[\n\\text{Crime}_i = \\beta_0 + \\beta_1\\text{Police}_i + u_i\n\\]\n\\(\\beta_0\\) and \\(\\beta_1\\) are the unobserved population parameters we want\n\n\nWe estimate\n\n\\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) generate predictions of \\(\\text{Crime}_i\\) called \\(\\widehat{\\text{Crime}_i}\\).\nWe call the predictions of the dependent variable fitted values.\n\n\n\n\nTogether, these trace a line: \\(\\widehat{\\text{Crime}_i} = \\hat{\\beta_0} + \\hat{\\beta_1}\\text{Police}_i\\)."
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#section-1",
    "href": "lectures/03-Estimators-01/030-compile.html#section-1",
    "title": "Estimators",
    "section": "",
    "text": "So, the question becomes, how do I pick \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#section-2",
    "href": "lectures/03-Estimators-01/030-compile.html#section-2",
    "title": "Estimators",
    "section": "",
    "text": "Let’s take some guesses: \\(\\hat{\\beta_0} = 60\\) and \\(\\hat{\\beta}_{1} = -7\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#section-3",
    "href": "lectures/03-Estimators-01/030-compile.html#section-3",
    "title": "Estimators",
    "section": "",
    "text": "Let’s take some guesses: \\(\\hat{\\beta_0} = 30\\) and \\(\\hat{\\beta}_{1} = 0\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#section-4",
    "href": "lectures/03-Estimators-01/030-compile.html#section-4",
    "title": "Estimators",
    "section": "",
    "text": "Let’s take some guesses: \\(\\hat{\\beta_0} = 15.6\\) and \\(\\hat{\\beta}_{1} = 7.94\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#residuals",
    "href": "lectures/03-Estimators-01/030-compile.html#residuals",
    "title": "Estimators",
    "section": "Residuals",
    "text": "Residuals\nUsing \\(\\hat{\\beta}_{0}\\) and \\(\\hat{\\beta}_{1}\\) to make \\(\\hat{y}_{i}\\) generates misses.\n\n\n\n \\(\\hat{\\beta_0} = 60 \\;\\) Guess\n\n \\(\\hat{\\beta_0} = 30 \\;\\) Guess\n\n \\(\\hat{\\beta_0} = 15 \\;\\) Guess"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#residuals-sum-of-squares-rss",
    "href": "lectures/03-Estimators-01/030-compile.html#residuals-sum-of-squares-rss",
    "title": "Estimators",
    "section": "Residuals Sum of Squares (RSS)",
    "text": "Residuals Sum of Squares (RSS)\nWhat if we picked an estimator that minimizes the residuals?\nWhy do we not minimize:\n\\[\n    \\sum_{i=1}^{n} \\hat{u}_{i}^{2}\n\\]\nso that the estimator makes fewer big misses?\nThis estimator, the residual sum of squares (RSS), is convenient because squared numbers are never negative so we can minimze an absolut sum of the residuals\nRSS will give bigger penalties to bigger residuals"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#minimizing-rss",
    "href": "lectures/03-Estimators-01/030-compile.html#minimizing-rss",
    "title": "Estimators",
    "section": "Minimizing RSS",
    "text": "Minimizing RSS\nWe could test thousands of guesses of \\(\\beta_0\\) and \\(\\beta_1\\) an pick the pair the has the smallest RSS\nWe could painstakingly do that, and eventually figure out which one fits best.\nOr… We could just do a little math"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#ols",
    "href": "lectures/03-Estimators-01/030-compile.html#ols",
    "title": "Estimators",
    "section": "OLS",
    "text": "OLS\nThe OLS Estimator chooses the parameters \\(\\hat{\\beta}_{0}\\) and \\(\\hat{\\beta}_{1}\\) that minimize the Residual Sum of Squares (RSS)\n\\[\n    \\min_{\\hat{\\beta}_{0},\\hat{\\beta}_{1}} \\sum_{i=1}^{n} \\hat{u}_{i}^{2}\n\\]\nThis is why we call the estimator ordinary least squares\nRecall that residuals are given by \\(y_{i} - \\hat{y}_{i}\\) and that:\n\\[\n    \\hat{y}_{i} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1} x_{i}\n\\]\nThen\n\\[\n    u_{i} = y_{i} - \\hat{\\beta}_{0} + \\hat{\\beta}_{1} x_{i}\n\\]"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#ols-calculus",
    "href": "lectures/03-Estimators-01/030-compile.html#ols-calculus",
    "title": "Estimators",
    "section": "OLS & Calculus",
    "text": "OLS & Calculus\nWe can find our choices \\(\\hat{\\beta}_{0}\\) and \\(\\hat{\\beta}_{1}\\) to minimize our residuals using calculus\nA minimization problem is essentially the same as an optimization problem where we find the point at which our choices have a slope of zero\nTo begin, let’s properly write out our minimization problem:\n\\[\n    \\min_{\\hat{\\beta}_{0}, \\hat{\\beta}_{1}} \\;\\; \\sum_{i} u_{i}^{2}\n\\]\n\\[\n    \\min_{\\hat{\\beta}_{0}, \\hat{\\beta}_{1}} \\; \\sum_{i} (y_{i} - \\hat{y}_{i})^{2}\n\\]\n\\[\n    \\min_{\\hat{\\beta}_{0}, \\hat{\\beta}_{1}} \\; \\sum_{i} (y_{i} - \\hat{\\beta}_{0} - \\hat{\\beta}_{1}x_{i}) (y_{i} - \\hat{\\beta}_{0} - \\hat{\\beta}_{1}x_{i})\n\\]"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#ols-calculus-1",
    "href": "lectures/03-Estimators-01/030-compile.html#ols-calculus-1",
    "title": "Estimators",
    "section": "OLS & Calculus",
    "text": "OLS & Calculus\nThe calculus we’ll use is by finding the derivatives of the function with respect to \\(\\hat{\\beta}_{0}\\) and \\(\\hat{\\beta}_{1}\\).\nIt’s a lot of algebra but it is simple math, just a lot of it:\n\n\\[\\begin{align*}\n    \\min_{\\hat{\\beta}_{0}, \\hat{\\beta}_{1}} &\\;\n    \\sum_{i} y_{i}^{2} - \\hat{\\beta}_{0}y_{i} - \\hat{\\beta}_{1}x_{i}y_{i} - \\hat{\\beta}_{0}y_{i} + \\hat{\\beta}_{0}^{2} + \\hat{\\beta}_{0}\\hat{\\beta}_{1}x_{i} - \\hat{\\beta}_{1}x_{i}y_{i} + \\hat{\\beta}_{0}\\hat{\\beta}_{1}x_{i} + \\hat{\\beta}_{1}^{2}x_{i}^{2} \\\\\n    \\min_{\\hat{\\beta}_{0}, \\hat{\\beta}_{1}} &\\;\n    \\sum_{i} y_{i}^{2} - 2 \\hat{\\beta}_{0}y_{i} + \\hat{\\beta}_{0}^{2} - 2 \\hat{\\beta}_{1}x_{i}y_{i} + 2\\hat{\\beta}_{0}\\hat{\\beta}_{1}x_{i} + \\hat{\\beta}_{1}^{2}x_{i}^{2}\n\\end{align*}\\]\n\nThen, we take partial derivatives over our choices \\(\\hat{\\beta}_{0}\\) and \\(\\hat{\\beta}_{1}\\) to figure the best choices.\nThese are called First Order Conditions (FOCs)"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#ols-calculus-2",
    "href": "lectures/03-Estimators-01/030-compile.html#ols-calculus-2",
    "title": "Estimators",
    "section": "OLS & Calculus",
    "text": "OLS & Calculus\nTo find our choices, we find the partial derivative and set it equal to 0\nFor our intercept \\(\\hat{\\beta}_{0}\\):\n\n\\[\\begin{align*}\n    &\\dfrac{\\partial u_{i}}{\\partial \\hat{\\beta}_{0}} = 0 \\\\\n    \\sum_{i} -2y_{i} + &2\\hat{\\beta}_{0} + 2\\hat{\\beta}_{1}x_{i} = 0\n\\end{align*}\\]\n\nFor our slope \\(\\hat{\\beta}_{1}\\):\n\n\\[\\begin{align*}\n    &\\dfrac{\\partial u_{i}}{\\partial \\hat{\\beta}_{1}} = 0 \\\\\n    \\sum_{i} -2x_{i}y_{i} + &2\\hat{\\beta}_{0}x_{i} + 2\\hat{\\beta}_{1}x_{i}^{2} = 0\n\\end{align*}\\]"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#hatbeta_0-derivation",
    "href": "lectures/03-Estimators-01/030-compile.html#hatbeta_0-derivation",
    "title": "Estimators",
    "section": "\\(\\hat{\\beta}_{0}\\) Derivation",
    "text": "\\(\\hat{\\beta}_{0}\\) Derivation\n\\[\n    \\sum_{i} -2y_{i} + 2\\hat{\\beta}_{0} + 2\\hat{\\beta}_{1}x_{i} = 0\n\\]\nOur task is to find solve the above for \\(\\hat{\\beta}_{0}\\):"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#hatbeta_1-derivation",
    "href": "lectures/03-Estimators-01/030-compile.html#hatbeta_1-derivation",
    "title": "Estimators",
    "section": "\\(\\hat{\\beta}_{1}\\) Derivation",
    "text": "\\(\\hat{\\beta}_{1}\\) Derivation\n\\[\n    \\sum_{i} -2x_{i}y_{i} + 2\\hat{\\beta}_{0}x_{i} + 2\\hat{\\beta}_{1}x_{i}^{2} = 0\n\\]\nOur task is to find solve the above for \\(\\hat{\\beta}_{1}\\):"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#ols-formulas",
    "href": "lectures/03-Estimators-01/030-compile.html#ols-formulas",
    "title": "Estimators",
    "section": "OLS Formulas",
    "text": "OLS Formulas\n\nIntercept\n\\[\n    \\hat{\\beta}_{0} = \\bar{y} - \\hat{\\beta}_{1}\\bar{x}\n\\]\n\nSlope Coefficient\n\\[\n    \\hat{\\beta}_{1} =\n    \\dfrac{\n        \\sum_{i=1}^{n} (y_{i} - \\bar{y})(x_{i} - \\bar{x})\n    }{\n        \\sum_{i=1}^{n} (x_{i} - \\bar{x})^{2}\n    }\n\\]\nThese may look slightly different to my derivation. Part of your assignments is to bridge the gap."
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#interpretation",
    "href": "lectures/03-Estimators-01/030-compile.html#interpretation",
    "title": "Estimators",
    "section": "Interpretation",
    "text": "Interpretation\nThere are two stages of interpretation of a regression equation\n\nInterpret regression estimates into words\nDeciding whether this interpretation should be taken at face value\n\n\nBoth stages are important, but for now, we will focus on the first\nLet’s revisit our crime example"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#ex-effect-of-police-on-crime",
    "href": "lectures/03-Estimators-01/030-compile.html#ex-effect-of-police-on-crime",
    "title": "Estimators",
    "section": "Ex: Effect of Police on Crime",
    "text": "Ex: Effect of Police on Crime\nUsing the OLS formulas, we get \\(\\hat{\\beta}_{0} = 18.41\\) and \\(\\hat{\\beta}_{1} = 1.76\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#coefficient-interpretation-1",
    "href": "lectures/03-Estimators-01/030-compile.html#coefficient-interpretation-1",
    "title": "Estimators",
    "section": "Coefficient Interpretation",
    "text": "Coefficient Interpretation\nHow do I interpret \\(\\hat{\\beta}_{0} = 18.41\\) and \\(\\hat{\\beta}_{1} = 1.76\\)?\nThe general interpration of the intercept is the estimated value of \\(y_{i}\\) when \\(x_{i} = 0\\)\nAnd the general interpretation of the slope parameter is the estimated change \\(y_{i}\\) for the marginal increase \\(x_{i}\\)\n\nFirst, it is important to understand the units:\n\n\\(\\widehat{\\text{Crime}}_{i}\\) is measured as a crime rate, the number of crimes per 1,000 students on campus\n\\(\\text{Police}_{i}\\) is also measured as a rate, the number of police officers per 1,000 students on campus"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#coefficient-interpretation-2",
    "href": "lectures/03-Estimators-01/030-compile.html#coefficient-interpretation-2",
    "title": "Estimators",
    "section": "Coefficient Interpretation",
    "text": "Coefficient Interpretation\nUsing OLS gives us the fitted line\n\\[\n\\widehat{\\text{Crime}_i} = \\hat{\\beta}_1 + \\hat{\\beta}_2\\text{Police}_i.\n\\]\nWhat does \\(\\hat{\\beta_0}\\) = \\(18.41\\) tell us? Without any police on campus, the crime rate is \\(18.41\\) per 1,000 people on campus\n\nWhat does \\(\\hat{\\beta_1}\\) = \\(1.76\\) tell us? For each additional police officer per 1,000, there is an associated increase in the crime rate by \\(1.76\\) crimes per 1,000 people on campus.\n\n\nDoes this mean that police cause crime? Probably not.\nThis is where deciding if the interpretation should be taken at face value. It now becomes your job to bring reason to the values."
  },
  {
    "objectID": "lectures/03-Estimators-01/032-linear-model.html",
    "href": "lectures/03-Estimators-01/032-linear-model.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Before we continue, let’s cover some important rules we will need to derive some OLS things in the near future:\nSummations \\((\\sum)\\) have certain rules that we cannot violate and are important to hold in mind:\n\n\n\\(\\sum_{i=1}^{n} x_{i} = x_{1} + x_{2} + \\cdots + x_{n}\\)\n\n\n\n\\(\\sum_{i} x_{i} + y_{i} = \\sum_{i} x_{i} + \\sum_{i} y_{i}\\)\n\n\n\n\\(\\sum_{i} x_{i} y_{i} \\neq \\sum_{i} x_{i} \\sum_{i} y_{i}\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/032-linear-model.html#sidebar-summation-rules",
    "href": "lectures/03-Estimators-01/032-linear-model.html#sidebar-summation-rules",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Before we continue, let’s cover some important rules we will need to derive some OLS things in the near future:\nSummations \\((\\sum)\\) have certain rules that we cannot violate and are important to hold in mind:\n\n\n\\(\\sum_{i=1}^{n} x_{i} = x_{1} + x_{2} + \\cdots + x_{n}\\)\n\n\n\n\\(\\sum_{i} x_{i} + y_{i} = \\sum_{i} x_{i} + \\sum_{i} y_{i}\\)\n\n\n\n\\(\\sum_{i} x_{i} y_{i} \\neq \\sum_{i} x_{i} \\sum_{i} y_{i}\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/032-linear-model.html#summation-rules",
    "href": "lectures/03-Estimators-01/032-linear-model.html#summation-rules",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Summation Rules",
    "text": "Summation Rules\n\\[\\sum_{i=1}^{n} x_{i} = x_{1} + x_{2} + \\cdots + x_{n}\\]\nLet \\(x\\) be the set of \\({1,5,2}\\) \\((x: \\{1,5,2\\})\\) then using our summation rule we have:\n\\[\n    \\sum_{i} x_{i} = 1 + 5 + 2 = 8.\n\\]"
  },
  {
    "objectID": "lectures/03-Estimators-01/032-linear-model.html#summation-rules-1",
    "href": "lectures/03-Estimators-01/032-linear-model.html#summation-rules-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Summation Rules",
    "text": "Summation Rules\n\\[\\sum_{i} x_{i} + y_{i} = \\sum_{i} x_{i} + \\sum_{i} y_{i}\\]\nLet \\(x: \\{1,5,2\\}\\) and \\(y: \\{1,2,1\\}\\), then using our summation rule we have:\n\\[\\begin{align*}\n    \\sum_{i} x_{i} + y_{i} &= x_{1} + y_{1} + x_{2} + y_{2} + x_{3} + y_{3} \\\\\n                           &= x_{1} + x_{2} + x_{3} + y_{1} + y_{2} + y_{3} \\\\\n                           &= 1 + 5 + 2 + 1 + 2 + 1 \\\\\n                           &= 12\n\\end{align*}\\]"
  },
  {
    "objectID": "lectures/03-Estimators-01/032-linear-model.html#summation-rules-2",
    "href": "lectures/03-Estimators-01/032-linear-model.html#summation-rules-2",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Summation Rules",
    "text": "Summation Rules\n\\[\\sum_{i} x_{i} y_{i} \\neq \\sum_{i} x_{i} \\sum_{i} y_{i}\\]\nIf we expand \\(\\sum_{i} x_{i} y_{i} \\neq \\sum_{i} x_{i} \\sum_{i} y_{i}\\), we get:\n\n\\[\\begin{align*}\n    x_{1}y_{1} + x_{2}y_{2} + x_{3}y_{3} \\neq (x_{1} + x_{2} + x_{3})(y_{1} + y_{2} + y_{3})\n\\end{align*}\\]\n\nI’ll leave it to you to use the above numbers to show this holds"
  },
  {
    "objectID": "lectures/03-Estimators-01/032-linear-model.html#linear-model-estimators",
    "href": "lectures/03-Estimators-01/032-linear-model.html#linear-model-estimators",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Linear Model Estimators",
    "text": "Linear Model Estimators\nWe will spend the rest of the course exploring how to use Ordinary Least Squares (OLS) to fit a linear model like:\n\\[\n    y_{i} = \\beta_{0} + \\beta_{1}x_{i} + u_{i},\n\\]\nThat is, if we wanted to hypothesize that some random variable \\(Y\\) depends on another random variable \\(X\\) and that there is a linear relationship between then, \\(\\beta_{0}\\) and \\(\\beta_{1}\\) are the parameters which describe the nature of that relationship.\nGiven a sample of \\(X\\) and \\(Y\\), we will derive unbiased estimators for the intercept \\(\\beta_{0}\\) and slope \\(\\beta_{1}\\). Those estimators help us combine observations of \\(X\\) and \\(Y\\) to estimate underlying relationships between them."
  },
  {
    "objectID": "lectures/03-Estimators-01/032-linear-model.html#the-linear-regression-model",
    "href": "lectures/03-Estimators-01/032-linear-model.html#the-linear-regression-model",
    "title": "EC 320 - Intro. Econometrics",
    "section": "The Linear Regression Model",
    "text": "The Linear Regression Model\nWe can estimate the effect of \\(X\\) on \\(Y\\) by estimating the model:\n\\[\n    y_{i} = \\beta_{0} + \\beta_{1}x_{i} + u_{i},\n\\]\n\n\\(y_i\\) is the dependent variable\n\\(x_i\\) is the independent variable (continuous)\n\\(\\beta_0\\) is the intercept parameter. \\(E\\left[ {y_i | x_i=0} \\right] = \\beta_0\\)\n\\(\\beta_1\\) is the slope parameter, which under the correct causal setting represents marginal change in \\(x_i\\)’s effect on \\(y_i\\). \\(\\frac{\\partial y_i}{\\partial x_i} = \\beta_1\\)\n\\(u_i\\) is an Error Term including all other (omitted) factors affecting \\(y_i\\)."
  },
  {
    "objectID": "lectures/03-Estimators-01/032-linear-model.html#the-error-term-u_i",
    "href": "lectures/03-Estimators-01/032-linear-model.html#the-error-term-u_i",
    "title": "EC 320 - Intro. Econometrics",
    "section": "The Error Term \\(u_{i}\\)",
    "text": "The Error Term \\(u_{i}\\)\n\\(u_{i}\\) is quite special\nConsider the data generating process of variable \\(y_{i}\\),\n\n\\(u_{i}\\) captures all unobserved variables that explain variation in \\(y_{i}\\)\n\n\nSome error will exist in all models, no model is perfect.\n\nOur aim is to minimize error under a set of constraints\n\n\nError is the price we are willing to accept for a simplified model"
  },
  {
    "objectID": "lectures/03-Estimators-01/032-linear-model.html#the-error-term",
    "href": "lectures/03-Estimators-01/032-linear-model.html#the-error-term",
    "title": "EC 320 - Intro. Econometrics",
    "section": "The Error Term",
    "text": "The Error Term\nFive items contribute to the existence of the disturbance term:\n. . .\n1. Omission of independent variables\n. . .\n\nOur description (model) of the relationship between \\(Y\\) and \\(X\\) is a simplification\nOther variables have been left out (omitted)"
  },
  {
    "objectID": "lectures/03-Estimators-01/032-linear-model.html#the-error-term-1",
    "href": "lectures/03-Estimators-01/032-linear-model.html#the-error-term-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "The Error Term",
    "text": "The Error Term\nFive items contribute to the existence of the disturbance term:\n1. Omission of independent variables\n2. Aggregation of Variables\n. . .\n\nMicroeconomic relationships are often summarized\nEx. Housing prices (\\(X\\)) are described by county-level median home value data"
  },
  {
    "objectID": "lectures/03-Estimators-01/032-linear-model.html#the-error-term-2",
    "href": "lectures/03-Estimators-01/032-linear-model.html#the-error-term-2",
    "title": "EC 320 - Intro. Econometrics",
    "section": "The Error Term",
    "text": "The Error Term\nFive items contribute to the existence of the disturbance term:\n1. Omission of independent variables\n2. Aggregation of Variables\n3. Model misspecificiation\n. . .\n\nModel structure is incorrectly specified\nEx. \\(Y\\) depends on the anticipated value of \\(X\\) in the previous period, not \\(X\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/032-linear-model.html#the-error-term-3",
    "href": "lectures/03-Estimators-01/032-linear-model.html#the-error-term-3",
    "title": "EC 320 - Intro. Econometrics",
    "section": "The Error Term",
    "text": "The Error Term\nFive items contribute to the existence of the disturbance term:\n1. Omission of independent variables\n2. Aggregation of Variables\n3. Model misspecificiation\n4. Functional misspecificiation\n. . .\n\nThe functional relationship is specified incorrectly\nTrue relationship is nonlinear, not linear"
  },
  {
    "objectID": "lectures/03-Estimators-01/032-linear-model.html#the-error-term-4",
    "href": "lectures/03-Estimators-01/032-linear-model.html#the-error-term-4",
    "title": "EC 320 - Intro. Econometrics",
    "section": "The Error Term",
    "text": "The Error Term\nFive items contribute to the existence of the disturbance term:\n1. Omission of independent variables\n2. Aggregation of Variables\n3. Model misspecificiation\n4. Functional misspecificiation\n5. Measurement error\n. . .\n\nMeasurement of the variables in the data is just wrong\n\\(Y\\) or \\(X\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/032-linear-model.html#the-error-term-5",
    "href": "lectures/03-Estimators-01/032-linear-model.html#the-error-term-5",
    "title": "EC 320 - Intro. Econometrics",
    "section": "The Error Term",
    "text": "The Error Term\nFive items contribute to the existence of the disturbance term:\n1. Omission of independent variables\n2. Aggregation of Variables\n3. Model misspecificiation\n4. Functional misspecificiation\n5. Measurement error"
  },
  {
    "objectID": "lectures/03-Estimators-01/032-linear-model.html#running-a-regression-model",
    "href": "lectures/03-Estimators-01/032-linear-model.html#running-a-regression-model",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Running a Regression Model",
    "text": "Running a Regression Model\nUsing an estimator with data on \\(x_{i}\\) and \\(y_{i}\\), we can estimate a fitted regression line:\n\\[\n    \\hat{y}_{i} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1}x_{i}\n\\]\n\n\\(\\hat{y}_{i}\\) is the fitted value of \\(y_{i}\\)\n\\(\\hat{\\beta}_{0}\\) is the estimated intercept\n\\(\\hat{\\beta}_{1}\\) is the estimated slope\n\nThis procedure produces misses, known as residuals \\(y_{i} - \\hat{y_{i}}\\)\nLet’s look at an example of how this works"
  },
  {
    "objectID": "lectures/03-Estimators-01/034-ols.html",
    "href": "lectures/03-Estimators-01/034-ols.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "The OLS Estimator chooses the parameters \\(\\hat{\\beta}_{0}\\) and \\(\\hat{\\beta}_{1}\\) that minimize the Residual Sum of Squares (RSS)\n\\[\n    \\min_{\\hat{\\beta}_{0},\\hat{\\beta}_{1}} \\sum_{i=1}^{n} \\hat{u}_{i}^{2}\n\\]\nThis is why we call the estimator ordinary least squares\nRecall that residuals are given by \\(y_{i} - \\hat{y}_{i}\\) and that:\n\\[\n    \\hat{y}_{i} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1} x_{i}\n\\]\nThen\n\\[\n    u_{i} = y_{i} - \\hat{\\beta}_{0} + \\hat{\\beta}_{1} x_{i}\n\\]"
  },
  {
    "objectID": "lectures/03-Estimators-01/034-ols.html#ols",
    "href": "lectures/03-Estimators-01/034-ols.html#ols",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "The OLS Estimator chooses the parameters \\(\\hat{\\beta}_{0}\\) and \\(\\hat{\\beta}_{1}\\) that minimize the Residual Sum of Squares (RSS)\n\\[\n    \\min_{\\hat{\\beta}_{0},\\hat{\\beta}_{1}} \\sum_{i=1}^{n} \\hat{u}_{i}^{2}\n\\]\nThis is why we call the estimator ordinary least squares\nRecall that residuals are given by \\(y_{i} - \\hat{y}_{i}\\) and that:\n\\[\n    \\hat{y}_{i} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1} x_{i}\n\\]\nThen\n\\[\n    u_{i} = y_{i} - \\hat{\\beta}_{0} + \\hat{\\beta}_{1} x_{i}\n\\]"
  },
  {
    "objectID": "lectures/03-Estimators-01/034-ols.html#ols-calculus",
    "href": "lectures/03-Estimators-01/034-ols.html#ols-calculus",
    "title": "EC 320 - Intro. Econometrics",
    "section": "OLS & Calculus",
    "text": "OLS & Calculus\nWe can find our choices \\(\\hat{\\beta}_{0}\\) and \\(\\hat{\\beta}_{1}\\) to minimize our residuals using calculus\nA minimization problem is essentially the same as an optimization problem where we find the point at which our choices have a slope of zero\nTo begin, let’s properly write out our minimization problem:\n\\[\n    \\min_{\\hat{\\beta}_{0}, \\hat{\\beta}_{1}} \\;\\; \\sum_{i} u_{i}^{2}\n\\]\n\\[\n    \\min_{\\hat{\\beta}_{0}, \\hat{\\beta}_{1}} \\; \\sum_{i} (y_{i} - \\hat{y}_{i})^{2}\n\\]\n\\[\n    \\min_{\\hat{\\beta}_{0}, \\hat{\\beta}_{1}} \\; \\sum_{i} (y_{i} - \\hat{\\beta}_{0} - \\hat{\\beta}_{1}x_{i}) (y_{i} - \\hat{\\beta}_{0} - \\hat{\\beta}_{1}x_{i})\n\\]"
  },
  {
    "objectID": "lectures/03-Estimators-01/034-ols.html#ols-calculus-1",
    "href": "lectures/03-Estimators-01/034-ols.html#ols-calculus-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "OLS & Calculus",
    "text": "OLS & Calculus\nThe calculus we’ll use is by finding the derivatives of the function with respect to \\(\\hat{\\beta}_{0}\\) and \\(\\hat{\\beta}_{1}\\).\nIt’s a lot of algebra but it is simple math, just a lot of it:\n\n\\[\\begin{align*}\n    \\min_{\\hat{\\beta}_{0}, \\hat{\\beta}_{1}} &\\;\n    \\sum_{i} y_{i}^{2} - \\hat{\\beta}_{0}y_{i} - \\hat{\\beta}_{1}x_{i}y_{i} - \\hat{\\beta}_{0}y_{i} + \\hat{\\beta}_{0}^{2} + \\hat{\\beta}_{0}\\hat{\\beta}_{1}x_{i} - \\hat{\\beta}_{1}x_{i}y_{i} + \\hat{\\beta}_{0}\\hat{\\beta}_{1}x_{i} + \\hat{\\beta}_{1}^{2}x_{i}^{2} \\\\\n    \\min_{\\hat{\\beta}_{0}, \\hat{\\beta}_{1}} &\\;\n    \\sum_{i} y_{i}^{2} - 2 \\hat{\\beta}_{0}y_{i} + \\hat{\\beta}_{0}^{2} - 2 \\hat{\\beta}_{1}x_{i}y_{i} + 2\\hat{\\beta}_{0}\\hat{\\beta}_{1}x_{i} + \\hat{\\beta}_{1}^{2}x_{i}^{2}\n\\end{align*}\\]\n\nThen, we take partial derivatives over our choices \\(\\hat{\\beta}_{0}\\) and \\(\\hat{\\beta}_{1}\\) to figure the best choices.\nThese are called First Order Conditions (FOCs)"
  },
  {
    "objectID": "lectures/03-Estimators-01/034-ols.html#ols-calculus-2",
    "href": "lectures/03-Estimators-01/034-ols.html#ols-calculus-2",
    "title": "EC 320 - Intro. Econometrics",
    "section": "OLS & Calculus",
    "text": "OLS & Calculus\nTo find our choices, we find the partial derivative and set it equal to 0\nFor our intercept \\(\\hat{\\beta}_{0}\\):\n\n\\[\\begin{align*}\n    &\\dfrac{\\partial u_{i}}{\\partial \\hat{\\beta}_{0}} = 0 \\\\\n    \\sum_{i} -2y_{i} + &2\\hat{\\beta}_{0} + 2\\hat{\\beta}_{1}x_{i} = 0\n\\end{align*}\\]\n\nFor our slope \\(\\hat{\\beta}_{1}\\):\n\n\\[\\begin{align*}\n    &\\dfrac{\\partial u_{i}}{\\partial \\hat{\\beta}_{1}} = 0 \\\\\n    \\sum_{i} -2x_{i}y_{i} + &2\\hat{\\beta}_{0}x_{i} + 2\\hat{\\beta}_{1}x_{i}^{2} = 0\n\\end{align*}\\]"
  },
  {
    "objectID": "lectures/03-Estimators-01/034-ols.html#hatbeta_0-derivation",
    "href": "lectures/03-Estimators-01/034-ols.html#hatbeta_0-derivation",
    "title": "EC 320 - Intro. Econometrics",
    "section": "\\(\\hat{\\beta}_{0}\\) Derivation",
    "text": "\\(\\hat{\\beta}_{0}\\) Derivation\n\\[\n    \\sum_{i} -2y_{i} + 2\\hat{\\beta}_{0} + 2\\hat{\\beta}_{1}x_{i} = 0\n\\]\nOur task is to find solve the above for \\(\\hat{\\beta}_{0}\\):"
  },
  {
    "objectID": "lectures/03-Estimators-01/034-ols.html#hatbeta_1-derivation",
    "href": "lectures/03-Estimators-01/034-ols.html#hatbeta_1-derivation",
    "title": "EC 320 - Intro. Econometrics",
    "section": "\\(\\hat{\\beta}_{1}\\) Derivation",
    "text": "\\(\\hat{\\beta}_{1}\\) Derivation\n\\[\n    \\sum_{i} -2x_{i}y_{i} + 2\\hat{\\beta}_{0}x_{i} + 2\\hat{\\beta}_{1}x_{i}^{2} = 0\n\\]\nOur task is to find solve the above for \\(\\hat{\\beta}_{1}\\):"
  },
  {
    "objectID": "lectures/03-Estimators-01/034-ols.html#ols-formulas",
    "href": "lectures/03-Estimators-01/034-ols.html#ols-formulas",
    "title": "EC 320 - Intro. Econometrics",
    "section": "OLS Formulas",
    "text": "OLS Formulas\n\nIntercept\n\\[\n    \\hat{\\beta}_{0} = \\bar{y} - \\hat{\\beta}_{1}\\bar{x}\n\\]\n\nSlope Coefficient\n\\[\n    \\hat{\\beta}_{1} =\n    \\dfrac{\n        \\sum_{i=1}^{n} (y_{i} - \\bar{y})(x_{i} - \\bar{x})\n    }{\n        \\sum_{i=1}^{n} (x_{i} - \\bar{x})^{2}\n    }\n\\]\nThese may look slightly different to my derivation. Part of your assignments is to bridge the gap."
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#important-properties",
    "href": "lectures/04-Estimators-02/040-compile.html#important-properties",
    "title": "Estimators Part II",
    "section": "Important Properties",
    "text": "Important Properties\nThere are three important OLS properties\n\n\nThe point \\((\\bar{x}, \\bar{y})\\) is always on the regression line\n\n\n\nResiduals sum to zero: \\(\\sum_{i}^{n} \\hat{u}_{i} = 0\\)\n\n\n\nThe sample covariance between the independent variable and the residuals is zero: \\(\\sum_{i}^{n} x_{i}\\hat{u}_{i} = 0\\)"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#property-1---proof",
    "href": "lectures/04-Estimators-02/040-compile.html#property-1---proof",
    "title": "Estimators Part II",
    "section": "Property 1 - Proof",
    "text": "Property 1 - Proof\nThe point \\((\\bar{x}, \\bar{y})\\) is always on the regression line\n\nStart with the regression line: \\(\\hat{y}_{i} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1} x_{i}\\)\nRecall that \\(\\hat{\\beta}_{0} = \\bar{y} - \\hat{\\beta}_{1}\\bar{x}\\)\nPlug that in \\(\\hat{\\beta}_{0}\\) and substitute \\(\\bar{x}\\) for \\(x_{i}\\):\n\n\\[\\begin{align*}\n    \\hat{y}_{i} &= \\bar{y} - \\hat{\\beta}_{1}\\bar{x} + \\hat{\\beta}_{1} \\bar{x} \\\\\n    \\hat{y}_{i} &= \\bar{y}\n\\end{align*}\\]"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#property-2---proof",
    "href": "lectures/04-Estimators-02/040-compile.html#property-2---proof",
    "title": "Estimators Part II",
    "section": "Property 2 - Proof",
    "text": "Property 2 - Proof\nResiduals sum to zero: \\(\\sum_{i}^{n} \\hat{u}_{i} = 0\\)\n\nRecall a couple of things we have derived:\n\n\\[\n    \\hat{y}_{i} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1}x_{i} \\;\\; \\text{and} \\;\\; \\hat{u}_{i} = y_{i} - \\hat{y}_{i}\n\\]\n\nThe sum of residuals is:\n\n\\[\n    \\sum_{i} \\hat{u}_{i} = \\sum_{i} (y_{i} - \\hat{y}_{i}) = \\sum_{i} y_{i} - \\sum \\hat{y}_{i}\n\\]\n\nRecall the fact that \\(\\sum_{i} y_{i} = n\\bar{y}\\) and also:\n\n\\[\\begin{align*}\n    \\sum_{i} \\hat{y}_{i} &= \\sum_{i} (\\hat{\\beta}_{0} + \\hat{\\beta}_{1}x_{i})\n    = n \\hat{\\beta}_{0} + \\hat{\\beta}_{1} \\sum_{i} x_{i} \\\\\n    &= n (\\bar{y}_{i} - \\hat{\\beta}_{1}\\bar{x}) + \\hat{\\beta}_{1} n\\bar{x} = n\\bar{y}_{i}\n\\end{align*}\\]\n\nSo:\n\n\\[\n    \\sum_{i} \\hat{u}_{i} = n\\bar{y}_{i} - n\\bar{y}_{i} = 0\n\\]"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#property-3---proof",
    "href": "lectures/04-Estimators-02/040-compile.html#property-3---proof",
    "title": "Estimators Part II",
    "section": "Property 3 - Proof",
    "text": "Property 3 - Proof\nThe sample covariance between the independent variable and the residuals is zero: \\(\\sum_{i}^{n} x_{i}\\hat{u}_{i} = 0\\)\n\nStart with our residuals: \\(\\hat{u}_{i} = y_{i} - \\hat{\\beta}_{0} - \\hat{\\beta}_{1}x_{i}\\)\nMultiply both sides by \\(x_{i}\\) and sum them:\n\n\\[\n    \\sum_{i} x_{i}\\hat{u}_{i} = \\sum_{i} x_{i}y_{i} - \\hat{\\beta}_{0}\\sum_{i} x_{i} - \\hat{\\beta}_{1}\\sum_{i} x_{i}^{2}\n\\]\n\nRecall from our \\(\\hat{\\beta}_{1}\\) derivation that \\(\\sum_{i} x_{i}y_{i} = \\hat{\\beta}_{0}\\sum_{i} x_{i} + \\hat{\\beta}_{1}\\sum_{i} x_{i}^{2}\\)\n\nSo: \\(\\sum_{i}^{n} x_{i}\\hat{u}_{i} = \\hat{\\beta}_{0}\\sum_{i} x_{i} + \\hat{\\beta}_{1}\\sum_{i} x_{i}^{2} - \\hat{\\beta}_{0}\\sum_{i} x_{i} - \\hat{\\beta}_{1}\\sum_{i} x_{i}^{2} = 0\\)"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#goodness-of-fit-1",
    "href": "lectures/04-Estimators-02/040-compile.html#goodness-of-fit-1",
    "title": "Estimators Part II",
    "section": "Goodness of Fit",
    "text": "Goodness of Fit\nSay there are two regressions Regression 1 and Regression 2 with the:\n\nSame slope\nSame intercept\n\nThe question is: Which fitted regression line “explains/fits” the data better?"
  },
  {
    "objectID": "lectures/04-Estimators-02/042-goodness-of-fit.html",
    "href": "lectures/04-Estimators-02/042-goodness-of-fit.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Say there are two regressions Regression 1 and Regression 2 with the:\n\nSame slope\nSame intercept\n\nThe question is: Which fitted regression line “explains/fits” the data better?"
  },
  {
    "objectID": "lectures/04-Estimators-02/042-goodness-of-fit.html#goodness-of-fit",
    "href": "lectures/04-Estimators-02/042-goodness-of-fit.html#goodness-of-fit",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Say there are two regressions Regression 1 and Regression 2 with the:\n\nSame slope\nSame intercept\n\nThe question is: Which fitted regression line “explains/fits” the data better?"
  }
]