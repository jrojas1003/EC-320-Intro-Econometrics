[
  {
    "objectID": "lectures/index.html",
    "href": "lectures/index.html",
    "title": "Lectures",
    "section": "",
    "text": "Expand\n\n\n\n\n\n\n\n  \n\n\n   Expand\n\n\n\n\n\n  \n\n\n   Expand\n\n\n\n\n\n\n  \n\n\n   Expand"
  },
  {
    "objectID": "lectures/index.html#theory",
    "href": "lectures/index.html#theory",
    "title": "Lectures",
    "section": "",
    "text": "Expand\n\n\n\n\n\n\n\n  \n\n\n   Expand\n\n\n\n\n\n  \n\n\n   Expand\n\n\n\n\n\n\n  \n\n\n   Expand"
  },
  {
    "objectID": "lectures/05-Inference/053-hypothesis-test.html#hypothesis-tests",
    "href": "lectures/05-Inference/053-hypothesis-test.html#hypothesis-tests",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Hypothesis Tests",
    "text": "Hypothesis Tests\nSystematic procedure that gives us evidence to hang our hat on. Starting with a Null hypothesis (\\(H_0\\)) and an Alternative hypothesis (\\(H_1\\))\n\\[\n\\begin{align*}\nH_0:& \\beta_1 = 0 \\\\\nH_1:& \\beta_1 \\neq 0\n\\end{align*}\n\\]\n. . .\nIn the context of the wage regression:\n\\[\n\\text{Wage}_i = \\beta_0 + \\beta_1 \\cdot \\text{Education}_i + u_i\n\\]\n\n\\(H_0\\): Education has no effect on wage\n\n\n\\(H_1\\): Education has an effect on wage"
  },
  {
    "objectID": "lectures/05-Inference/053-hypothesis-test.html#possible-outcomes",
    "href": "lectures/05-Inference/053-hypothesis-test.html#possible-outcomes",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Possible outcomes",
    "text": "Possible outcomes\nWithin this structure, four possible outcomes exist:\n\n. . .\n1. We fail to reject the null hypothesis and the null is true.\n. . .\nEx. Education has no effect on wage and, correctly, we fail to reject \\(H_0\\)."
  },
  {
    "objectID": "lectures/05-Inference/053-hypothesis-test.html#possible-outcomes-1",
    "href": "lectures/05-Inference/053-hypothesis-test.html#possible-outcomes-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Possible outcomes",
    "text": "Possible outcomes\nWithin this structure, four possible outcomes exist:\n\n1. We fail to reject the null hypothesis and the null is true.\n2. We reject the null hypothesis and the null is false.\n. . .\nEx. Education has an effect on wage and, correctly, we reject \\(H_0\\)."
  },
  {
    "objectID": "lectures/05-Inference/053-hypothesis-test.html#possible-outcomes-2",
    "href": "lectures/05-Inference/053-hypothesis-test.html#possible-outcomes-2",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Possible outcomes",
    "text": "Possible outcomes\nWithin this structure, four possible outcomes exist:\n\n1. We fail to reject the null hypothesis and the null is true.\n2. We reject the null hypothesis and the null is false.\n3. We reject the null hypothesis, but the null is actually true.\n. . .\nEx. Education has no effect on wage, but we incorrectly reject \\(H_0\\).\nThis is an error. Defined as a Type I error."
  },
  {
    "objectID": "lectures/05-Inference/053-hypothesis-test.html#possible-outcomes-3",
    "href": "lectures/05-Inference/053-hypothesis-test.html#possible-outcomes-3",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Possible outcomes",
    "text": "Possible outcomes\nWithin this structure, four possible outcomes exist:\n\n1. We fail to reject the null hypothesis and the null is true.\n2. We reject the null hypothesis and the null is false.\n3. We reject the null hypothesis, but the null is actually true.\n4. We fail to reject the null hypothesis, but the null is actually false.\n. . .\nEx. Education has an effect on wage, but we incorrectly fail to reject \\(H_0\\).\nThis is an error. Defined as a Type II error."
  },
  {
    "objectID": "lectures/05-Inference/053-hypothesis-test.html#possible-outcomes-4",
    "href": "lectures/05-Inference/053-hypothesis-test.html#possible-outcomes-4",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Possible outcomes",
    "text": "Possible outcomes\nWithin this structure, four possible outcomes exist:\n\n1. We fail to reject the null hypothesis and the null is true.\n2. We reject the null hypothesis and the null is false.\n3. We reject the null hypothesis, but the null is actually true.1\n4. We fail to reject the null hypothesis, but the null is actually false.2\n\nOr… from the golden age of textbook illustrations\n. . .\n\n\n\nHow I think of it"
  },
  {
    "objectID": "lectures/05-Inference/053-hypothesis-test.html#hypothesis-tests-1",
    "href": "lectures/05-Inference/053-hypothesis-test.html#hypothesis-tests-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Hypothesis Tests",
    "text": "Hypothesis Tests\nGoal: Make a statement about \\(\\beta_1\\) using information on \\(\\hat{\\beta}_1\\).\n. . .\n\\(\\hat{\\beta}_1\\) is random—it could be anything, even if \\(\\beta_1 = 0\\) is true.\n\nBut if \\(\\beta_1 = 0\\) is true, then \\(\\hat{\\beta}_1\\) is unlikely to take values far from zero.\nAs the standard error shrinks, we are even less likely to observe “extreme” values of \\(\\hat{\\beta}_1\\) (assuming \\(\\beta_1 = 0\\)).\n\n. . .\nHypothesis testing takes extreme values of \\(\\hat{\\beta}_1\\) as evidence against the null hypothesis, but it will weight them by information about variance the estimated variance of \\(\\hat{\\beta}_1\\)."
  },
  {
    "objectID": "lectures/05-Inference/053-hypothesis-test.html#hypothesis-tests-2",
    "href": "lectures/05-Inference/053-hypothesis-test.html#hypothesis-tests-2",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Hypothesis Tests",
    "text": "Hypothesis Tests\n\n\\(H_0\\): \\(\\beta_1 = 0\\)\n\n\n\\(H_1\\): \\(\\beta \\neq 0\\)\n\nTo conduct the test, we calculate a \\(t\\)-statistic3:\n\\[\nt = \\frac{\\hat{\\beta}_1 - \\beta_1^0}{\\mathop{\\hat{\\text{SE}}} \\left( \\hat{\\beta}_1 \\right)}\n\\]\nDistributed by a \\(t\\)-distribution with \\(n-2\\) degrees of freedom4."
  },
  {
    "objectID": "lectures/05-Inference/053-hypothesis-test.html#hypothesis-testing",
    "href": "lectures/05-Inference/053-hypothesis-test.html#hypothesis-testing",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\nNormal distribution vs. \\(t\\) distribution\n\nA normal distribution has the same shape for any sample size.\nThe shape of the t distribution depends the degrees of freedom.\n\n\n\n\n\n\n\nDegrees of freedom = 5."
  },
  {
    "objectID": "lectures/05-Inference/053-hypothesis-test.html#hypothesis-testing-1",
    "href": "lectures/05-Inference/053-hypothesis-test.html#hypothesis-testing-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\nNormal distribution vs. \\(t\\) distribution\n\nA normal distribution has the same shape for any sample size.\nThe shape of the t distribution depends the degrees of freedom.\n\n\n\n\n\n\n\nDegrees of freedom = 50."
  },
  {
    "objectID": "lectures/05-Inference/053-hypothesis-test.html#hypothesis-testing-2",
    "href": "lectures/05-Inference/053-hypothesis-test.html#hypothesis-testing-2",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\nNormal distribution vs. \\(t\\) distribution\n\nA normal distribution has the same shape for any sample size.\nThe shape of the t distribution depends the degrees of freedom.\n\n\n\n\n\n\n\nDegrees of freedom = 500."
  },
  {
    "objectID": "lectures/05-Inference/053-hypothesis-test.html#hypothesis-testing-3",
    "href": "lectures/05-Inference/053-hypothesis-test.html#hypothesis-testing-3",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\nTwo sided t Tests\nTo conduct a t test, compare the \\(t\\) statistic to the appropriate critical value of the t distribution.\n\nTo find the critical value in a t table, we need the degrees of freedom and the significance level \\(\\alpha\\).\n\nReject (\\(\\text{H}_0\\)) at the \\(\\alpha \\cdot 100\\)-percent level if\n\\[\n\\left| t \\right| = \\left| \\dfrac{\\hat{\\mu} - \\mu_0}{\\mathop{\\text{SE}}(\\hat{\\mu})} \\right| &gt; t_\\text{crit}.\n\\]"
  },
  {
    "objectID": "lectures/05-Inference/053-hypothesis-test.html#hypothesis-tests-3",
    "href": "lectures/05-Inference/053-hypothesis-test.html#hypothesis-tests-3",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Hypothesis Tests",
    "text": "Hypothesis Tests\nNext, we use the \\(\\color{#434C5E}{t}\\)-statistic to calculate a \\(\\color{#B48EAD}{p}\\)-value.\n\n\n\n\n\nDescribes the probability of seeing a \\(\\color{#434C5E}{t}\\)-statistic as extreme as the one we observe if the null hypothesis is actually true.\n. . .\nBut…we still need some benchmark to compare our \\(\\color{#B48EAD}{p}\\)-value against."
  },
  {
    "objectID": "lectures/05-Inference/053-hypothesis-test.html#hypothesis-tests-4",
    "href": "lectures/05-Inference/053-hypothesis-test.html#hypothesis-tests-4",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Hypothesis Tests",
    "text": "Hypothesis Tests\nWe worry mostly about false positives, so we conduct hypothesis tests based on the probability of making a Type I error5.\n. . .\nHow? We select a significance level, \\(\\color{#434C5E}{\\alpha}\\), that specifies our tolerance for false positives (i.e., the probability of Type I error we choose to live with).\n. . .\n\n\n\n\n\n\n\nTo visualize Type I and Type II, we can plot the sampling distributions of \\(\\hat{\\beta}_1\\) under the null and alternative hypotheses"
  },
  {
    "objectID": "lectures/05-Inference/053-hypothesis-test.html#hypothesis-tests-5",
    "href": "lectures/05-Inference/053-hypothesis-test.html#hypothesis-tests-5",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Hypothesis Tests",
    "text": "Hypothesis Tests\nWe then compare \\(\\color{#434C5E}{\\alpha}\\) to the \\(\\color{#B48EAD}{p}\\)-value of our test.\n\nIf the \\(\\color{#B48EAD}{p}\\)-value is less than \\(\\color{#434C5E}{\\alpha}\\), then we reject the null hypothesis at the \\(\\color{#434C5E}{\\alpha}\\cdot100\\) percent level.\nIf the \\(\\color{#B48EAD}{p}\\)-value is greater than \\(\\color{#434C5E}{\\alpha}\\), then we fail to reject the null hypothesis at the \\(\\color{#434C5E}{\\alpha}\\cdot100\\) percent level.6\n\n\n\nEx. Are campus police associated with campus crime?\n\n\n\n\n\n\n\n\\(H_0\\): \\(\\beta_\\text{Police} = 0\\)\n\\(H_1\\): \\(\\beta_\\text{Police} \\neq 0\\)\n\nSignificance level: \\(\\color{#434C5E}{\\alpha} = 0.05\\) (i.e., 5 percent test)\nTest Condition: Reject \\(H_0\\) if \\(p &lt; \\alpha\\)\nWhat is the \\(\\color{#B48EAD}{p}\\)-value? \\(p = 0.18\\)\nDo we reject the null hypothesis? No."
  },
  {
    "objectID": "lectures/05-Inference/053-hypothesis-test.html#hypothesis-tests-6",
    "href": "lectures/05-Inference/053-hypothesis-test.html#hypothesis-tests-6",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Hypothesis Tests",
    "text": "Hypothesis Tests\n\\(\\color{#B48EAD}{p}\\)-values are difficult to calculate by hand.\nAlternative: Compare \\(\\color{#434C5E}{t}\\)-statistic to critical values from the \\({\\color{#434C5E} t}\\)-distribution."
  },
  {
    "objectID": "lectures/05-Inference/053-hypothesis-test.html#hypothesis-tests-7",
    "href": "lectures/05-Inference/053-hypothesis-test.html#hypothesis-tests-7",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Hypothesis Tests",
    "text": "Hypothesis Tests\nNotation: \\(t_{1-\\alpha/2, n-2}\\) or \\(t_\\text{crit}\\).\n\nFind in a \\(t\\)-table using \\(\\color{#434C5E}{\\alpha}\\) and \\(n-2\\) degrees of freedom.\n\nCompare the the critical value to your \\(t\\)-statistic:\n\nIf \\(|t| &gt; |t_{1-\\alpha/2, n-2}|\\), then reject the null.\nIf \\(|t| &lt; |t_{1-\\alpha/2, n-2}|\\), then fail to reject the null."
  },
  {
    "objectID": "lectures/05-Inference/053-hypothesis-test.html#two-sided-tests",
    "href": "lectures/05-Inference/053-hypothesis-test.html#two-sided-tests",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Two-sided tests",
    "text": "Two-sided tests\nBased on a critical value of \\(t_{1-\\alpha/2, n-2} = t_{0.975, 100} = 1.98\\) we can identify a rejection region on the \\(\\color{#434C5E}{t}\\)-distribution.\n\n. . .\nIf our \\(\\color{#434C5E}{t}\\)-statistic is in the rejection region, then we reject the null hypothesis at the 5 percent level.\n\nEx.7 \\(\\alpha = 0.05\\)\n\n\n\n\\(H_0\\): \\(\\beta_1 = 0\\)\n\\(H_1\\): \\(\\beta_1 \\neq 0\\)\n\nNotice that the \\(\\color{#434C5E}{t}\\)-statistic is 7.15. The critical value is \\(\\color{#434C5E}{t_{\\text{0.975, 28}}} = 2.05\\).\nWhich implies that \\(p &lt; 0.05\\). Therefore, we reject \\(H_0\\) at the 5% level.\n\nEx. Are campus police associated with campus crime? (\\(\\alpha = 0.1\\))\n\n\n\n\\(H_0\\): \\(\\beta_\\text{Police} = 0\\)\n\\(H_1\\): \\(\\beta_\\text{Police} \\neq 0\\)\n\nThe \\(\\color{#434C5E}{t \\text{-stat}} = 1.35\\). The critical value is \\(\\color{#434C5E}{t_{\\text{0.95, 94}}} = 1.66\\).\n|\\(\\color{#434C5E}{t \\text{-stat}}| &lt; |\\color{#434C5E}{t_{\\text{crit}}}|\\) implies that \\(p &gt; 0.05\\). Therefore, we fail to reject \\(H_0\\) at the 10% level."
  },
  {
    "objectID": "lectures/05-Inference/053-hypothesis-test.html#one-sided-tests",
    "href": "lectures/05-Inference/053-hypothesis-test.html#one-sided-tests",
    "title": "EC 320 - Intro. Econometrics",
    "section": "One-sided tests",
    "text": "One-sided tests\nWe might be confident in a parameter being non-negative/non-positive.\nOne-sided tests assume that the parameter of interest is either greater than/less than \\(H_0\\).\n\nOption 1 \\(H_0\\): \\(\\beta_1 = 0\\) vs. \\(H_1\\): \\(\\beta_1 &gt; 0\\)\nOption 2 \\(H_0\\): \\(\\beta_1 = 0\\) vs. \\(H_1\\): \\(\\beta_1 &lt; 0\\)\n\n. . .\nIf this assumption is reasonable, then our rejection region changes.\n\nSame \\(\\alpha\\)."
  },
  {
    "objectID": "lectures/05-Inference/053-hypothesis-test.html#one-sided-tests-1",
    "href": "lectures/05-Inference/053-hypothesis-test.html#one-sided-tests-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "One-sided tests",
    "text": "One-sided tests\nLeft-tailed: Based on a critical value of \\(t_{1-\\alpha, n-2} = t_{0.95, 100} = 1.66\\), we can identify a rejection region on the \\(t\\)-distribution.\n\n\n\n\n\n. . .\nIf our \\(t\\) statistic is in the rejection region, then we reject the null hypothesis at the 5 percent level."
  },
  {
    "objectID": "lectures/05-Inference/053-hypothesis-test.html#one-sided-tests-2",
    "href": "lectures/05-Inference/053-hypothesis-test.html#one-sided-tests-2",
    "title": "EC 320 - Intro. Econometrics",
    "section": "One-sided tests",
    "text": "One-sided tests\nRight-tailed: Based on a critical value of \\(t_{1-\\alpha, n-2} = t_{0.95, 100} = 1.66\\), we can identify a rejection region on the \\(t\\)-distribution.\n\n\n\n\n\n. . .\nIf our \\(t\\) statistic is in the rejection region, then we reject the null hypothesis at the 5 percent level.\n\nEx. Do campus police deter campus crime? (\\(\\alpha = 0.1\\))\nSuppose we rule out the possibility that police increase crime, but not that they have no effect.\n\n\n\n\n\n\n\n\\(H_0\\): \\(\\beta_\\text{Police} = 0\\)\n\\(H_1\\): \\(\\beta_\\text{Police} &lt; 0\\)\n\nNotice that the \\(\\color{#434C5E}{t \\text{-stat}} = 1.35\\). The critical value is \\(\\color{#434C5E}{t_{\\text{0.9, 94}}} = 1.29\\).\nWhich implies that \\(p &gt; 0.05\\). Therefore, we reject \\(H_0\\) at the 5% level."
  },
  {
    "objectID": "lectures/05-Inference/053-hypothesis-test.html#footnotes",
    "href": "lectures/05-Inference/053-hypothesis-test.html#footnotes",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nType I error↩︎\nType II error↩︎\n\\(\\beta_1^0\\) is the value of \\(\\beta_1\\) in our null hypothesis (e.g., \\(\\beta_1^0 = 0\\)).↩︎\nrepresents the number of independent values in a sample that are free to vary when estimating statistical parameters.↩︎\nWe reject the null hypothesis, but the null is actually true.↩︎\nNote: Fail to reject \\(\\neq\\) accept.↩︎\n{{&lt; fa brands r-project &gt;}} defaults to testing hypotheses against the null hypothesis of zero.↩︎"
  },
  {
    "objectID": "lectures/05-Inference/051-prologue.html",
    "href": "lectures/05-Inference/051-prologue.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Up to now, we have been focusing on OLS considering:\n\nHow we model regressions with this estimator\nHow the estimator is derived and what properties it demonstrates\nHow the classical assumptions make the estimator BLUE\n\n. . .\nWe have mostly ignored drawing conclusions about the true population parameters from the estimates of the sample data\n\nThis is inference"
  },
  {
    "objectID": "lectures/05-Inference/051-prologue.html#ols",
    "href": "lectures/05-Inference/051-prologue.html#ols",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Up to now, we have been focusing on OLS considering:\n\nHow we model regressions with this estimator\nHow the estimator is derived and what properties it demonstrates\nHow the classical assumptions make the estimator BLUE\n\n. . .\nWe have mostly ignored drawing conclusions about the true population parameters from the estimates of the sample data\n\nThis is inference"
  },
  {
    "objectID": "lectures/05-Inference/051-prologue.html#ols-1",
    "href": "lectures/05-Inference/051-prologue.html#ols-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "OLS",
    "text": "OLS\nThus far we have fit an OLS model to find an answer to the following questions:\n\nHow much does an additional year of schooling increase earnings?\nDoes the number of police officers affect campus crime rates?\n\n. . .\nUp to now, we have not discussed our confidence in our fitted relationship\nEven if all 6 Assumptions hold, sample selection might generate the incorrect conclusions in a completely unbiased, coincidental fashion.\n\nPreviously we used the first 3 assumptions to show that OLS is unbiased:\n\\[\n\\mathop{\\mathbb{E}}\\left[ \\hat{\\beta} \\right] = \\beta\n\\]\n\nWe used the first 5 assumptions to derive a formula for the variance of the OLS estimator:\n\\[\n\\mathop{\\text{Var}}(\\hat{\\beta}) = \\frac{\\sigma^2}{\\sum_{i=1}^n (X_i - \\bar{X})^2}\n\\]\n. . .\nBy using the variance of the OLS estimator, we can infer confidence from the sampling distribution"
  },
  {
    "objectID": "lectures/05-Inference/051-prologue.html#sampling-distribution",
    "href": "lectures/05-Inference/051-prologue.html#sampling-distribution",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Sampling distribution",
    "text": "Sampling distribution\n\nThe probability distribution of the OLS estimators obtained from repeatedly drawing random samples of the same size from a population and fitting point estimates each time.\n\nProvides information about their variability, accuracy, and precision across different samples.\n. . .\n\nPoint estimates\n\nThe fitted values of the OLS estimator (e.g., \\(\\hat{\\beta}_0, \\hat{\\beta}_1\\))"
  },
  {
    "objectID": "lectures/05-Inference/051-prologue.html#sampling-distribution-properties",
    "href": "lectures/05-Inference/051-prologue.html#sampling-distribution-properties",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Sampling distribution properties",
    "text": "Sampling distribution properties\n1. Unbiasedness: If the Gauss-Markov assumptions hold, the OLS estimators are unbiased (i.e., \\(E(\\hat{\\beta}_0) = \\beta_0\\) and \\(E(\\hat{\\beta}_1) = \\beta_1\\))\n. . .\n2. Variance: The variance of the OLS estimators describes their dispersion around the true population parameters.\n. . .\n3. Normality: If the errors are normally distributed or the sample size is large enough, by the Central Limit Theorem, the sampling distribution of the OLS estimators will be approximately normal.1"
  },
  {
    "objectID": "lectures/05-Inference/051-prologue.html#sampling-distribution-1",
    "href": "lectures/05-Inference/051-prologue.html#sampling-distribution-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Sampling distribution",
    "text": "Sampling distribution\nThe sampling distribution of \\(\\hat{\\beta}\\) to conduct hypothesis tests.\nUse all 6 classical assumptions to show that OLS is normally distributed:\n\\[\n\\hat{\\beta} \\sim \\mathop{N}\\left( \\beta, \\frac{\\sigma^2}{\\sum_{i=1}^n (X_i - \\bar{X})^2} \\right)\n\\]\n. . .\nLet’s look at a simulation"
  },
  {
    "objectID": "lectures/05-Inference/051-prologue.html#section",
    "href": "lectures/05-Inference/051-prologue.html#section",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Plotting the distributions of the point estimates in a histogram\n \n\n\n\n\n\nSimulating 1,000 draws\n\nPlotting the distributions of the point estimates in a histogram\n \n\n\n\n\n\nSimulating 10,000 draws"
  },
  {
    "objectID": "lectures/05-Inference/051-prologue.html#footnotes",
    "href": "lectures/05-Inference/051-prologue.html#footnotes",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nUseful for making inferences, constructing confidence intervals, and performing hypothesis tests using the t-distribution.↩︎"
  },
  {
    "objectID": "lectures/04-Estimators-02/044-gauss-markov.html",
    "href": "lectures/04-Estimators-02/044-gauss-markov.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "OLS is the Best Linear Unbiased Estimator (BLUE) when the following assumptions hold:\n\n. . .\nA1. Linearity: The population relationship is linear in parameters with an additive error term.\nA2. Sample Variation: There is variation in \\(X\\).\nA3. Exogeniety: The \\(X\\) variable is exogenous\nA4. Homoskedasticity: The error term has the same variance for each value of the independent variable\nA5. Non-autocorrelation: The values of error terms have independent distributions"
  },
  {
    "objectID": "lectures/04-Estimators-02/044-gauss-markov.html#gauss-markov-theorem",
    "href": "lectures/04-Estimators-02/044-gauss-markov.html#gauss-markov-theorem",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "OLS is the Best Linear Unbiased Estimator (BLUE) when the following assumptions hold:\n\n. . .\nA1. Linearity: The population relationship is linear in parameters with an additive error term.\nA2. Sample Variation: There is variation in \\(X\\).\nA3. Exogeniety: The \\(X\\) variable is exogenous\nA4. Homoskedasticity: The error term has the same variance for each value of the independent variable\nA5. Non-autocorrelation: The values of error terms have independent distributions"
  },
  {
    "objectID": "lectures/04-Estimators-02/044-gauss-markov.html#gauss-markov-theorem-1",
    "href": "lectures/04-Estimators-02/044-gauss-markov.html#gauss-markov-theorem-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Gauss-Markov Theorem",
    "text": "Gauss-Markov Theorem\n\nOLS is the Best Unbiased Estimator (BUE) when the following assumptions hold:\n\n. . .\nA1. Linearity: The population relationship is linear in parameters with an additive error term.\nA2. Sample Variation: There is variation in \\(X\\).\nA3. Exogeniety: The \\(X\\) variable is exogenous\nA4. Homoskedasticity: The error term has the same variance for each value of the independent variable\nA5. Non-autocorrelation: The values of error terms have independent distributions\nA6. Normality: The population error term in normally distributed with mean zero and variance \\(\\sigma^2\\)"
  },
  {
    "objectID": "lectures/04-Estimators-02/042-goodness-of-fit.html",
    "href": "lectures/04-Estimators-02/042-goodness-of-fit.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Say there are two regressions Regression 1 and Regression 2 with the:\n\nSame slope\nSame intercept\n\nThe question is: Which fitted regression line “explains/fits” the data better?"
  },
  {
    "objectID": "lectures/04-Estimators-02/042-goodness-of-fit.html#goodness-of-fit",
    "href": "lectures/04-Estimators-02/042-goodness-of-fit.html#goodness-of-fit",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Say there are two regressions Regression 1 and Regression 2 with the:\n\nSame slope\nSame intercept\n\nThe question is: Which fitted regression line “explains/fits” the data better?"
  },
  {
    "objectID": "lectures/04-Estimators-02/042-goodness-of-fit.html#goodness-of-fit-1",
    "href": "lectures/04-Estimators-02/042-goodness-of-fit.html#goodness-of-fit-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Goodness of Fit",
    "text": "Goodness of Fit\nRegression 1 vs Regression 2\nThe coefficient of determination, \\(R^{2}\\), is the fraction of the variation in \\(y_{i}\\) “explained” by \\(x_{i}\\).\n\n\\(R^{2} = 1 \\Rightarrow x_{i}\\) explains all of the variation in \\(y_{i}\\)\n\\(R^{2} = 0 \\Rightarrow x_{i}\\) explains none of the variation in \\(y_{i}\\)"
  },
  {
    "objectID": "lectures/04-Estimators-02/042-goodness-of-fit.html#explained-and-unexplained-variation",
    "href": "lectures/04-Estimators-02/042-goodness-of-fit.html#explained-and-unexplained-variation",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Explained and Unexplained Variation",
    "text": "Explained and Unexplained Variation\nResiduals remind us that there are parts of \\(y_{i}\\) we cannot explain:\n\\[\n    y_{i} = \\hat{y}_{i} + \\hat{u}_{i}\n\\]\n\nIf you sum the above, divide by \\(n\\), and use the fact that OLS residuals sum to zero, you get:\n\n\\[\n    \\bar{\\hat{u}} = 0 \\Rightarrow \\bar{y} = \\bar{\\hat{y}}\n\\]\n\nSo the fitted values average out to the actual values"
  },
  {
    "objectID": "lectures/04-Estimators-02/042-goodness-of-fit.html#explained-and-unexplained-variation-1",
    "href": "lectures/04-Estimators-02/042-goodness-of-fit.html#explained-and-unexplained-variation-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Explained and Unexplained Variation",
    "text": "Explained and Unexplained Variation\nTotal Sum of Squares (TSS) measures variation in \\(y_{i}\\):\n\\[\n    \\color{#BF616A}{TSS} \\equiv \\sum_{i = 1}^{n} (y_{i} - \\bar{y})^{2}\n\\]\n\nTSS can be decomposed into explained and unexplained variation\n\n\n\nExplained Sum of Squared (ESS) measures the variation in \\(\\hat{y}_{i}\\):\n\\[\n    \\color{#8FBCBB}{ESS} \\equiv \\sum_{i = 1}^{n} (\\hat{y}_{i} - \\bar{y})^{2}\n\\]\n\nResidual Sum of Squares (ESS) measures the variation in $ _{i}$:\n\\[\n    \\color{#D08770}{RSS} \\equiv \\sum_{i = 1}^{n} \\hat{u}_{i}^{2}\n\\]"
  },
  {
    "objectID": "lectures/04-Estimators-02/042-goodness-of-fit.html#section",
    "href": "lectures/04-Estimators-02/042-goodness-of-fit.html#section",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "This means that we can show \\(\\color{#BF616A}{TSS} = \\color{#8FBCBB}{ESS} + \\color{#D08770}{RSS}\\)\nStep 01: Plug \\(y_{i} = \\hat{y}_{i} + \\hat{u}_{i}\\) into TSS\n\\[\\begin{align*}\n    \\color{#BF616A}{TSS} &= \\sum_{i = 1}^{n} (\\hat{y}_{i} - \\bar{y})^{2} \\\\\n    &= \\sum_{i=1}^{n} ([\\hat{y}_{i} + \\hat{u}_{i}] - [\\bar{\\hat{y}} + \\bar{\\hat{u}}])^{2}\n\\end{align*}\\]"
  },
  {
    "objectID": "lectures/04-Estimators-02/042-goodness-of-fit.html#section-1",
    "href": "lectures/04-Estimators-02/042-goodness-of-fit.html#section-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "This means that we can show \\(\\color{#BF616A}{TSS} = \\color{#8FBCBB}{ESS} + \\color{#D08770}{RSS}\\)\nStep 02: Recall that \\(\\bar{\\hat{u}} = 0\\) & \\(\\bar{y} = \\bar{\\hat{y}}\\).\n\\[\\begin{align*}\n    \\color{#BF616A}{TSS} &= \\sum_{i=1}^{n} ([\\hat{y}_{i} + \\hat{u}_{i}] - [\\bar{\\hat{y}} + \\bar{\\hat{u}}])^{2} \\\\\n    &= \\sum_{i=1}^{n} ([\\hat{y}_{i} + \\hat{u}_{i}] - \\bar{\\hat{y}})^{2} \\\\\n    &= \\sum_{i=1}^{n} ([\\hat{y}_{i} - \\bar{y}] + \\hat{u}_{i}) ([\\hat{y}_{i} - \\bar{y}] + \\hat{u}_{i}) \\\\\n    &= \\sum_{i=1}^{n} (\\hat{y}_{i} - \\bar{y})^{2} +\n    \\sum_{i=1}^{n} \\hat{u}_{i}^{2} +\n    2\\sum_{i=1}^{n} \\left( (\\hat{y}_{i} - \\bar{y}) \\hat{u}_{i} \\right)\n\\end{align*}\\]"
  },
  {
    "objectID": "lectures/04-Estimators-02/042-goodness-of-fit.html#section-2",
    "href": "lectures/04-Estimators-02/042-goodness-of-fit.html#section-2",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Step 03: Notice ESS and RSS\n\\[\\begin{align*}\n    \\color{#BF616A}{TSS} &= \\color{#8FBCBB}{\\sum_{i=1}^{n} (\\hat{y}_{i} - \\bar{y})^{2}} +\n    \\color{#D08770}{\\sum_{i=1}^{n} \\hat{u}_{i}^{2}} +\n    2\\sum_{i=1}^{n} \\left( (\\hat{y}_{i} - \\bar{y}) \\hat{u}_{i} \\right) \\\\\n    &= \\color{#8FBCBB}{ESS} + \\color{#D08770}{RSS} + 2\\sum_{i=1}^{n} \\left( (\\hat{y}_{i} - \\bar{y}) \\hat{u}_{i} \\right) \\\\\n\\end{align*}\\]\nStep 04: Simplify\n\\[\\begin{align*}\n    \\color{#BF616A}{TSS} = \\color{#8FBCBB}{ESS} + \\color{#D08770}{RSS} +\n    2\\sum_{i=1}^{n}\\hat{y}_{i}\\hat{u}_{i} -\n    2\\bar{y} \\sum_{i=1}^{n} \\hat{u}_{i}\n\\end{align*}\\]"
  },
  {
    "objectID": "lectures/04-Estimators-02/042-goodness-of-fit.html#section-3",
    "href": "lectures/04-Estimators-02/042-goodness-of-fit.html#section-3",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Step 05: Shut down that last two terms by noticing that:\n\\[\\begin{align*}\n    2\\sum_{i=1}^{n}\\hat{y}_{i}\\hat{u}_{i} -\n    2\\bar{y} \\sum_{i=1}^{n} \\hat{u}_{i} =\n    0\n\\end{align*}\\]\nYou will prove this in an assignment\nThen we have:\n\\[\\begin{align*}\n     \\color{#BF616A}{TSS} = \\color{#8FBCBB}{ESS} + \\color{#D08770}{RSS}\n\\end{align*}\\]\n. . .\nSome visual intuition makes all the math seem a lot simpler\n\nPlot our data\n\n\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n\n\n\n\n\n\n\n\n\n\n\\[\n\\color{#148B25}{\\overline{\\text{MPG}}_{i}} = 20.09\n\\]\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead."
  },
  {
    "objectID": "lectures/04-Estimators-02/042-goodness-of-fit.html#section-4",
    "href": "lectures/04-Estimators-02/042-goodness-of-fit.html#section-4",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "\\[\n\\color{#BF616A}{\\text{TSS}} \\equiv \\sum_{i=1}^n (y_i - \\bar{y})^2\n\\]"
  },
  {
    "objectID": "lectures/04-Estimators-02/042-goodness-of-fit.html#section-5",
    "href": "lectures/04-Estimators-02/042-goodness-of-fit.html#section-5",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "\\[\n\\color{#148B25}{\\widehat{\\text{MPG}}_{i}} = 37.3 - 5.34 \\cdot \\text{weight}_i\n\\]"
  },
  {
    "objectID": "lectures/04-Estimators-02/042-goodness-of-fit.html#section-6",
    "href": "lectures/04-Estimators-02/042-goodness-of-fit.html#section-6",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "\\[\n\\color{#8FBCBB}{\\text{ESS}} \\equiv \\sum_{i=1}^n (\\hat{y}_{i} - \\bar{y})^2\n\\]"
  },
  {
    "objectID": "lectures/04-Estimators-02/042-goodness-of-fit.html#section-7",
    "href": "lectures/04-Estimators-02/042-goodness-of-fit.html#section-7",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "\\[\n\\color{#D08770}{\\text{RSS}} \\equiv \\sum_{i=1}^n \\hat{u}_i^2\n\\]"
  },
  {
    "objectID": "lectures/04-Estimators-02/042-goodness-of-fit.html#section-8",
    "href": "lectures/04-Estimators-02/042-goodness-of-fit.html#section-8",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "\\[\n\\color{#BF616A}{\\text{TSS}} \\equiv \\sum_{i=1}^n (Y_i - \\bar{Y})^2\n\\]\n\n\\[\n\\color{#8FBCBB}{\\text{ESS}} \\equiv \\sum_{i=1}^n (\\hat{Y_i} - \\bar{Y})^2\n\\]\n\n\\[\n\\color{#D08770}{\\text{RSS}} \\equiv \\sum_{i=1}^n \\hat{u}_i^2\n\\]\n\n\n\n\nWarning: `position_dodge()` requires non-overlapping x intervals.\n`position_dodge()` requires non-overlapping x intervals."
  },
  {
    "objectID": "lectures/04-Estimators-02/042-goodness-of-fit.html#goodness-of-fit-2",
    "href": "lectures/04-Estimators-02/042-goodness-of-fit.html#goodness-of-fit-2",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Goodness of Fit",
    "text": "Goodness of Fit\nWhat percentage of the variation in our \\(y_{i}\\) is apparently explained by our model? The \\(R^{2}\\) term represents this percentage.\nTotal variation is represented by TSS and our model is capturing the ‘explained’ sum of squares, ESS.\nTaking a simple ratio reveals how much variation our model explains:\n\n\\(R^{2} = \\dfrac{\\color{#8FBCBB}{ESS}}{\\color{#BF616A}{TSS}}\\) varies between 0 and 1\n\\(R^{2} = 1 - \\dfrac{\\color{#D08770}{RSS}}{\\color{#BF616A}{TSS}}\\), 100% minus the unexplained variation\n\n\\(R^{2}\\) is related to the correlation between the actual values of \\(y\\) and the fitted values of \\(y\\)."
  },
  {
    "objectID": "lectures/04-Estimators-02/042-goodness-of-fit.html#goodness-of-fit-3",
    "href": "lectures/04-Estimators-02/042-goodness-of-fit.html#goodness-of-fit-3",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Goodness of Fit",
    "text": "Goodness of Fit\nSo what? In the social sciences, low \\(R^{2}\\) values are common.\nLow \\(R^{2}\\) does not necessarily mean you have a “good” regression:\n\nWorries about selection bias and omitted variables still apply\nSome ‘powerfully high’ \\(R^{2}\\) values are the result of simple accounting exercises, and tell us nothing about causality"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#important-properties",
    "href": "lectures/04-Estimators-02/040-compile.html#important-properties",
    "title": "Estimators Part II",
    "section": "Important Properties",
    "text": "Important Properties\nThere are three important OLS properties\n\n\nThe point \\((\\bar{x}, \\bar{y})\\) is always on the regression line\n\n\n\nResiduals sum to zero: \\(\\sum_{i}^{n} \\hat{u}_{i} = 0\\)\n\n\n\nThe sample covariance between the independent variable and the residuals is zero: \\(\\sum_{i}^{n} x_{i}\\hat{u}_{i} = 0\\)"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#property-1---proof",
    "href": "lectures/04-Estimators-02/040-compile.html#property-1---proof",
    "title": "Estimators Part II",
    "section": "Property 1 - Proof",
    "text": "Property 1 - Proof\nThe point \\((\\bar{x}, \\bar{y})\\) is always on the regression line\n\nStart with the regression line: \\(\\hat{y}_{i} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1} x_{i}\\)\nRecall that \\(\\hat{\\beta}_{0} = \\bar{y} - \\hat{\\beta}_{1}\\bar{x}\\)\nPlug that in \\(\\hat{\\beta}_{0}\\) and substitute \\(\\bar{x}\\) for \\(x_{i}\\):\n\n\\[\\begin{align*}\n    \\hat{y}_{i} &= \\bar{y} - \\hat{\\beta}_{1}\\bar{x} + \\hat{\\beta}_{1} \\bar{x} \\\\\n    \\hat{y}_{i} &= \\bar{y}\n\\end{align*}\\]"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#property-2---proof",
    "href": "lectures/04-Estimators-02/040-compile.html#property-2---proof",
    "title": "Estimators Part II",
    "section": "Property 2 - Proof",
    "text": "Property 2 - Proof\nResiduals sum to zero: \\(\\sum_{i}^{n} \\hat{u}_{i} = 0\\)\n\nRecall a couple of things we have derived:\n\n\\[\n    \\hat{y}_{i} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1}x_{i} \\;\\; \\text{and} \\;\\; \\hat{u}_{i} = y_{i} - \\hat{y}_{i}\n\\]\n\nThe sum of residuals is:\n\n\\[\n    \\sum_{i} \\hat{u}_{i} = \\sum_{i} (y_{i} - \\hat{y}_{i}) = \\sum_{i} y_{i} - \\sum \\hat{y}_{i}\n\\]\n\nRecall the fact that \\(\\sum_{i} y_{i} = n\\bar{y}\\) and also:\n\n\\[\\begin{align*}\n    \\sum_{i} \\hat{y}_{i} &= \\sum_{i} (\\hat{\\beta}_{0} + \\hat{\\beta}_{1}x_{i})\n    = n \\hat{\\beta}_{0} + \\hat{\\beta}_{1} \\sum_{i} x_{i} \\\\\n    &= n (\\bar{y}_{i} - \\hat{\\beta}_{1}\\bar{x}) + \\hat{\\beta}_{1} n\\bar{x} = n\\bar{y}_{i}\n\\end{align*}\\]\n\nSo:\n\n\\[\n    \\sum_{i} \\hat{u}_{i} = n\\bar{y}_{i} - n\\bar{y}_{i} = 0\n\\]"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#property-3---proof",
    "href": "lectures/04-Estimators-02/040-compile.html#property-3---proof",
    "title": "Estimators Part II",
    "section": "Property 3 - Proof",
    "text": "Property 3 - Proof\nThe sample covariance between the independent variable and the residuals is zero: \\(\\sum_{i}^{n} x_{i}\\hat{u}_{i} = 0\\)\n\nStart with our residuals: \\(\\hat{u}_{i} = y_{i} - \\hat{\\beta}_{0} - \\hat{\\beta}_{1}x_{i}\\)\nMultiply both sides by \\(x_{i}\\) and sum them:\n\n\\[\n    \\sum_{i} x_{i}\\hat{u}_{i} = \\sum_{i} x_{i}y_{i} - \\hat{\\beta}_{0}\\sum_{i} x_{i} - \\hat{\\beta}_{1}\\sum_{i} x_{i}^{2}\n\\]\n\nRecall from our \\(\\hat{\\beta}_{1}\\) derivation that \\(\\sum_{i} x_{i}y_{i} = \\hat{\\beta}_{0}\\sum_{i} x_{i} + \\hat{\\beta}_{1}\\sum_{i} x_{i}^{2}\\)\n\nSo: \\(\\sum_{i}^{n} x_{i}\\hat{u}_{i} = \\hat{\\beta}_{0}\\sum_{i} x_{i} + \\hat{\\beta}_{1}\\sum_{i} x_{i}^{2} - \\hat{\\beta}_{0}\\sum_{i} x_{i} - \\hat{\\beta}_{1}\\sum_{i} x_{i}^{2} = 0\\)"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#goodness-of-fit-1",
    "href": "lectures/04-Estimators-02/040-compile.html#goodness-of-fit-1",
    "title": "Estimators Part II",
    "section": "Goodness of Fit",
    "text": "Goodness of Fit\nSay there are two regressions Regression 1 and Regression 2 with the:\n\nSame slope\nSame intercept\n\nThe question is: Which fitted regression line “explains/fits” the data better?"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#goodness-of-fit-2",
    "href": "lectures/04-Estimators-02/040-compile.html#goodness-of-fit-2",
    "title": "Estimators Part II",
    "section": "Goodness of Fit",
    "text": "Goodness of Fit\nRegression 1 vs Regression 2\nThe coefficient of determination, \\(R^{2}\\), is the fraction of the variation in \\(y_{i}\\) “explained” by \\(x_{i}\\).\n\n\\(R^{2} = 1 \\Rightarrow x_{i}\\) explains all of the variation in \\(y_{i}\\)\n\\(R^{2} = 0 \\Rightarrow x_{i}\\) explains none of the variation in \\(y_{i}\\)"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#explained-and-unexplained-variation",
    "href": "lectures/04-Estimators-02/040-compile.html#explained-and-unexplained-variation",
    "title": "Estimators Part II",
    "section": "Explained and Unexplained Variation",
    "text": "Explained and Unexplained Variation\nResiduals remind us that there are parts of \\(y_{i}\\) we cannot explain:\n\\[\n    y_{i} = \\hat{y}_{i} + \\hat{u}_{i}\n\\]\n\nIf you sum the above, divide by \\(n\\), and use the fact that OLS residuals sum to zero, you get:\n\n\\[\n    \\bar{\\hat{u}} = 0 \\Rightarrow \\bar{y} = \\bar{\\hat{y}}\n\\]\n\nSo the fitted values average out to the actual values"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#explained-and-unexplained-variation-1",
    "href": "lectures/04-Estimators-02/040-compile.html#explained-and-unexplained-variation-1",
    "title": "Estimators Part II",
    "section": "Explained and Unexplained Variation",
    "text": "Explained and Unexplained Variation\nTotal Sum of Squares (TSS) measures variation in \\(y_{i}\\):\n\\[\n    \\color{#BF616A}{TSS} \\equiv \\sum_{i = 1}^{n} (y_{i} - \\bar{y})^{2}\n\\]\n\nTSS can be decomposed into explained and unexplained variation\n\n\n\nExplained Sum of Squared (ESS) measures the variation in \\(\\hat{y}_{i}\\):\n\\[\n    \\color{#8FBCBB}{ESS} \\equiv \\sum_{i = 1}^{n} (\\hat{y}_{i} - \\bar{y})^{2}\n\\]\n\nResidual Sum of Squares (ESS) measures the variation in $ _{i}$:\n\\[\n    \\color{#D08770}{RSS} \\equiv \\sum_{i = 1}^{n} \\hat{u}_{i}^{2}\n\\]"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#section",
    "href": "lectures/04-Estimators-02/040-compile.html#section",
    "title": "Estimators Part II",
    "section": "",
    "text": "This means that we can show \\(\\color{#BF616A}{TSS} = \\color{#8FBCBB}{ESS} + \\color{#D08770}{RSS}\\)\nStep 01: Plug \\(y_{i} = \\hat{y}_{i} + \\hat{u}_{i}\\) into TSS\n\\[\\begin{align*}\n    \\color{#BF616A}{TSS} &= \\sum_{i = 1}^{n} (\\hat{y}_{i} - \\bar{y})^{2} \\\\\n    &= \\sum_{i=1}^{n} ([\\hat{y}_{i} + \\hat{u}_{i}] - [\\bar{\\hat{y}} + \\bar{\\hat{u}}])^{2}\n\\end{align*}\\]"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#section-1",
    "href": "lectures/04-Estimators-02/040-compile.html#section-1",
    "title": "Estimators Part II",
    "section": "",
    "text": "This means that we can show \\(\\color{#BF616A}{TSS} = \\color{#8FBCBB}{ESS} + \\color{#D08770}{RSS}\\)\nStep 02: Recall that \\(\\bar{\\hat{u}} = 0\\) & \\(\\bar{y} = \\bar{\\hat{y}}\\).\n\\[\\begin{align*}\n    \\color{#BF616A}{TSS} &= \\sum_{i=1}^{n} ([\\hat{y}_{i} + \\hat{u}_{i}] - [\\bar{\\hat{y}} + \\bar{\\hat{u}}])^{2} \\\\\n    &= \\sum_{i=1}^{n} ([\\hat{y}_{i} + \\hat{u}_{i}] - \\bar{\\hat{y}})^{2} \\\\\n    &= \\sum_{i=1}^{n} ([\\hat{y}_{i} - \\bar{y}] + \\hat{u}_{i}) ([\\hat{y}_{i} - \\bar{y}] + \\hat{u}_{i}) \\\\\n    &= \\sum_{i=1}^{n} (\\hat{y}_{i} - \\bar{y})^{2} +\n    \\sum_{i=1}^{n} \\hat{u}_{i}^{2} +\n    2\\sum_{i=1}^{n} \\left( (\\hat{y}_{i} - \\bar{y}) \\hat{u}_{i} \\right)\n\\end{align*}\\]"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#section-2",
    "href": "lectures/04-Estimators-02/040-compile.html#section-2",
    "title": "Estimators Part II",
    "section": "",
    "text": "Step 03: Notice ESS and RSS\n\\[\\begin{align*}\n    \\color{#BF616A}{TSS} &= \\color{#8FBCBB}{\\sum_{i=1}^{n} (\\hat{y}_{i} - \\bar{y})^{2}} +\n    \\color{#D08770}{\\sum_{i=1}^{n} \\hat{u}_{i}^{2}} +\n    2\\sum_{i=1}^{n} \\left( (\\hat{y}_{i} - \\bar{y}) \\hat{u}_{i} \\right) \\\\\n    &= \\color{#8FBCBB}{ESS} + \\color{#D08770}{RSS} + 2\\sum_{i=1}^{n} \\left( (\\hat{y}_{i} - \\bar{y}) \\hat{u}_{i} \\right) \\\\\n\\end{align*}\\]\nStep 04: Simplify\n\\[\\begin{align*}\n    \\color{#BF616A}{TSS} = \\color{#8FBCBB}{ESS} + \\color{#D08770}{RSS} +\n    2\\sum_{i=1}^{n}\\hat{y}_{i}\\hat{u}_{i} -\n    2\\bar{y} \\sum_{i=1}^{n} \\hat{u}_{i}\n\\end{align*}\\]"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#section-3",
    "href": "lectures/04-Estimators-02/040-compile.html#section-3",
    "title": "Estimators Part II",
    "section": "",
    "text": "Step 05: Shut down that last two terms by noticing that:\n\\[\\begin{align*}\n    2\\sum_{i=1}^{n}\\hat{y}_{i}\\hat{u}_{i} -\n    2\\bar{y} \\sum_{i=1}^{n} \\hat{u}_{i} =\n    0\n\\end{align*}\\]\nYou will prove this in an assignment\nThen we have:\n\\[\\begin{align*}\n     \\color{#BF616A}{TSS} = \\color{#8FBCBB}{ESS} + \\color{#D08770}{RSS}\n\\end{align*}\\]\n\nSome visual intuition makes all the math seem a lot simpler"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#section-4",
    "href": "lectures/04-Estimators-02/040-compile.html#section-4",
    "title": "Estimators Part II",
    "section": "",
    "text": "\\[\n\\color{#BF616A}{\\text{TSS}} \\equiv \\sum_{i=1}^n (y_i - \\bar{y})^2\n\\]"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#section-5",
    "href": "lectures/04-Estimators-02/040-compile.html#section-5",
    "title": "Estimators Part II",
    "section": "",
    "text": "\\[\n\\color{#148B25}{\\widehat{\\text{MPG}}_{i}} = 37.3 - 5.34 \\cdot \\text{weight}_i\n\\]"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#section-6",
    "href": "lectures/04-Estimators-02/040-compile.html#section-6",
    "title": "Estimators Part II",
    "section": "",
    "text": "\\[\n\\color{#8FBCBB}{\\text{ESS}} \\equiv \\sum_{i=1}^n (\\hat{y}_{i} - \\bar{y})^2\n\\]"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#section-7",
    "href": "lectures/04-Estimators-02/040-compile.html#section-7",
    "title": "Estimators Part II",
    "section": "",
    "text": "\\[\n\\color{#D08770}{\\text{RSS}} \\equiv \\sum_{i=1}^n \\hat{u}_i^2\n\\]"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#section-8",
    "href": "lectures/04-Estimators-02/040-compile.html#section-8",
    "title": "Estimators Part II",
    "section": "",
    "text": "\\[\n\\color{#BF616A}{\\text{TSS}} \\equiv \\sum_{i=1}^n (Y_i - \\bar{Y})^2\n\\]\n\n\\[\n\\color{#8FBCBB}{\\text{ESS}} \\equiv \\sum_{i=1}^n (\\hat{Y_i} - \\bar{Y})^2\n\\]\n\n\\[\n\\color{#D08770}{\\text{RSS}} \\equiv \\sum_{i=1}^n \\hat{u}_i^2\n\\]"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#goodness-of-fit-3",
    "href": "lectures/04-Estimators-02/040-compile.html#goodness-of-fit-3",
    "title": "Estimators Part II",
    "section": "Goodness of Fit",
    "text": "Goodness of Fit\nWhat percentage of the variation in our \\(y_{i}\\) is apparently explained by our model? The \\(R^{2}\\) term represents this percentage.\nTotal variation is represented by TSS and our model is capturing the ‘explained’ sum of squares, ESS.\nTaking a simple ratio reveals how much variation our model explains:\n\n\\(R^{2} = \\dfrac{\\color{#8FBCBB}{ESS}}{\\color{#BF616A}{TSS}}\\) varies between 0 and 1\n\\(R^{2} = 1 - \\dfrac{\\color{#D08770}{RSS}}{\\color{#BF616A}{TSS}}\\), 100% minus the unexplained variation\n\n\\(R^{2}\\) is related to the correlation between the actual values of \\(y\\) and the fitted values of \\(y\\)."
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#goodness-of-fit-4",
    "href": "lectures/04-Estimators-02/040-compile.html#goodness-of-fit-4",
    "title": "Estimators Part II",
    "section": "Goodness of Fit",
    "text": "Goodness of Fit\nSo what? In the social sciences, low \\(R^{2}\\) values are common.\nLow \\(R^{2}\\) does not necessarily mean you have a “good” regression:\n\nWorries about selection bias and omitted variables still apply\nSome ‘powerfully high’ \\(R^{2}\\) values are the result of simple accounting exercises, and tell us nothing about causality"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#residuals-vs-errors",
    "href": "lectures/04-Estimators-02/040-compile.html#residuals-vs-errors",
    "title": "Estimators Part II",
    "section": "Residuals vs Errors",
    "text": "Residuals vs Errors\n\nThe most important assumptions concern the error term \\(u_{i}\\).\nImportant: An error \\(u_{i}\\) and a residual \\(\\hat{u}_{i}\\) are related, but different.\nTake for example, a model of the effects of education on wages.\n\nError:\n\nDifference between the wage of a worker with 11 years of education and the expected wage with 11 years of education\n\nResidual:\n\nDifference between the wage of a worker with 11 years of education and the average wage of workers with 11 years of education\n\n\n\nPopulation vs. Sample"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#residuals-vs-errors-1",
    "href": "lectures/04-Estimators-02/040-compile.html#residuals-vs-errors-1",
    "title": "Estimators Part II",
    "section": "Residuals vs Errors",
    "text": "Residuals vs Errors\nA residual tells us how a worker’s wages comapre to the average wages of workers in the sample with the same level of education"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#residuals-vs-errors-2",
    "href": "lectures/04-Estimators-02/040-compile.html#residuals-vs-errors-2",
    "title": "Estimators Part II",
    "section": "Residuals vs Errors",
    "text": "Residuals vs Errors\nA residual tells us how a worker’s wages comapre to the average wages of workers in the sample with the same level of education"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#residuals-vs-errors-3",
    "href": "lectures/04-Estimators-02/040-compile.html#residuals-vs-errors-3",
    "title": "Estimators Part II",
    "section": "Residuals vs Errors",
    "text": "Residuals vs Errors\nAn error tells us how a worker’s wages compare to the expected wages of workers in the population with the same level of education"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#classical-assumptions-of-ols",
    "href": "lectures/04-Estimators-02/040-compile.html#classical-assumptions-of-ols",
    "title": "Estimators Part II",
    "section": "Classical Assumptions of OLS",
    "text": "Classical Assumptions of OLS\nA1. Linearity: The population relationship is linear in parameters with an additive error term\nA2. Sample Variation: There is variation in \\(X\\)\nA3. Exogeneity: The \\(X\\) variable is exogenous\nA4. Homosekdasticity: The error term has the same variance for each value of the independent variable\nA5. Non-Autocorrelation: The values of error terms have independent distributions\nA6. Normality: The population error term is normally distributed with mean zero and variance \\(\\sigma^{2}\\)"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#a1.-linearity",
    "href": "lectures/04-Estimators-02/040-compile.html#a1.-linearity",
    "title": "Estimators Part II",
    "section": "A1. Linearity",
    "text": "A1. Linearity\n\nThe population relationship is linear in parameters with an additive error term\n\nExamples\n\n\\(\\text{Wage}_i = \\beta_1 + \\beta_2 \\text{Experience}_i + u_i\\)\n\n\n\n\\(\\log(\\text{Happiness}_i) = \\beta_1 + \\beta_2 \\log(\\text{Money}_i) + u_i\\)\n\n\n\n\n\\(\\sqrt{\\text{Convictions}_i} = \\beta_1 + \\beta_2 (\\text{Early Childhood Lead Exposure})_i + u_i\\)\n\n\n\n\n\\(\\log(\\text{Earnings}_i) = \\beta_1 + \\beta_2 \\text{Education}_i + u_i\\)"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#a1.-linearity-1",
    "href": "lectures/04-Estimators-02/040-compile.html#a1.-linearity-1",
    "title": "Estimators Part II",
    "section": "A1. Linearity",
    "text": "A1. Linearity\n\nThe population relationship is linear in parameters with an additive error term.\n\nViolations\n\n\\(\\text{Wage}_i = (\\beta_1 + \\beta_2 \\text{Experience}_i)u_i\\)\n\n\n\n\\(\\text{Consumption}_i = \\frac{1}{\\beta_1 + \\beta_2 \\text{Income}_i} + u_i\\)\n\n\n\n\n\\(\\text{Population}_i = \\frac{\\beta_1}{1 + e^{\\beta_2 + \\beta_3 \\text{Food}_i}} + u_i\\)\n\n\n\n\n\\(\\text{Batting Average}_i = \\beta_1 (\\text{Wheaties Consumption})_i^{\\beta_2} + u_i\\)"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#a2.-sample-variation",
    "href": "lectures/04-Estimators-02/040-compile.html#a2.-sample-variation",
    "title": "Estimators Part II",
    "section": "A2. Sample Variation",
    "text": "A2. Sample Variation\n\nThere is variation in \\(X\\).\n\nExample"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#a2.-sample-variation-1",
    "href": "lectures/04-Estimators-02/040-compile.html#a2.-sample-variation-1",
    "title": "Estimators Part II",
    "section": "A2. Sample Variation",
    "text": "A2. Sample Variation\n\nThere is variation in \\(X\\).\n\nViolation\n\nWe will see later that variation matters for inference as well"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#a3.-exogeneity",
    "href": "lectures/04-Estimators-02/040-compile.html#a3.-exogeneity",
    "title": "Estimators Part II",
    "section": "A3. Exogeneity",
    "text": "A3. Exogeneity\n\nThe \\(X\\) variable is exogenous\n\nWe can write this as:\n\\[\n    \\mathbb{E}[(u|X)] = 0\n\\]\nWhich essentially says that the expected value of the errors term, conditional on the variable \\(X\\) is 0. The assignment of \\(X\\) is effectively random.\nA significant implication of this is no selection bias or omitted variable bias"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#a3.-exogeneity-1",
    "href": "lectures/04-Estimators-02/040-compile.html#a3.-exogeneity-1",
    "title": "Estimators Part II",
    "section": "A3. Exogeneity",
    "text": "A3. Exogeneity\n\nThe \\(X\\) variable is exogenous\n\n\\[\n    \\mathbb{E}[(u|X)] = 0\n\\]\nExample\nIn the labor market, an important component of \\(u\\) is unobserved ability\n\n\\(\\mathbb{E}(u|\\text{Education} = 12) = 0\\) and \\(\\mathbb{E}(u|\\text{Education} = 20) = 0\\)\n\\(\\mathbb{E}(u|\\text{Education} = 0) = 0\\) and \\(\\mathbb{E}(u|\\text{Education} = 40) = 0\\)\n\nnote: This is an assumption that does not necessarily hold true in real life, but with enough observations we can comfortably assume something like this"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#a3.-exogeneity-2",
    "href": "lectures/04-Estimators-02/040-compile.html#a3.-exogeneity-2",
    "title": "Estimators Part II",
    "section": "A3. Exogeneity",
    "text": "A3. Exogeneity\n\n\nValid Exogeneity\n\\[\n    \\mathbb{E}[(u|X)] = 0\n\\]\n\n\n\n\n\n\nInvalid Exogeneity\n\\[\n    \\mathbb{E}[(u|X)] \\neq 0\n\\]"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#interlude-unbiasedness-of-ols",
    "href": "lectures/04-Estimators-02/040-compile.html#interlude-unbiasedness-of-ols",
    "title": "Estimators Part II",
    "section": "Interlude: Unbiasedness of OLS",
    "text": "Interlude: Unbiasedness of OLS\nWhen can we trust OLS?\nIn estimators, the concept of bias means that the expected value of the estimate is different from the true population parameter.\nGraphically we have:\n\n\nUnbiased estimator: \\(\\mathop{\\mathbb{E}}\\left[ \\hat{\\beta} \\right] = \\beta\\)\n\n\n\n\n\n\n\n\n\n\nBiased estimator: \\(\\mathop{\\mathbb{E}}\\left[ \\hat{\\beta} \\right] \\neq \\beta\\)"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#is-ols-unbiased",
    "href": "lectures/04-Estimators-02/040-compile.html#is-ols-unbiased",
    "title": "Estimators Part II",
    "section": "Is OLS Unbiased?",
    "text": "Is OLS Unbiased?\nWe require our first 3 assumptions for unbaised OLS estimator\nA1. Linearity: The population relationship is linear in parameters with an additive error term\nA2. Sample Variation: There is variation in \\(X\\)\nA3. Exogeneity: The \\(X\\) variable is exogenous\nAnd we can mathematically prove it!"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#proving-unbiasedness-of-ols",
    "href": "lectures/04-Estimators-02/040-compile.html#proving-unbiasedness-of-ols",
    "title": "Estimators Part II",
    "section": "Proving Unbiasedness of OLS",
    "text": "Proving Unbiasedness of OLS\nSuppose we have the following model\n\\[\n    y_{i} = \\beta_{1} + \\beta_{2}x_{i} + u_{i}\n\\]\n\nThe slope parameter follows as:\n\\[\n\\hat{\\beta}_2 = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum(x_i - \\bar{x})^2}\n\\]\n\n\n(As shown in section 2.3 in ItE) that the estimator \\(\\hat{\\beta_2}\\), can be broken up into a nonrandom and a random component:"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#required-assumptions",
    "href": "lectures/04-Estimators-02/040-compile.html#required-assumptions",
    "title": "Estimators Part II",
    "section": "Required Assumptions",
    "text": "Required Assumptions\nA1. Linearity: The population relationship is linear in parameters with an additive error term.\nA2. Sample Variation: There is variation in \\(X\\).\nA3. Exogeniety: The \\(X\\) variable is exogenous\n\nA3 implies random sampling.\n\n\nResult: OLS is unbiased."
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#classical-assumptions-of-ols-1",
    "href": "lectures/04-Estimators-02/040-compile.html#classical-assumptions-of-ols-1",
    "title": "Estimators Part II",
    "section": "Classical Assumptions of OLS",
    "text": "Classical Assumptions of OLS\nA1. Linearity: The population relationship is linear in parameters with an additive error term.\nA2. Sample Variation: There is variation in \\(X\\).\nA3. Exogeniety: The \\(X\\) variable is exogenous\n\n \n\nThe following 2 assumptions are not required for unbiasedness…\n\n\nBut they are important for an efficient estimator\n\n\nLet’s talk about why variance matters"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#why-variance-matters",
    "href": "lectures/04-Estimators-02/040-compile.html#why-variance-matters",
    "title": "Estimators Part II",
    "section": "Why variance matters",
    "text": "Why variance matters\nUnbiasedness tells us that OLS gets it right, on average. But we can’t tell whether our sample is “typical.”\n\n\nVariance tells us how far OLS can deviate from the population mean.\n\nHow tight is OLS centered on its expected value?\nThis determines the efficiency of our estimator."
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#why-variance-matters-1",
    "href": "lectures/04-Estimators-02/040-compile.html#why-variance-matters-1",
    "title": "Estimators Part II",
    "section": "Why variance matters",
    "text": "Why variance matters\nUnbiasedness tells us that OLS gets it right, on average. But we can’t tell whether our sample is “typical.”\n\nThe smaller the variance, the closer OLS gets, on average, to the true population parameters on any sample.\n\nGiven two unbiased estimators, we want the one with smaller variance.\nIf two more assumptions are satisfied, we are using the most efficient linear estimator."
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#classical-assumptions-of-ols-2",
    "href": "lectures/04-Estimators-02/040-compile.html#classical-assumptions-of-ols-2",
    "title": "Estimators Part II",
    "section": "Classical Assumptions of OLS",
    "text": "Classical Assumptions of OLS\nA1. Linearity: The population relationship is linear in parameters with an additive error term.\nA2. Sample Variation: There is variation in \\(X\\).\nA3. Exogeniety: The \\(X\\) variable is exogenous\n\nA4. Homoskedasticity: The error term has the same variance for each value of the independent variable"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#a4.-homoskedasticity",
    "href": "lectures/04-Estimators-02/040-compile.html#a4.-homoskedasticity",
    "title": "Estimators Part II",
    "section": "A4. Homoskedasticity",
    "text": "A4. Homoskedasticity\n\nThe error term has the same variance for each value of the independent variable \\(x_{i}\\)\n\n\\[\n    Var(u|X) = \\sigma^{2}.\n\\]\nExample:"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#a4.-homoskedasticity-1",
    "href": "lectures/04-Estimators-02/040-compile.html#a4.-homoskedasticity-1",
    "title": "Estimators Part II",
    "section": "A4. Homoskedasticity",
    "text": "A4. Homoskedasticity\n\nThe error term has the same variance for each value of the independent variable \\(x_{i}\\)\n\n\\[\n    Var(u|X) = \\sigma^{2}.\n\\]\nViolation:"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#a4.-homoskedasticity-2",
    "href": "lectures/04-Estimators-02/040-compile.html#a4.-homoskedasticity-2",
    "title": "Estimators Part II",
    "section": "A4. Homoskedasticity",
    "text": "A4. Homoskedasticity\n\nThe error term has the same variance for each value of the independent variable \\(x_{i}\\)\n\n\\[\n    Var(u|X) = \\sigma^{2}.\n\\]\nViolation:"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#heteroskedasticity-example",
    "href": "lectures/04-Estimators-02/040-compile.html#heteroskedasticity-example",
    "title": "Estimators Part II",
    "section": "Heteroskedasticity Example",
    "text": "Heteroskedasticity Example\nSuppose we study the following relationship:\n\\[\n\\text{Luxury Expenditure}_i = \\beta_1 + \\beta_2 \\text{Income}_i + u_i\n\\]\n\nAs income increases, variation in luxury expenditures increase\n\nVariance of \\(u_i\\) is likely larger for higher-income households\nPlot of the residuals against the household income would likely reveal a funnel-shaped pattern"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#section-9",
    "href": "lectures/04-Estimators-02/040-compile.html#section-9",
    "title": "Estimators Part II",
    "section": "",
    "text": "Common test for heteroskedasticity… Plot the residuals across \\(X\\)"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#classical-assumptions-of-ols-3",
    "href": "lectures/04-Estimators-02/040-compile.html#classical-assumptions-of-ols-3",
    "title": "Estimators Part II",
    "section": "Classical Assumptions of OLS",
    "text": "Classical Assumptions of OLS\nA1. Linearity: The population relationship is linear in parameters with an additive error term.\nA2.Sample Variation: There is variation in \\(X\\).\nA3. Exogeniety: The \\(X\\) variable is exogenous\nA4. Homoskedasticity: The error term has the same variance for each value of the independent variable\n\nA5. Non-autocorrelation: The values of error terms have independent distributions"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#a5.-non-autocorrelation",
    "href": "lectures/04-Estimators-02/040-compile.html#a5.-non-autocorrelation",
    "title": "Estimators Part II",
    "section": "A5. Non-Autocorrelation",
    "text": "A5. Non-Autocorrelation\n\nThe values of error terms have independent distributions1\n\n\\[\nE[u_i u_j]=0, \\forall i \\text{ s.t. } i \\neq j\n\\]\n\nOr…\n\\[\n\\begin{align*}\n\\mathop{\\text{Cov}}(u_i, u_j) &= E[(u_i - \\mu_u)(u_j - \\mu_u)]\\\\\n                              &= E[u_i u_j] = E[u_i] E[u_j]  = 0, \\text{where } i \\neq j\n\\end{align*}\n\\]\n\nNotes: \\(\\forall i = \\text{for all} \\: i\\), \\(\\text{s.t.} = \\text{such that}\\), \\(i \\neq j \\: \\text{means} \\: i \\: \\text{is not equal to} \\: j\\)"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#a5.-non-autocorrelation-1",
    "href": "lectures/04-Estimators-02/040-compile.html#a5.-non-autocorrelation-1",
    "title": "Estimators Part II",
    "section": "A5. Non-Autocorrelation",
    "text": "A5. Non-Autocorrelation\n\nThe values of error terms have independent distributions\n\n\\[\nE[u_i u_j]=0, \\forall i \\text{ s.t. } i \\neq j\n\\]\n\nImplies no systematic association between pairs of individual \\(u_i\\)\nAlmost always some unobserved correlation across individuals1\nReferred to as clustering problem.\nAn easy solution exists where we can adjust our standard errors\n\n(e.g. common correlation in unobservables among individuals within a given US state)"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#section-10",
    "href": "lectures/04-Estimators-02/040-compile.html#section-10",
    "title": "Estimators Part II",
    "section": "",
    "text": "Let’s take a moment to talk about the variance of the OLS estimator\n\n\\[\n    Var(\\hat{\\beta}_{1}) = \\dfrac{\n        \\sigma^{2}\n        }{\n        \\sum (x_{i} - \\bar{x})^{2}\n        }\n\\]"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#classical-assumptions-of-ols-4",
    "href": "lectures/04-Estimators-02/040-compile.html#classical-assumptions-of-ols-4",
    "title": "Estimators Part II",
    "section": "Classical Assumptions of OLS",
    "text": "Classical Assumptions of OLS\nA1. Linearity: The population relationship is linear in parameters with an additive error term.\nA2. Sample Variation: There is variation in \\(X\\).\nA3. Exogeniety: The \\(X\\) variable is exogenous\nA4. Homoskedasticity: The error term has the same variance for each value of the independent variable\nA5. Non-autocorrelation: The values of error terms have independent distributions\n\nIf A4 and A5 are satisfied, along with A1, A2, and A3, then we are using the most efficient linear estimator"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#classical-assumptions-of-ols-5",
    "href": "lectures/04-Estimators-02/040-compile.html#classical-assumptions-of-ols-5",
    "title": "Estimators Part II",
    "section": "Classical Assumptions of OLS",
    "text": "Classical Assumptions of OLS\nA1. Linearity: The population relationship is linear in parameters with an additive error term.\nA2. Sample Variation: There is variation in \\(X\\).\nA3. Exogeniety: The \\(X\\) variable is exogenous\nA4. Homoskedasticity: The error term has the same variance for each value of the independent variable\nA5. Non-autocorrelation: The values of error terms have independent distributions\n\nA6. Normality The population error term in normally distributed with mean zero and variance \\(\\sigma^{2}\\)"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#a6.-normality",
    "href": "lectures/04-Estimators-02/040-compile.html#a6.-normality",
    "title": "Estimators Part II",
    "section": "A6. Normality",
    "text": "A6. Normality\n\nThe population error term in normally distributed with mean zero and variance \\(\\sigma^{2}\\)\n\nAlso known as:\n\\[\n    u \\sim N(0,\\sigma^{2})\n\\]\nWhere \\(\\sim\\) means distributed by and \\(N\\) stands for normal distribution\nHowever, A6 is not required for efficiency nor unbiasedness"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#gauss-markov-theorem-1",
    "href": "lectures/04-Estimators-02/040-compile.html#gauss-markov-theorem-1",
    "title": "Estimators Part II",
    "section": "Gauss-Markov Theorem",
    "text": "Gauss-Markov Theorem\n\nOLS is the Best Linear Unbiased Estimator (BLUE) when the following assumptions hold:\n\n\nA1. Linearity: The population relationship is linear in parameters with an additive error term.\nA2. Sample Variation: There is variation in \\(X\\).\nA3. Exogeniety: The \\(X\\) variable is exogenous\nA4. Homoskedasticity: The error term has the same variance for each value of the independent variable\nA5. Non-autocorrelation: The values of error terms have independent distributions"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#gauss-markov-theorem-2",
    "href": "lectures/04-Estimators-02/040-compile.html#gauss-markov-theorem-2",
    "title": "Estimators Part II",
    "section": "Gauss-Markov Theorem",
    "text": "Gauss-Markov Theorem\n\nOLS is the Best Unbiased Estimator (BUE) when the following assumptions hold:\n\n\nA1. Linearity: The population relationship is linear in parameters with an additive error term.\nA2. Sample Variation: There is variation in \\(X\\).\nA3. Exogeniety: The \\(X\\) variable is exogenous\nA4. Homoskedasticity: The error term has the same variance for each value of the independent variable\nA5. Non-autocorrelation: The values of error terms have independent distributions\nA6. Normality: The population error term in normally distributed with mean zero and variance \\(\\sigma^2\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/034-ols.html",
    "href": "lectures/03-Estimators-01/034-ols.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "The OLS Estimator chooses the parameters \\(\\hat{\\beta}_{0}\\) and \\(\\hat{\\beta}_{1}\\) that minimize the Residual Sum of Squares (RSS)\n\\[\n    \\min_{\\hat{\\beta}_{0},\\hat{\\beta}_{1}} \\sum_{i=1}^{n} \\hat{u}_{i}^{2}\n\\]\nThis is why we call the estimator ordinary least squares\nRecall that residuals are given by \\(y_{i} - \\hat{y}_{i}\\) and that:\n\\[\n    \\hat{y}_{i} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1} x_{i}\n\\]\nThen\n\\[\n    u_{i} = y_{i} - \\hat{\\beta}_{0} + \\hat{\\beta}_{1} x_{i}\n\\]"
  },
  {
    "objectID": "lectures/03-Estimators-01/034-ols.html#ols",
    "href": "lectures/03-Estimators-01/034-ols.html#ols",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "The OLS Estimator chooses the parameters \\(\\hat{\\beta}_{0}\\) and \\(\\hat{\\beta}_{1}\\) that minimize the Residual Sum of Squares (RSS)\n\\[\n    \\min_{\\hat{\\beta}_{0},\\hat{\\beta}_{1}} \\sum_{i=1}^{n} \\hat{u}_{i}^{2}\n\\]\nThis is why we call the estimator ordinary least squares\nRecall that residuals are given by \\(y_{i} - \\hat{y}_{i}\\) and that:\n\\[\n    \\hat{y}_{i} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1} x_{i}\n\\]\nThen\n\\[\n    u_{i} = y_{i} - \\hat{\\beta}_{0} + \\hat{\\beta}_{1} x_{i}\n\\]"
  },
  {
    "objectID": "lectures/03-Estimators-01/034-ols.html#ols-calculus",
    "href": "lectures/03-Estimators-01/034-ols.html#ols-calculus",
    "title": "EC 320 - Intro. Econometrics",
    "section": "OLS & Calculus",
    "text": "OLS & Calculus\nWe can find our choices \\(\\hat{\\beta}_{0}\\) and \\(\\hat{\\beta}_{1}\\) to minimize our residuals using calculus\nA minimization problem is essentially the same as an optimization problem where we find the point at which our choices have a slope of zero\nTo begin, let’s properly write out our minimization problem:\n\\[\n    \\min_{\\hat{\\beta}_{0}, \\hat{\\beta}_{1}} \\;\\; \\sum_{i} u_{i}^{2}\n\\]\n\\[\n    \\min_{\\hat{\\beta}_{0}, \\hat{\\beta}_{1}} \\; \\sum_{i} (y_{i} - \\hat{y}_{i})^{2}\n\\]\n\\[\n    \\min_{\\hat{\\beta}_{0}, \\hat{\\beta}_{1}} \\; \\sum_{i} (y_{i} - \\hat{\\beta}_{0} - \\hat{\\beta}_{1}x_{i}) (y_{i} - \\hat{\\beta}_{0} - \\hat{\\beta}_{1}x_{i})\n\\]"
  },
  {
    "objectID": "lectures/03-Estimators-01/034-ols.html#ols-calculus-1",
    "href": "lectures/03-Estimators-01/034-ols.html#ols-calculus-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "OLS & Calculus",
    "text": "OLS & Calculus\nThe calculus we’ll use is by finding the derivatives of the function with respect to \\(\\hat{\\beta}_{0}\\) and \\(\\hat{\\beta}_{1}\\).\nIt’s a lot of algebra but it is simple math, just a lot of it:\n\n\\[\\begin{align*}\n    \\min_{\\hat{\\beta}_{0}, \\hat{\\beta}_{1}} &\\;\n    \\sum_{i} y_{i}^{2} - \\hat{\\beta}_{0}y_{i} - \\hat{\\beta}_{1}x_{i}y_{i} - \\hat{\\beta}_{0}y_{i} + \\hat{\\beta}_{0}^{2} + \\hat{\\beta}_{0}\\hat{\\beta}_{1}x_{i} - \\hat{\\beta}_{1}x_{i}y_{i} + \\hat{\\beta}_{0}\\hat{\\beta}_{1}x_{i} + \\hat{\\beta}_{1}^{2}x_{i}^{2} \\\\\n    \\min_{\\hat{\\beta}_{0}, \\hat{\\beta}_{1}} &\\;\n    \\sum_{i} y_{i}^{2} - 2 \\hat{\\beta}_{0}y_{i} + \\hat{\\beta}_{0}^{2} - 2 \\hat{\\beta}_{1}x_{i}y_{i} + 2\\hat{\\beta}_{0}\\hat{\\beta}_{1}x_{i} + \\hat{\\beta}_{1}^{2}x_{i}^{2}\n\\end{align*}\\]\n\nThen, we take partial derivatives over our choices \\(\\hat{\\beta}_{0}\\) and \\(\\hat{\\beta}_{1}\\) to figure the best choices.\nThese are called First Order Conditions (FOCs)"
  },
  {
    "objectID": "lectures/03-Estimators-01/034-ols.html#ols-calculus-2",
    "href": "lectures/03-Estimators-01/034-ols.html#ols-calculus-2",
    "title": "EC 320 - Intro. Econometrics",
    "section": "OLS & Calculus",
    "text": "OLS & Calculus\nTo find our choices, we find the partial derivative and set it equal to 0\nFor our intercept \\(\\hat{\\beta}_{0}\\):\n\n\\[\\begin{align*}\n    &\\dfrac{\\partial u_{i}}{\\partial \\hat{\\beta}_{0}} = 0 \\\\\n    \\sum_{i} -2y_{i} + &2\\hat{\\beta}_{0} + 2\\hat{\\beta}_{1}x_{i} = 0\n\\end{align*}\\]\n\nFor our slope \\(\\hat{\\beta}_{1}\\):\n\n\\[\\begin{align*}\n    &\\dfrac{\\partial u_{i}}{\\partial \\hat{\\beta}_{1}} = 0 \\\\\n    \\sum_{i} -2x_{i}y_{i} + &2\\hat{\\beta}_{0}x_{i} + 2\\hat{\\beta}_{1}x_{i}^{2} = 0\n\\end{align*}\\]"
  },
  {
    "objectID": "lectures/03-Estimators-01/034-ols.html#hatbeta_0-derivation",
    "href": "lectures/03-Estimators-01/034-ols.html#hatbeta_0-derivation",
    "title": "EC 320 - Intro. Econometrics",
    "section": "\\(\\hat{\\beta}_{0}\\) Derivation",
    "text": "\\(\\hat{\\beta}_{0}\\) Derivation\n\\[\n    \\sum_{i} -2y_{i} + 2\\hat{\\beta}_{0} + 2\\hat{\\beta}_{1}x_{i} = 0\n\\]\nOur task is to find solve the above for \\(\\hat{\\beta}_{0}\\):"
  },
  {
    "objectID": "lectures/03-Estimators-01/034-ols.html#hatbeta_1-derivation",
    "href": "lectures/03-Estimators-01/034-ols.html#hatbeta_1-derivation",
    "title": "EC 320 - Intro. Econometrics",
    "section": "\\(\\hat{\\beta}_{1}\\) Derivation",
    "text": "\\(\\hat{\\beta}_{1}\\) Derivation\n\\[\n    \\sum_{i} -2x_{i}y_{i} + 2\\hat{\\beta}_{0}x_{i} + 2\\hat{\\beta}_{1}x_{i}^{2} = 0\n\\]\nOur task is to find solve the above for \\(\\hat{\\beta}_{1}\\):"
  },
  {
    "objectID": "lectures/03-Estimators-01/034-ols.html#ols-formulas",
    "href": "lectures/03-Estimators-01/034-ols.html#ols-formulas",
    "title": "EC 320 - Intro. Econometrics",
    "section": "OLS Formulas",
    "text": "OLS Formulas\n\nIntercept\n\\[\n    \\hat{\\beta}_{0} = \\bar{y} - \\hat{\\beta}_{1}\\bar{x}\n\\]\n\nSlope Coefficient\n\\[\n    \\hat{\\beta}_{1} =\n    \\dfrac{\n        \\sum_{i=1}^{n} (y_{i} - \\bar{y})(x_{i} - \\bar{x})\n    }{\n        \\sum_{i=1}^{n} (x_{i} - \\bar{x})^{2}\n    }\n\\]\nThese may look slightly different to my derivation. Part of your assignments is to bridge the gap."
  },
  {
    "objectID": "lectures/03-Estimators-01/032-linear-model.html",
    "href": "lectures/03-Estimators-01/032-linear-model.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Before we continue, let’s cover some important rules we will need to derive some OLS things in the near future:\nSummations \\((\\sum)\\) have certain rules that we cannot violate and are important to hold in mind:\n\n\n\\(\\sum_{i=1}^{n} x_{i} = x_{1} + x_{2} + \\cdots + x_{n}\\)\n\n\n\n\\(\\sum_{i} x_{i} + y_{i} = \\sum_{i} x_{i} + \\sum_{i} y_{i}\\)\n\n\n\n\\(\\sum_{i} x_{i} y_{i} \\neq \\sum_{i} x_{i} \\sum_{i} y_{i}\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/032-linear-model.html#sidebar-summation-rules",
    "href": "lectures/03-Estimators-01/032-linear-model.html#sidebar-summation-rules",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Before we continue, let’s cover some important rules we will need to derive some OLS things in the near future:\nSummations \\((\\sum)\\) have certain rules that we cannot violate and are important to hold in mind:\n\n\n\\(\\sum_{i=1}^{n} x_{i} = x_{1} + x_{2} + \\cdots + x_{n}\\)\n\n\n\n\\(\\sum_{i} x_{i} + y_{i} = \\sum_{i} x_{i} + \\sum_{i} y_{i}\\)\n\n\n\n\\(\\sum_{i} x_{i} y_{i} \\neq \\sum_{i} x_{i} \\sum_{i} y_{i}\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/032-linear-model.html#summation-rules",
    "href": "lectures/03-Estimators-01/032-linear-model.html#summation-rules",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Summation Rules",
    "text": "Summation Rules\n\\[\\sum_{i=1}^{n} x_{i} = x_{1} + x_{2} + \\cdots + x_{n}\\]\nLet \\(x\\) be the set of \\({1,5,2}\\) \\((x: \\{1,5,2\\})\\) then using our summation rule we have:\n\\[\n    \\sum_{i} x_{i} = 1 + 5 + 2 = 8.\n\\]"
  },
  {
    "objectID": "lectures/03-Estimators-01/032-linear-model.html#summation-rules-1",
    "href": "lectures/03-Estimators-01/032-linear-model.html#summation-rules-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Summation Rules",
    "text": "Summation Rules\n\\[\\sum_{i} x_{i} + y_{i} = \\sum_{i} x_{i} + \\sum_{i} y_{i}\\]\nLet \\(x: \\{1,5,2\\}\\) and \\(y: \\{1,2,1\\}\\), then using our summation rule we have:\n\\[\\begin{align*}\n    \\sum_{i} x_{i} + y_{i} &= x_{1} + y_{1} + x_{2} + y_{2} + x_{3} + y_{3} \\\\\n                           &= x_{1} + x_{2} + x_{3} + y_{1} + y_{2} + y_{3} \\\\\n                           &= 1 + 5 + 2 + 1 + 2 + 1 \\\\\n                           &= 12\n\\end{align*}\\]"
  },
  {
    "objectID": "lectures/03-Estimators-01/032-linear-model.html#summation-rules-2",
    "href": "lectures/03-Estimators-01/032-linear-model.html#summation-rules-2",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Summation Rules",
    "text": "Summation Rules\n\\[\\sum_{i} x_{i} y_{i} \\neq \\sum_{i} x_{i} \\sum_{i} y_{i}\\]\nIf we expand \\(\\sum_{i} x_{i} y_{i} \\neq \\sum_{i} x_{i} \\sum_{i} y_{i}\\), we get:\n\n\\[\\begin{align*}\n    x_{1}y_{1} + x_{2}y_{2} + x_{3}y_{3} \\neq (x_{1} + x_{2} + x_{3})(y_{1} + y_{2} + y_{3})\n\\end{align*}\\]\n\nI’ll leave it to you to use the above numbers to show this holds"
  },
  {
    "objectID": "lectures/03-Estimators-01/032-linear-model.html#linear-model-estimators",
    "href": "lectures/03-Estimators-01/032-linear-model.html#linear-model-estimators",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Linear Model Estimators",
    "text": "Linear Model Estimators\nWe will spend the rest of the course exploring how to use Ordinary Least Squares (OLS) to fit a linear model like:\n\\[\n    y_{i} = \\beta_{0} + \\beta_{1}x_{i} + u_{i},\n\\]\nThat is, if we wanted to hypothesize that some random variable \\(Y\\) depends on another random variable \\(X\\) and that there is a linear relationship between then, \\(\\beta_{0}\\) and \\(\\beta_{1}\\) are the parameters which describe the nature of that relationship.\nGiven a sample of \\(X\\) and \\(Y\\), we will derive unbiased estimators for the intercept \\(\\beta_{0}\\) and slope \\(\\beta_{1}\\). Those estimators help us combine observations of \\(X\\) and \\(Y\\) to estimate underlying relationships between them."
  },
  {
    "objectID": "lectures/03-Estimators-01/032-linear-model.html#the-linear-regression-model",
    "href": "lectures/03-Estimators-01/032-linear-model.html#the-linear-regression-model",
    "title": "EC 320 - Intro. Econometrics",
    "section": "The Linear Regression Model",
    "text": "The Linear Regression Model\nWe can estimate the effect of \\(X\\) on \\(Y\\) by estimating the model:\n\\[\n    y_{i} = \\beta_{0} + \\beta_{1}x_{i} + u_{i},\n\\]\n\n\\(y_i\\) is the dependent variable\n\\(x_i\\) is the independent variable (continuous)\n\\(\\beta_0\\) is the intercept parameter. \\(E\\left[ {y_i | x_i=0} \\right] = \\beta_0\\)\n\\(\\beta_1\\) is the slope parameter, which under the correct causal setting represents marginal change in \\(x_i\\)’s effect on \\(y_i\\). \\(\\frac{\\partial y_i}{\\partial x_i} = \\beta_1\\)\n\\(u_i\\) is an Error Term including all other (omitted) factors affecting \\(y_i\\)."
  },
  {
    "objectID": "lectures/03-Estimators-01/032-linear-model.html#the-error-term-u_i",
    "href": "lectures/03-Estimators-01/032-linear-model.html#the-error-term-u_i",
    "title": "EC 320 - Intro. Econometrics",
    "section": "The Error Term \\(u_{i}\\)",
    "text": "The Error Term \\(u_{i}\\)\n\\(u_{i}\\) is quite special\nConsider the data generating process of variable \\(y_{i}\\),\n\n\\(u_{i}\\) captures all unobserved variables that explain variation in \\(y_{i}\\)\n\n\nSome error will exist in all models, no model is perfect.\n\nOur aim is to minimize error under a set of constraints\n\n\nError is the price we are willing to accept for a simplified model"
  },
  {
    "objectID": "lectures/03-Estimators-01/032-linear-model.html#the-error-term",
    "href": "lectures/03-Estimators-01/032-linear-model.html#the-error-term",
    "title": "EC 320 - Intro. Econometrics",
    "section": "The Error Term",
    "text": "The Error Term\nFive items contribute to the existence of the disturbance term:\n. . .\n1. Omission of independent variables\n. . .\n\nOur description (model) of the relationship between \\(Y\\) and \\(X\\) is a simplification\nOther variables have been left out (omitted)"
  },
  {
    "objectID": "lectures/03-Estimators-01/032-linear-model.html#the-error-term-1",
    "href": "lectures/03-Estimators-01/032-linear-model.html#the-error-term-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "The Error Term",
    "text": "The Error Term\nFive items contribute to the existence of the disturbance term:\n1. Omission of independent variables\n2. Aggregation of Variables\n. . .\n\nMicroeconomic relationships are often summarized\nEx. Housing prices (\\(X\\)) are described by county-level median home value data"
  },
  {
    "objectID": "lectures/03-Estimators-01/032-linear-model.html#the-error-term-2",
    "href": "lectures/03-Estimators-01/032-linear-model.html#the-error-term-2",
    "title": "EC 320 - Intro. Econometrics",
    "section": "The Error Term",
    "text": "The Error Term\nFive items contribute to the existence of the disturbance term:\n1. Omission of independent variables\n2. Aggregation of Variables\n3. Model misspecificiation\n. . .\n\nModel structure is incorrectly specified\nEx. \\(Y\\) depends on the anticipated value of \\(X\\) in the previous period, not \\(X\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/032-linear-model.html#the-error-term-3",
    "href": "lectures/03-Estimators-01/032-linear-model.html#the-error-term-3",
    "title": "EC 320 - Intro. Econometrics",
    "section": "The Error Term",
    "text": "The Error Term\nFive items contribute to the existence of the disturbance term:\n1. Omission of independent variables\n2. Aggregation of Variables\n3. Model misspecificiation\n4. Functional misspecificiation\n. . .\n\nThe functional relationship is specified incorrectly\nTrue relationship is nonlinear, not linear"
  },
  {
    "objectID": "lectures/03-Estimators-01/032-linear-model.html#the-error-term-4",
    "href": "lectures/03-Estimators-01/032-linear-model.html#the-error-term-4",
    "title": "EC 320 - Intro. Econometrics",
    "section": "The Error Term",
    "text": "The Error Term\nFive items contribute to the existence of the disturbance term:\n1. Omission of independent variables\n2. Aggregation of Variables\n3. Model misspecificiation\n4. Functional misspecificiation\n5. Measurement error\n. . .\n\nMeasurement of the variables in the data is just wrong\n\\(Y\\) or \\(X\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/032-linear-model.html#the-error-term-5",
    "href": "lectures/03-Estimators-01/032-linear-model.html#the-error-term-5",
    "title": "EC 320 - Intro. Econometrics",
    "section": "The Error Term",
    "text": "The Error Term\nFive items contribute to the existence of the disturbance term:\n1. Omission of independent variables\n2. Aggregation of Variables\n3. Model misspecificiation\n4. Functional misspecificiation\n5. Measurement error"
  },
  {
    "objectID": "lectures/03-Estimators-01/032-linear-model.html#running-a-regression-model",
    "href": "lectures/03-Estimators-01/032-linear-model.html#running-a-regression-model",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Running a Regression Model",
    "text": "Running a Regression Model\nUsing an estimator with data on \\(x_{i}\\) and \\(y_{i}\\), we can estimate a fitted regression line:\n\\[\n    \\hat{y}_{i} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1}x_{i}\n\\]\n\n\\(\\hat{y}_{i}\\) is the fitted value of \\(y_{i}\\)\n\\(\\hat{\\beta}_{0}\\) is the estimated intercept\n\\(\\hat{\\beta}_{1}\\) is the estimated slope\n\nThis procedure produces misses, known as residuals \\(y_{i} - \\hat{y_{i}}\\)\nLet’s look at an example of how this works"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#why-estimate-things",
    "href": "lectures/03-Estimators-01/030-compile.html#why-estimate-things",
    "title": "Estimators",
    "section": "Why Estimate Things?",
    "text": "Why Estimate Things?\nWe estimate because we cannot measure everything\nSuppose we want to know the average height of the US population.\n\nWe only have a sample of 1 million Americans\n\nHow can we use these data to estimate the height of the population?\nWe will learn what we can do"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#estimators-1",
    "href": "lectures/03-Estimators-01/030-compile.html#estimators-1",
    "title": "Estimators",
    "section": "Estimators",
    "text": "Estimators\nLet’s define some concepts first:\nEstimand\n\nQuantity that is to be estimated in a statistical analysis\n\nEstimator\n\nA rule (or formula) for estimating an unknown population parameter given a sample of data\n\nEstimate\n\nA specific numerical value that we obtain from the smaple data by applying the estimator"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#estimators-example",
    "href": "lectures/03-Estimators-01/030-compile.html#estimators-example",
    "title": "Estimators",
    "section": "Estimators Example",
    "text": "Estimators Example\nSuppose we want to know the average height of the population in the US\n\nWe have a sample of 1 million Americans\n\nSo then we can identify our Estimand, Estimator, and Estimate\n\nEstimand: The population mean \\((\\mu)\\)\nEstimator: The sample mean \\((\\bar{X})\\)\n\n\\[\n    \\bar{X} = \\dfrac{1}{n} \\sum_{i=1}^{n} X_{i}\n\\]\n\nEstimate: The sample mean \\((\\hat{\\mu} = 5'6'')\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#properties-of-estimators",
    "href": "lectures/03-Estimators-01/030-compile.html#properties-of-estimators",
    "title": "Estimators",
    "section": "Properties of Estimators",
    "text": "Properties of Estimators\nThere are many ways to estimate things and they all have their benefits and costs.\nImagine we want to estimate an unknown parameter \\(\\mu\\), and we know the distributions of three competing estimators.\nWhich one should we use?"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#properties-of-estimators---unbiasedness",
    "href": "lectures/03-Estimators-01/030-compile.html#properties-of-estimators---unbiasedness",
    "title": "Estimators",
    "section": "Properties of Estimators - Unbiasedness",
    "text": "Properties of Estimators - Unbiasedness\nWe ask: What properties make an estimator reliable?\nAnswer (1): Unbiasedness\nOn average, does the estimator tend toward the correct value?\n\nFormally: Does the mean of the estimator’s distribution equal the parameter it estimates?\n\\[\n    \\text{Bias}_{\\mu} (\\hat{\\mu}) = E[\\hat{\\mu}] - \\mu\n\\]"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#properties-of-estimators-1",
    "href": "lectures/03-Estimators-01/030-compile.html#properties-of-estimators-1",
    "title": "Estimators",
    "section": "Properties of estimators",
    "text": "Properties of estimators\nQuestion What properties make an estimator reliable?\nA01: Unbiasedness\n\n\nUnbiased estimator: \\(E\\left[ \\hat{\\mu} \\right] = \\mu\\)\n\n\n\n\n\n\n\n\n\n\nBiased estimator \\(E\\left[ \\hat{\\mu} \\right] \\neq \\mu\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#properties-of-estimators---efficiency",
    "href": "lectures/03-Estimators-01/030-compile.html#properties-of-estimators---efficiency",
    "title": "Estimators",
    "section": "Properties of Estimators - Efficiency",
    "text": "Properties of Estimators - Efficiency\nWe ask: What properties make an estimator reliable?\nAnswer (1): Efficiency (Low Variance)\nThe central tendencies (means) of competing distribution are not the only things that matter. We also care about the variance of an estimator.\n\\[\n    Var(\\hat{\\mu}) = E \\left[ (\\hat{\\mu} - E[\\hat{\\mu}])^{2} \\right]\n\\]\nLower variance estimators estimate closer to the mean in each sample"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#properties-of-estimators---efficiency-1",
    "href": "lectures/03-Estimators-01/030-compile.html#properties-of-estimators---efficiency-1",
    "title": "Estimators",
    "section": "Properties of Estimators - Efficiency",
    "text": "Properties of Estimators - Efficiency\nImagine low variance to be similar to accuracy \\(\\rightarrow\\) tighter estimates"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#the-bias-variance-tradeoff",
    "href": "lectures/03-Estimators-01/030-compile.html#the-bias-variance-tradeoff",
    "title": "Estimators",
    "section": "The Bias-Variance Tradeoff",
    "text": "The Bias-Variance Tradeoff\nMuch like everything, there are tradeoffs from gaining one thing over another.\nShould we be willing to take a bit of bias to reduce the variance?\nIn economics/causal inference, we emphasize unbiasedness"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#unbiased-estimators",
    "href": "lectures/03-Estimators-01/030-compile.html#unbiased-estimators",
    "title": "Estimators",
    "section": "Unbiased estimators",
    "text": "Unbiased estimators\nIn addition to the sample mean, there are other unbiased estimators we will often use\n\n\nSample variance estimates the variance \\(\\sigma^{2}\\)\n\n\n\nSample covariance setimates the covariance \\(\\sigma_{XY}\\)\n\n\n\nSample correlation estimates the pop. correlation coefficient \\(\\rho_{XY}\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#sample-variance",
    "href": "lectures/03-Estimators-01/030-compile.html#sample-variance",
    "title": "Estimators",
    "section": "Sample Variance",
    "text": "Sample Variance\nThe sample variance, \\(S_{X}^{2}\\), is an unbiased estimator of the population variance\n\n\\[\n    S_{X}^{2} = \\dfrac{1}{n - 1} \\sum_{i=1}^{n} (X_{i} - \\bar{X})^{2}.\n\\]"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#sample-covariance",
    "href": "lectures/03-Estimators-01/030-compile.html#sample-covariance",
    "title": "Estimators",
    "section": "Sample Covariance",
    "text": "Sample Covariance\nThe sample covariance, \\(S_{XY}\\), is an unbiaed estimator of the population covariance\n\n\\[\n    S_{XY} = \\dfrac{1}{n-1} \\sum_{i=1}^{n} (X_{i} - \\bar{X})(Y_{i} - \\bar{Y}).\n\\]"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#sample-correlation",
    "href": "lectures/03-Estimators-01/030-compile.html#sample-correlation",
    "title": "Estimators",
    "section": "Sample Correlation",
    "text": "Sample Correlation\nSample correlation, \\(r_{XY}\\), is an unbiased estimator of the population correlation coefficient\n\n\\[\n    r_{XY} = \\dfrac{S_{XY}}{\\sqrt{S_{X}^{2}}\\sqrt{S_{Y}^{2}}}.\n\\]"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#sidebar-summation-rules",
    "href": "lectures/03-Estimators-01/030-compile.html#sidebar-summation-rules",
    "title": "Estimators",
    "section": "Sidebar: Summation Rules",
    "text": "Sidebar: Summation Rules\nBefore we continue, let’s cover some important rules we will need to derive some OLS things in the near future:\nSummations \\((\\sum)\\) have certain rules that we cannot violate and are important to hold in mind:\n\n\n\\(\\sum_{i=1}^{n} x_{i} = x_{1} + x_{2} + \\cdots + x_{n}\\)\n\n\n\n\\(\\sum_{i} x_{i} + y_{i} = \\sum_{i} x_{i} + \\sum_{i} y_{i}\\)\n\n\n\n\\(\\sum_{i} x_{i} y_{i} \\neq \\sum_{i} x_{i} \\sum_{i} y_{i}\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#summation-rules",
    "href": "lectures/03-Estimators-01/030-compile.html#summation-rules",
    "title": "Estimators",
    "section": "Summation Rules",
    "text": "Summation Rules\n\\[\\sum_{i=1}^{n} x_{i} = x_{1} + x_{2} + \\cdots + x_{n}\\]\nLet \\(x\\) be the set of \\({1,5,2}\\) \\((x: \\{1,5,2\\})\\) then using our summation rule we have:\n\\[\n    \\sum_{i} x_{i} = 1 + 5 + 2 = 8.\n\\]"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#summation-rules-1",
    "href": "lectures/03-Estimators-01/030-compile.html#summation-rules-1",
    "title": "Estimators",
    "section": "Summation Rules",
    "text": "Summation Rules\n\\[\\sum_{i} x_{i} + y_{i} = \\sum_{i} x_{i} + \\sum_{i} y_{i}\\]\nLet \\(x: \\{1,5,2\\}\\) and \\(y: \\{1,2,1\\}\\), then using our summation rule we have:\n\\[\\begin{align*}\n    \\sum_{i} x_{i} + y_{i} &= x_{1} + y_{1} + x_{2} + y_{2} + x_{3} + y_{3} \\\\\n                           &= x_{1} + x_{2} + x_{3} + y_{1} + y_{2} + y_{3} \\\\\n                           &= 1 + 5 + 2 + 1 + 2 + 1 \\\\\n                           &= 12\n\\end{align*}\\]"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#summation-rules-2",
    "href": "lectures/03-Estimators-01/030-compile.html#summation-rules-2",
    "title": "Estimators",
    "section": "Summation Rules",
    "text": "Summation Rules\n\\[\\sum_{i} x_{i} y_{i} \\neq \\sum_{i} x_{i} \\sum_{i} y_{i}\\]\nIf we expand \\(\\sum_{i} x_{i} y_{i} \\neq \\sum_{i} x_{i} \\sum_{i} y_{i}\\), we get:\n\n\\[\\begin{align*}\n    x_{1}y_{1} + x_{2}y_{2} + x_{3}y_{3} \\neq (x_{1} + x_{2} + x_{3})(y_{1} + y_{2} + y_{3})\n\\end{align*}\\]\n\nI’ll leave it to you to use the above numbers to show this holds"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#linear-model-estimators",
    "href": "lectures/03-Estimators-01/030-compile.html#linear-model-estimators",
    "title": "Estimators",
    "section": "Linear Model Estimators",
    "text": "Linear Model Estimators\nWe will spend the rest of the course exploring how to use Ordinary Least Squares (OLS) to fit a linear model like:\n\\[\n    y_{i} = \\beta_{0} + \\beta_{1}x_{i} + u_{i},\n\\]\nThat is, if we wanted to hypothesize that some random variable \\(Y\\) depends on another random variable \\(X\\) and that there is a linear relationship between then, \\(\\beta_{0}\\) and \\(\\beta_{1}\\) are the parameters which describe the nature of that relationship.\nGiven a sample of \\(X\\) and \\(Y\\), we will derive unbiased estimators for the intercept \\(\\beta_{0}\\) and slope \\(\\beta_{1}\\). Those estimators help us combine observations of \\(X\\) and \\(Y\\) to estimate underlying relationships between them."
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#the-linear-regression-model",
    "href": "lectures/03-Estimators-01/030-compile.html#the-linear-regression-model",
    "title": "Estimators",
    "section": "The Linear Regression Model",
    "text": "The Linear Regression Model\nWe can estimate the effect of \\(X\\) on \\(Y\\) by estimating the model:\n\\[\n    y_{i} = \\beta_{0} + \\beta_{1}x_{i} + u_{i},\n\\]\n\n\\(y_i\\) is the dependent variable\n\\(x_i\\) is the independent variable (continuous)\n\\(\\beta_0\\) is the intercept parameter. \\(E\\left[ {y_i | x_i=0} \\right] = \\beta_0\\)\n\\(\\beta_1\\) is the slope parameter, which under the correct causal setting represents marginal change in \\(x_i\\)’s effect on \\(y_i\\). \\(\\frac{\\partial y_i}{\\partial x_i} = \\beta_1\\)\n\\(u_i\\) is an Error Term including all other (omitted) factors affecting \\(y_i\\)."
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#the-error-term-u_i",
    "href": "lectures/03-Estimators-01/030-compile.html#the-error-term-u_i",
    "title": "Estimators",
    "section": "The Error Term \\(u_{i}\\)",
    "text": "The Error Term \\(u_{i}\\)\n\\(u_{i}\\) is quite special\nConsider the data generating process of variable \\(y_{i}\\),\n\n\\(u_{i}\\) captures all unobserved variables that explain variation in \\(y_{i}\\)\n\n\nSome error will exist in all models, no model is perfect.\n\nOur aim is to minimize error under a set of constraints\n\n\nError is the price we are willing to accept for a simplified model"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#the-error-term",
    "href": "lectures/03-Estimators-01/030-compile.html#the-error-term",
    "title": "Estimators",
    "section": "The Error Term",
    "text": "The Error Term\nFive items contribute to the existence of the disturbance term:\n\n1. Omission of independent variables\n\n\n\nOur description (model) of the relationship between \\(Y\\) and \\(X\\) is a simplification\nOther variables have been left out (omitted)"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#the-error-term-1",
    "href": "lectures/03-Estimators-01/030-compile.html#the-error-term-1",
    "title": "Estimators",
    "section": "The Error Term",
    "text": "The Error Term\nFive items contribute to the existence of the disturbance term:\n1. Omission of independent variables\n2. Aggregation of Variables\n\n\nMicroeconomic relationships are often summarized\nEx. Housing prices (\\(X\\)) are described by county-level median home value data"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#the-error-term-2",
    "href": "lectures/03-Estimators-01/030-compile.html#the-error-term-2",
    "title": "Estimators",
    "section": "The Error Term",
    "text": "The Error Term\nFive items contribute to the existence of the disturbance term:\n1. Omission of independent variables\n2. Aggregation of Variables\n3. Model misspecificiation\n\n\nModel structure is incorrectly specified\nEx. \\(Y\\) depends on the anticipated value of \\(X\\) in the previous period, not \\(X\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#the-error-term-3",
    "href": "lectures/03-Estimators-01/030-compile.html#the-error-term-3",
    "title": "Estimators",
    "section": "The Error Term",
    "text": "The Error Term\nFive items contribute to the existence of the disturbance term:\n1. Omission of independent variables\n2. Aggregation of Variables\n3. Model misspecificiation\n4. Functional misspecificiation\n\n\nThe functional relationship is specified incorrectly\nTrue relationship is nonlinear, not linear"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#the-error-term-4",
    "href": "lectures/03-Estimators-01/030-compile.html#the-error-term-4",
    "title": "Estimators",
    "section": "The Error Term",
    "text": "The Error Term\nFive items contribute to the existence of the disturbance term:\n1. Omission of independent variables\n2. Aggregation of Variables\n3. Model misspecificiation\n4. Functional misspecificiation\n5. Measurement error\n\n\nMeasurement of the variables in the data is just wrong\n\\(Y\\) or \\(X\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#the-error-term-5",
    "href": "lectures/03-Estimators-01/030-compile.html#the-error-term-5",
    "title": "Estimators",
    "section": "The Error Term",
    "text": "The Error Term\nFive items contribute to the existence of the disturbance term:\n1. Omission of independent variables\n2. Aggregation of Variables\n3. Model misspecificiation\n4. Functional misspecificiation\n5. Measurement error"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#running-a-regression-model",
    "href": "lectures/03-Estimators-01/030-compile.html#running-a-regression-model",
    "title": "Estimators",
    "section": "Running a Regression Model",
    "text": "Running a Regression Model\nUsing an estimator with data on \\(x_{i}\\) and \\(y_{i}\\), we can estimate a fitted regression line:\n\\[\n    \\hat{y}_{i} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1}x_{i}\n\\]\n\n\\(\\hat{y}_{i}\\) is the fitted value of \\(y_{i}\\)\n\\(\\hat{\\beta}_{0}\\) is the estimated intercept\n\\(\\hat{\\beta}_{1}\\) is the estimated slope\n\nThis procedure produces misses, known as residuals \\(y_{i} - \\hat{y_{i}}\\)\nLet’s look at an example of how this works"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#ex.-effect-of-police-on-crime",
    "href": "lectures/03-Estimators-01/030-compile.html#ex.-effect-of-police-on-crime",
    "title": "Estimators",
    "section": "Ex. Effect of police on crime",
    "text": "Ex. Effect of police on crime\n\nEmpirical question:\n\nDoes the number of on-campus police officers affect campus crime rates? If so, by how much?\n\n\n\n\nAlways plot your data first"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#ex.-effect-of-police-on-crime-1",
    "href": "lectures/03-Estimators-01/030-compile.html#ex.-effect-of-police-on-crime-1",
    "title": "Estimators",
    "section": "Ex. Effect of police on crime",
    "text": "Ex. Effect of police on crime\nThe scatter plot suggest that a weak positive relationship exists\n\nA sample correlation of 0.14 confirms this\n\n\n\nBut correlation does not imply causation\n\n\n\nLets estimate a statistical model"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#ex.-effect-of-police-on-crime-2",
    "href": "lectures/03-Estimators-01/030-compile.html#ex.-effect-of-police-on-crime-2",
    "title": "Estimators",
    "section": "Ex. Effect of police on crime",
    "text": "Ex. Effect of police on crime\nWe express the relationship between a dependent variable and an independent variable as linear:\n\\[\n{\\text{Crime}_i} = \\beta_0 + \\beta_1 \\text{Police}_i + u_i.\n\\]\n\n\\(\\beta_0\\) is the intercept or constant.\n\\(\\beta_1\\) is the slope coefficient.\n\\(u_i\\) is an error term or disturbance term."
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#ex.-effect-of-police-on-crime-3",
    "href": "lectures/03-Estimators-01/030-compile.html#ex.-effect-of-police-on-crime-3",
    "title": "Estimators",
    "section": "Ex. Effect of police on crime",
    "text": "Ex. Effect of police on crime\nThe intercept tells us the expected value of \\(\\text{Crime}_i\\) when \\(\\text{Police}_i = 0\\).\n\\[\n\\text{Crime}_i = {\\color{#BF616A} \\beta_{0}} + \\beta_1\\text{Police}_i + u_i\n\\]\nUsually not the focus of an analysis."
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#ex.-effect-of-police-on-crime-4",
    "href": "lectures/03-Estimators-01/030-compile.html#ex.-effect-of-police-on-crime-4",
    "title": "Estimators",
    "section": "Ex. Effect of police on crime",
    "text": "Ex. Effect of police on crime\nThe slope coefficient tells us the expected change in \\(\\text{Crime}_i\\) when \\(\\text{Police}_i\\) increases by one.\n\\[\n\\text{Crime}_i = \\beta_0 + {\\color{#BF616A} \\beta_1} \\text{Police}_i + u_i\n\\]\n“A one-unit increase in \\(\\text{Police}_i\\) is associated with a \\(\\color{#BF616A}{\\beta_1}\\)-unit increase in \\(\\text{Crime}_i\\).”\n\nInterpretation of this parameter is crucial\n\n\nUnder certain (strong) assumptions1, \\(\\color{#BF616A}{\\beta_1}\\) is the effect of \\(X_i\\) on \\(Y_i\\).\n\nOtherwise, it’s the association of \\(X_i\\) with \\(Y_i\\).\n\n\nAssumptions regarding the error term"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#ex.-effect-of-police-on-crime-5",
    "href": "lectures/03-Estimators-01/030-compile.html#ex.-effect-of-police-on-crime-5",
    "title": "Estimators",
    "section": "Ex. Effect of police on crime",
    "text": "Ex. Effect of police on crime\nThe error term reminds us that \\(\\text{Police}_i\\) does not perfectly explain \\(Y_i\\).\n\\[\n\\text{Crime}_i = \\beta_0 + \\beta_1\\text{Police}_i + {\\color{#BF616A} u_i}\n\\]\nRepresents all other factors that explain \\(\\text{Crime}_i\\).\n\nUseful mnemonic: pretend that \\(u\\) stands for “unobserved” or “unexplained.”"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#ex.-effect-of-police-on-crime-6",
    "href": "lectures/03-Estimators-01/030-compile.html#ex.-effect-of-police-on-crime-6",
    "title": "Estimators",
    "section": "Ex. Effect of police on crime",
    "text": "Ex. Effect of police on crime\nHow might we apply the simple linear regression model to our question about the effect of on-campus police on campus crime?\n\\[\n\\text{Crime}_i = \\beta_0 + \\beta_1\\text{Police}_i + u_i.\n\\]\n\n\\(\\beta_0\\) is the crime rate for colleges without police.\n\\(\\beta_1\\) is the increase in the crime rate for an additional police officer per 1000 students."
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#ex.-effect-of-police-on-crime-7",
    "href": "lectures/03-Estimators-01/030-compile.html#ex.-effect-of-police-on-crime-7",
    "title": "Estimators",
    "section": "Ex. Effect of police on crime",
    "text": "Ex. Effect of police on crime\nHow might we apply the simple linear regression model to our question?\n\\[\n\\text{Crime}_i = \\beta_0 + \\beta_1\\text{Police}_i + u_i\n\\]\n\\(\\beta_0\\) and \\(\\beta_1\\) are the unobserved population parameters we want\n\n\nWe estimate\n\n\\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) generate predictions of \\(\\text{Crime}_i\\) called \\(\\widehat{\\text{Crime}_i}\\).\nWe call the predictions of the dependent variable fitted values.\n\n\n\n\nTogether, these trace a line: \\(\\widehat{\\text{Crime}_i} = \\hat{\\beta_0} + \\hat{\\beta_1}\\text{Police}_i\\)."
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#section-1",
    "href": "lectures/03-Estimators-01/030-compile.html#section-1",
    "title": "Estimators",
    "section": "",
    "text": "So, the question becomes, how do I pick \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#section-2",
    "href": "lectures/03-Estimators-01/030-compile.html#section-2",
    "title": "Estimators",
    "section": "",
    "text": "Let’s take some guesses: \\(\\hat{\\beta_0} = 60\\) and \\(\\hat{\\beta}_{1} = -7\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#section-3",
    "href": "lectures/03-Estimators-01/030-compile.html#section-3",
    "title": "Estimators",
    "section": "",
    "text": "Let’s take some guesses: \\(\\hat{\\beta_0} = 30\\) and \\(\\hat{\\beta}_{1} = 0\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#section-4",
    "href": "lectures/03-Estimators-01/030-compile.html#section-4",
    "title": "Estimators",
    "section": "",
    "text": "Let’s take some guesses: \\(\\hat{\\beta_0} = 15.6\\) and \\(\\hat{\\beta}_{1} = 7.94\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#residuals",
    "href": "lectures/03-Estimators-01/030-compile.html#residuals",
    "title": "Estimators",
    "section": "Residuals",
    "text": "Residuals\nUsing \\(\\hat{\\beta}_{0}\\) and \\(\\hat{\\beta}_{1}\\) to make \\(\\hat{y}_{i}\\) generates misses.\n\n\n\n \\(\\hat{\\beta_0} = 60 \\;\\) Guess\n\n \\(\\hat{\\beta_0} = 30 \\;\\) Guess\n\n \\(\\hat{\\beta_0} = 15 \\;\\) Guess"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#residuals-sum-of-squares-rss",
    "href": "lectures/03-Estimators-01/030-compile.html#residuals-sum-of-squares-rss",
    "title": "Estimators",
    "section": "Residuals Sum of Squares (RSS)",
    "text": "Residuals Sum of Squares (RSS)\nWhat if we picked an estimator that minimizes the residuals?\nWhy do we not minimize:\n\\[\n    \\sum_{i=1}^{n} \\hat{u}_{i}^{2}\n\\]\nso that the estimator makes fewer big misses?\nThis estimator, the residual sum of squares (RSS), is convenient because squared numbers are never negative so we can minimze an absolut sum of the residuals\nRSS will give bigger penalties to bigger residuals"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#minimizing-rss",
    "href": "lectures/03-Estimators-01/030-compile.html#minimizing-rss",
    "title": "Estimators",
    "section": "Minimizing RSS",
    "text": "Minimizing RSS\nWe could test thousands of guesses of \\(\\beta_0\\) and \\(\\beta_1\\) an pick the pair the has the smallest RSS\nWe could painstakingly do that, and eventually figure out which one fits best.\nOr… We could just do a little math"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#ols",
    "href": "lectures/03-Estimators-01/030-compile.html#ols",
    "title": "Estimators",
    "section": "OLS",
    "text": "OLS\nThe OLS Estimator chooses the parameters \\(\\hat{\\beta}_{0}\\) and \\(\\hat{\\beta}_{1}\\) that minimize the Residual Sum of Squares (RSS)\n\\[\n    \\min_{\\hat{\\beta}_{0},\\hat{\\beta}_{1}} \\sum_{i=1}^{n} \\hat{u}_{i}^{2}\n\\]\nThis is why we call the estimator ordinary least squares\nRecall that residuals are given by \\(y_{i} - \\hat{y}_{i}\\) and that:\n\\[\n    \\hat{y}_{i} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1} x_{i}\n\\]\nThen\n\\[\n    u_{i} = y_{i} - \\hat{\\beta}_{0} + \\hat{\\beta}_{1} x_{i}\n\\]"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#ols-calculus",
    "href": "lectures/03-Estimators-01/030-compile.html#ols-calculus",
    "title": "Estimators",
    "section": "OLS & Calculus",
    "text": "OLS & Calculus\nWe can find our choices \\(\\hat{\\beta}_{0}\\) and \\(\\hat{\\beta}_{1}\\) to minimize our residuals using calculus\nA minimization problem is essentially the same as an optimization problem where we find the point at which our choices have a slope of zero\nTo begin, let’s properly write out our minimization problem:\n\\[\n    \\min_{\\hat{\\beta}_{0}, \\hat{\\beta}_{1}} \\;\\; \\sum_{i} u_{i}^{2}\n\\]\n\\[\n    \\min_{\\hat{\\beta}_{0}, \\hat{\\beta}_{1}} \\; \\sum_{i} (y_{i} - \\hat{y}_{i})^{2}\n\\]\n\\[\n    \\min_{\\hat{\\beta}_{0}, \\hat{\\beta}_{1}} \\; \\sum_{i} (y_{i} - \\hat{\\beta}_{0} - \\hat{\\beta}_{1}x_{i}) (y_{i} - \\hat{\\beta}_{0} - \\hat{\\beta}_{1}x_{i})\n\\]"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#ols-calculus-1",
    "href": "lectures/03-Estimators-01/030-compile.html#ols-calculus-1",
    "title": "Estimators",
    "section": "OLS & Calculus",
    "text": "OLS & Calculus\nThe calculus we’ll use is by finding the derivatives of the function with respect to \\(\\hat{\\beta}_{0}\\) and \\(\\hat{\\beta}_{1}\\).\nIt’s a lot of algebra but it is simple math, just a lot of it:\n\n\\[\\begin{align*}\n    \\min_{\\hat{\\beta}_{0}, \\hat{\\beta}_{1}} &\\;\n    \\sum_{i} y_{i}^{2} - \\hat{\\beta}_{0}y_{i} - \\hat{\\beta}_{1}x_{i}y_{i} - \\hat{\\beta}_{0}y_{i} + \\hat{\\beta}_{0}^{2} + \\hat{\\beta}_{0}\\hat{\\beta}_{1}x_{i} - \\hat{\\beta}_{1}x_{i}y_{i} + \\hat{\\beta}_{0}\\hat{\\beta}_{1}x_{i} + \\hat{\\beta}_{1}^{2}x_{i}^{2} \\\\\n    \\min_{\\hat{\\beta}_{0}, \\hat{\\beta}_{1}} &\\;\n    \\sum_{i} y_{i}^{2} - 2 \\hat{\\beta}_{0}y_{i} + \\hat{\\beta}_{0}^{2} - 2 \\hat{\\beta}_{1}x_{i}y_{i} + 2\\hat{\\beta}_{0}\\hat{\\beta}_{1}x_{i} + \\hat{\\beta}_{1}^{2}x_{i}^{2}\n\\end{align*}\\]\n\nThen, we take partial derivatives over our choices \\(\\hat{\\beta}_{0}\\) and \\(\\hat{\\beta}_{1}\\) to figure the best choices.\nThese are called First Order Conditions (FOCs)"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#ols-calculus-2",
    "href": "lectures/03-Estimators-01/030-compile.html#ols-calculus-2",
    "title": "Estimators",
    "section": "OLS & Calculus",
    "text": "OLS & Calculus\nTo find our choices, we find the partial derivative and set it equal to 0\nFor our intercept \\(\\hat{\\beta}_{0}\\):\n\n\\[\\begin{align*}\n    &\\dfrac{\\partial u_{i}}{\\partial \\hat{\\beta}_{0}} = 0 \\\\\n    \\sum_{i} -2y_{i} + &2\\hat{\\beta}_{0} + 2\\hat{\\beta}_{1}x_{i} = 0\n\\end{align*}\\]\n\nFor our slope \\(\\hat{\\beta}_{1}\\):\n\n\\[\\begin{align*}\n    &\\dfrac{\\partial u_{i}}{\\partial \\hat{\\beta}_{1}} = 0 \\\\\n    \\sum_{i} -2x_{i}y_{i} + &2\\hat{\\beta}_{0}x_{i} + 2\\hat{\\beta}_{1}x_{i}^{2} = 0\n\\end{align*}\\]"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#hatbeta_0-derivation",
    "href": "lectures/03-Estimators-01/030-compile.html#hatbeta_0-derivation",
    "title": "Estimators",
    "section": "\\(\\hat{\\beta}_{0}\\) Derivation",
    "text": "\\(\\hat{\\beta}_{0}\\) Derivation\n\\[\n    \\sum_{i} -2y_{i} + 2\\hat{\\beta}_{0} + 2\\hat{\\beta}_{1}x_{i} = 0\n\\]\nOur task is to find solve the above for \\(\\hat{\\beta}_{0}\\):"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#hatbeta_1-derivation",
    "href": "lectures/03-Estimators-01/030-compile.html#hatbeta_1-derivation",
    "title": "Estimators",
    "section": "\\(\\hat{\\beta}_{1}\\) Derivation",
    "text": "\\(\\hat{\\beta}_{1}\\) Derivation\n\\[\n    \\sum_{i} -2x_{i}y_{i} + 2\\hat{\\beta}_{0}x_{i} + 2\\hat{\\beta}_{1}x_{i}^{2} = 0\n\\]\nOur task is to find solve the above for \\(\\hat{\\beta}_{1}\\):"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#ols-formulas",
    "href": "lectures/03-Estimators-01/030-compile.html#ols-formulas",
    "title": "Estimators",
    "section": "OLS Formulas",
    "text": "OLS Formulas\n\nIntercept\n\\[\n    \\hat{\\beta}_{0} = \\bar{y} - \\hat{\\beta}_{1}\\bar{x}\n\\]\n\nSlope Coefficient\n\\[\n    \\hat{\\beta}_{1} =\n    \\dfrac{\n        \\sum_{i=1}^{n} (y_{i} - \\bar{y})(x_{i} - \\bar{x})\n    }{\n        \\sum_{i=1}^{n} (x_{i} - \\bar{x})^{2}\n    }\n\\]\nThese may look slightly different to my derivation. Part of your assignments is to bridge the gap."
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#interpretation",
    "href": "lectures/03-Estimators-01/030-compile.html#interpretation",
    "title": "Estimators",
    "section": "Interpretation",
    "text": "Interpretation\nThere are two stages of interpretation of a regression equation\n\nInterpret regression estimates into words\nDeciding whether this interpretation should be taken at face value\n\n\nBoth stages are important, but for now, we will focus on the first\nLet’s revisit our crime example"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#ex-effect-of-police-on-crime",
    "href": "lectures/03-Estimators-01/030-compile.html#ex-effect-of-police-on-crime",
    "title": "Estimators",
    "section": "Ex: Effect of Police on Crime",
    "text": "Ex: Effect of Police on Crime\nUsing the OLS formulas, we get \\(\\hat{\\beta}_{0} = 18.41\\) and \\(\\hat{\\beta}_{1} = 1.76\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#coefficient-interpretation-1",
    "href": "lectures/03-Estimators-01/030-compile.html#coefficient-interpretation-1",
    "title": "Estimators",
    "section": "Coefficient Interpretation",
    "text": "Coefficient Interpretation\nHow do I interpret \\(\\hat{\\beta}_{0} = 18.41\\) and \\(\\hat{\\beta}_{1} = 1.76\\)?\nThe general interpration of the intercept is the estimated value of \\(y_{i}\\) when \\(x_{i} = 0\\)\nAnd the general interpretation of the slope parameter is the estimated change \\(y_{i}\\) for the marginal increase \\(x_{i}\\)\n\nFirst, it is important to understand the units:\n\n\\(\\widehat{\\text{Crime}}_{i}\\) is measured as a crime rate, the number of crimes per 1,000 students on campus\n\\(\\text{Police}_{i}\\) is also measured as a rate, the number of police officers per 1,000 students on campus"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#coefficient-interpretation-2",
    "href": "lectures/03-Estimators-01/030-compile.html#coefficient-interpretation-2",
    "title": "Estimators",
    "section": "Coefficient Interpretation",
    "text": "Coefficient Interpretation\nUsing OLS gives us the fitted line\n\\[\n\\widehat{\\text{Crime}_i} = \\hat{\\beta}_1 + \\hat{\\beta}_2\\text{Police}_i.\n\\]\nWhat does \\(\\hat{\\beta_0}\\) = \\(18.41\\) tell us? Without any police on campus, the crime rate is \\(18.41\\) per 1,000 people on campus\n\nWhat does \\(\\hat{\\beta_1}\\) = \\(1.76\\) tell us? For each additional police officer per 1,000, there is an associated increase in the crime rate by \\(1.76\\) crimes per 1,000 people on campus.\n\n\nDoes this mean that police cause crime? Probably not.\nThis is where deciding if the interpretation should be taken at face value. It now becomes your job to bring reason to the values."
  },
  {
    "objectID": "lectures/01-Random-Variables/016-continuous-rvs.html",
    "href": "lectures/01-Random-Variables/016-continuous-rvs.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "When the variable can take on an infinite number of possible values, the probability it takes on any given value must be zero.\nThe variable takes so many values that we cannot count all possibilities, so the probability of any one particular value is zero.\nWe can use probability density functions (PDFs) to help describe continuous RVs of which there are many but we will give emphasis to two:\n\nUniform Distribution\nNormal Distribution"
  },
  {
    "objectID": "lectures/01-Random-Variables/016-continuous-rvs.html#probabilities-of-continuous-rvs",
    "href": "lectures/01-Random-Variables/016-continuous-rvs.html#probabilities-of-continuous-rvs",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "When the variable can take on an infinite number of possible values, the probability it takes on any given value must be zero.\nThe variable takes so many values that we cannot count all possibilities, so the probability of any one particular value is zero.\nWe can use probability density functions (PDFs) to help describe continuous RVs of which there are many but we will give emphasis to two:\n\nUniform Distribution\nNormal Distribution"
  },
  {
    "objectID": "lectures/01-Random-Variables/016-continuous-rvs.html#distributions",
    "href": "lectures/01-Random-Variables/016-continuous-rvs.html#distributions",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Distributions",
    "text": "Distributions\nA distribution is a function that represents all outcomes of a random variable and the corresponding probabilities. It is:\n\nA summary that describes the spread of data points in a set\nEssential for making inferences and assumptions from data\n\nKey Takeaway: The shape of a distribution provides valuable information of the data"
  },
  {
    "objectID": "lectures/01-Random-Variables/016-continuous-rvs.html#uniform-distribution",
    "href": "lectures/01-Random-Variables/016-continuous-rvs.html#uniform-distribution",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Uniform Distribution",
    "text": "Uniform Distribution\nThe probability density function of a variable uniformly distributed between 0 and 2 is\n\\[\\begin{align*}\n    f(x) =\n        \\begin{cases}\n        \\dfrac{1}{2} & \\text{if } 0 \\leq x \\leq 2 \\\\\n        0   & \\text{otherwise }\n        \\end{cases}\n\\end{align*}\\]"
  },
  {
    "objectID": "lectures/01-Random-Variables/016-continuous-rvs.html#uniform-distribution-1",
    "href": "lectures/01-Random-Variables/016-continuous-rvs.html#uniform-distribution-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Uniform Distribution",
    "text": "Uniform Distribution\nBy definition, the area under \\(f(x)\\) is equal to 1.\nThe shaded area illustrates the probability of the event \\(1 \\leq X \\leq 1.5\\).\n\\[\n    P(1 \\leq X \\leq 1.5) = (1.5 - 1) \\times 0.5 = 0.25\n\\]\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead."
  },
  {
    "objectID": "lectures/01-Random-Variables/016-continuous-rvs.html#normal-distribution",
    "href": "lectures/01-Random-Variables/016-continuous-rvs.html#normal-distribution",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Normal Distribution",
    "text": "Normal Distribution\nThis is commonly called a “bell curve”. It is:\n\nSymmetric: Mean and median occur at the same point (i.e. no skew)\nLow-probability events are in the tails\nHigh-probability events are near the center\n\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union"
  },
  {
    "objectID": "lectures/01-Random-Variables/016-continuous-rvs.html#normal-distribution-1",
    "href": "lectures/01-Random-Variables/016-continuous-rvs.html#normal-distribution-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Normal Distribution",
    "text": "Normal Distribution\nThe shaded area illustrates the probability of the event \\(-2 \\leq X \\leq 2\\) occurring\n\nTo “find the area under the curve” we use integral calculus (or, in practice ).\n\n\\[\n    P(-2 \\leq X \\leq 2) \\approx 0.95\n\\]"
  },
  {
    "objectID": "lectures/01-Random-Variables/016-continuous-rvs.html#normal-distribution-2",
    "href": "lectures/01-Random-Variables/016-continuous-rvs.html#normal-distribution-2",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Normal Distribution",
    "text": "Normal Distribution\nContinuous distribution where \\(x_{i}\\) takes the value of any real number \\((\\mathbb{R})\\)\n\nThe domain spans the entire real line\nCentered on the distribution mean \\(\\mu\\)\n\nA couple of important rules to recall:\n\nThe probability that the random variable takes a value \\(x_{i}\\) is 0 for any \\(x_{i} \\in \\mathbb{R}\\)\nThe probability that the random variable falls between \\([x_{i},x_{j}]\\) range, where \\(x_{i} \\neq x_{j}\\), is the area under \\(p(x)\\) between those two values.\n\nThe area highlighted in the previous graph represents \\(p(x) = 0.95\\). The values \\({-1.96,1.95}\\) represent the 95% confidence interval for \\(\\mu\\)"
  },
  {
    "objectID": "lectures/01-Random-Variables/016-continuous-rvs.html#primary-differences-in-expected-values-by-rv-type",
    "href": "lectures/01-Random-Variables/016-continuous-rvs.html#primary-differences-in-expected-values-by-rv-type",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Primary Differences in Expected Values by RV Type",
    "text": "Primary Differences in Expected Values by RV Type\nTo find the expected value or variance of a continuous random variable instead of a discrete random variable, we just swap integrals for sums and the PDF \\(f(X)\\) for \\(p_{i}\\):\n\n\n\n\n\n\n\n\n\n\\(E[X]\\)\n\\(Var(X) = E[(X - \\mu_{X})^{2}]\\)\n\n\n\n\nDiscrete\n\\(\\sum_{i=1}^{n} x_{i}p_{i}\\)\n\\(\\sum_{i=1}^{n} (x_{i} - \\mu_{X})^{2} p_{i}\\)\n\n\nContinuous\n\\(\\int X f(X) dX\\)\n\\(\\int (X - \\mu_{x})^{2} f(X) dX\\)"
  },
  {
    "objectID": "lectures/01-Random-Variables/014-covariance.html",
    "href": "lectures/01-Random-Variables/014-covariance.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "The covariance of two random variables \\((\\sigma_{XY})\\) is a measure of the linear association between those variables. For example, since people who are taller are generally heavier, we would say that the random variables height and weight have a positive covariance. On the other hand, if large values for one random variable tend to correspond to small values in the other, we would say the two variables have a negative covariance. Two variables are independent have a covariance of 0.\nThe formula is:\n\\[\n    Cov(X,Y) = \\sigma_{XY} = E[(X - \\mu_{X})(Y - \\mu_{Y})]\n\\]\nNotice that the covariance of a random variable \\(X\\) with itself is the variance of \\(X\\)"
  },
  {
    "objectID": "lectures/01-Random-Variables/014-covariance.html#definition",
    "href": "lectures/01-Random-Variables/014-covariance.html#definition",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "The covariance of two random variables \\((\\sigma_{XY})\\) is a measure of the linear association between those variables. For example, since people who are taller are generally heavier, we would say that the random variables height and weight have a positive covariance. On the other hand, if large values for one random variable tend to correspond to small values in the other, we would say the two variables have a negative covariance. Two variables are independent have a covariance of 0.\nThe formula is:\n\\[\n    Cov(X,Y) = \\sigma_{XY} = E[(X - \\mu_{X})(Y - \\mu_{Y})]\n\\]\nNotice that the covariance of a random variable \\(X\\) with itself is the variance of \\(X\\)"
  },
  {
    "objectID": "lectures/01-Random-Variables/014-covariance.html#rules",
    "href": "lectures/01-Random-Variables/014-covariance.html#rules",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Rules",
    "text": "Rules\nSome important rules about the way variance works. Let \\(X\\),\\(Y\\), and \\(Z\\) be random variables and let \\(b\\) be a constant.\n\nThe covariance of a random variable with a constant is 0 \\[\nCov(X,b) = 0\n\\]\nThe covariance of a random variable with itself is its variance: \\[\nCov(X,X) = Var(X)\n\\]\nConstants can come outside of the covariance: \\[\nCov(X,bY) = bCov(X,Y)\n\\]\nIf \\(Z\\) is a third random variable, we write: \\[\nCov(X,Y + Z) = Cov(X,Y) + Cov(X,Z)\n\\]"
  },
  {
    "objectID": "lectures/01-Random-Variables/012-expected-values.html",
    "href": "lectures/01-Random-Variables/012-expected-values.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "The expected value of a random variable is its long-term average.\nWe will use the greek letter \\(\\mu\\) (“mew”) to refer to expected values. That is, we will say that the expected value of \\(X\\) is \\(\\mu_{X}\\), or equivalently, \\(E[X] = \\mu_{X}\\).\nIf the variable is discrete, you can calculate its expectation by taking the sum of all possible values of the random variable, each multiplied by their corresponding probabilities.\nWe write this as:\n\\[\n    E[X] = \\sum_{i} x_{i}p_{i}\n\\]\nWhere \\(x_{i}\\) is a potential outcome for \\(X\\) and \\(p_{i}\\) is the probability that outcome occurs"
  },
  {
    "objectID": "lectures/01-Random-Variables/012-expected-values.html#expected-values-of-discrete-random-variables",
    "href": "lectures/01-Random-Variables/012-expected-values.html#expected-values-of-discrete-random-variables",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "The expected value of a random variable is its long-term average.\nWe will use the greek letter \\(\\mu\\) (“mew”) to refer to expected values. That is, we will say that the expected value of \\(X\\) is \\(\\mu_{X}\\), or equivalently, \\(E[X] = \\mu_{X}\\).\nIf the variable is discrete, you can calculate its expectation by taking the sum of all possible values of the random variable, each multiplied by their corresponding probabilities.\nWe write this as:\n\\[\n    E[X] = \\sum_{i} x_{i}p_{i}\n\\]\nWhere \\(x_{i}\\) is a potential outcome for \\(X\\) and \\(p_{i}\\) is the probability that outcome occurs"
  },
  {
    "objectID": "lectures/01-Random-Variables/012-expected-values.html#expected-value-rules",
    "href": "lectures/01-Random-Variables/012-expected-values.html#expected-value-rules",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Expected Value Rules",
    "text": "Expected Value Rules\n\nHere are some very important math rules to know about the way expected values work. Let \\(X\\),\\(Y\\), and \\(Z\\) be random variables and let \\(b\\) be a constant.\n\n\nThe expectation of the sum of several RVs is the sum of their expectation: \\[\nE[X + Y + Z] = E[X] + E[Y] + E[Z]\n\\]\nConstants can pass outside of an expectation: \\[\nE[bX] = bE[X]\n\\]\nThe expected value of a constant is that constant: \\[\nE[b] = b\n\\]"
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#preview",
    "href": "lectures/01-Random-Variables/010-compile.html#preview",
    "title": "Random Variables",
    "section": "Preview",
    "text": "Preview\nIn this chapter we will:\n\nLearn what discrete and continuous random variables are\nHow to use the probability distribution of a discrete random variable to obtain the expected value and variance of the random variable\nHow to use the probability density function (PDF) of a continuous random variable to obtain the expected value and variance of the random variable\nHow to obtain the covariance and correlation between two random variables"
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#notation",
    "href": "lectures/01-Random-Variables/010-compile.html#notation",
    "title": "Random Variables",
    "section": "Notation",
    "text": "Notation\nSome important notation we need to introduce:\n\n\n\n\nSymbol\nMeaning\n\n\n\n\n\\(X\\)\nRandom Variable (RV)\n\n\n\\(x_i\\)\nA potential outcome for the RV \\(X\\)\n\n\n\\(p_i\\)\nThe probability a certain outcome will occur (discrete RVs)\n\n\n\\(\\mu_X\\)\nThe expected value of \\(X\\), also known as \\(E[X]\\)\n\n\n\\(\\sigma_X^2\\)\nThe variance of \\(X\\)\n\n\n\\(\\sigma_X\\)\nThe standard deviation of \\(X\\)\n\n\n\\(\\sigma_{XY}\\)\nThe covariance of \\(X\\) and \\(Y\\)\n\n\n\\(\\rho_{XY}\\)\nThe correlation between \\(X\\) and \\(Y\\)"
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#random-variables-1",
    "href": "lectures/01-Random-Variables/010-compile.html#random-variables-1",
    "title": "Random Variables",
    "section": "Random Variables",
    "text": "Random Variables\nA Random Variable is any variable whose value cannot be predicted exactly. For example:\n\nThe message you get in a fortune cookie\nThe amount of time spent searching for your keys\nThe number of likes you get on a social media post\nThe number of customers that enter a store in a day\n\nAll of these are random variables.\nSome random variables are discrete and some are continuous"
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#discrete-and-continuous-rv",
    "href": "lectures/01-Random-Variables/010-compile.html#discrete-and-continuous-rv",
    "title": "Random Variables",
    "section": "Discrete and Continuous RV",
    "text": "Discrete and Continuous RV\nWhat’s the difference?\n\n\nDiscrete\n\nCounted\nTake on a small number of possible values\nEx: Number of M&Ms in your bag\n\n\nContinuous\n\nMeasured\nCan take on an infinite number of possible values\nEx: How heavy your bag is\n\n\nVariables can also be categorical instead of numeric. They may represent qualitative data that can be divided into categories or groups. For now, we will lump them in with discrete variables"
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#discrete-probability-distributions",
    "href": "lectures/01-Random-Variables/010-compile.html#discrete-probability-distributions",
    "title": "Random Variables",
    "section": "Discrete Probability Distributions",
    "text": "Discrete Probability Distributions\nConsider the event of a dice roll. This action produces a discrete random variable.\nIt could take on values 1 to 6 and, if it is a fair die, it takes on each of those values with equali probability \\(1/6\\).\nOur notation will be:\n\n\\(X\\) is the random variable, \\(x_{i}\\) is a potential outcome for \\(X\\), and each potential outcome \\(x_{i}\\) happens with probability \\(p_{i}\\)\n\n\n\n\n\\(x_{i}\\)\n1\n2\n3\n4\n5\n6\n\n\n\n\n\n\\(p_{i}\\)\n1/6\n1/6\n1/6\n1/6\n1/6\n1/6"
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#discrete-probability-distributions-1",
    "href": "lectures/01-Random-Variables/010-compile.html#discrete-probability-distributions-1",
    "title": "Random Variables",
    "section": "Discrete Probability Distributions",
    "text": "Discrete Probability Distributions\nConsider another random variable \\(X\\) to be the sum of two dice rolls. In the table below, the first row represents the potential outcomes for the first roll and the first column represents the potential outcomes for the second roll. The values inside the table represent the potential outcomes for \\(X\\) (the sum)\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n2\n3\n4\n5\n6\n7\n8\n\n\n3\n4\n5\n6\n7\n8\n9\n\n\n4\n5\n6\n7\n8\n9\n10\n\n\n5\n6\n7\n8\n9\n10\n11\n\n\n6\n7\n8\n9\n10\n11\n12\n\n\n\n\nEach of the cells occur with equal probability. So that X = 2 has probability 1/36. X = 3 has probability 2/36, as it can occur in two ways."
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#expected-values-of-discrete-random-variables",
    "href": "lectures/01-Random-Variables/010-compile.html#expected-values-of-discrete-random-variables",
    "title": "Random Variables",
    "section": "Expected Values of Discrete Random Variables",
    "text": "Expected Values of Discrete Random Variables\nThe expected value of a random variable is its long-term average.\nWe will use the greek letter \\(\\mu\\) (“mew”) to refer to expected values. That is, we will say that the expected value of \\(X\\) is \\(\\mu_{X}\\), or equivalently, \\(E[X] = \\mu_{X}\\).\nIf the variable is discrete, you can calculate its expectation by taking the sum of all possible values of the random variable, each multiplied by their corresponding probabilities.\nWe write this as:\n\\[\n    E[X] = \\sum_{i} x_{i}p_{i}\n\\]\nWhere \\(x_{i}\\) is a potential outcome for \\(X\\) and \\(p_{i}\\) is the probability that outcome occurs"
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#expected-value-rules",
    "href": "lectures/01-Random-Variables/010-compile.html#expected-value-rules",
    "title": "Random Variables",
    "section": "Expected Value Rules",
    "text": "Expected Value Rules\n\nHere are some very important math rules to know about the way expected values work. Let \\(X\\),\\(Y\\), and \\(Z\\) be random variables and let \\(b\\) be a constant.\n\n\nThe expectation of the sum of several RVs is the sum of their expectation: \\[\nE[X + Y + Z] = E[X] + E[Y] + E[Z]\n\\]\nConstants can pass outside of an expectation: \\[\nE[bX] = bE[X]\n\\]\nThe expected value of a constant is that constant: \\[\nE[b] = b\n\\]"
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#definition",
    "href": "lectures/01-Random-Variables/010-compile.html#definition",
    "title": "Random Variables",
    "section": "Definition",
    "text": "Definition\nThe variance of a random variable measures its dispersion. It asks “on average, how far is the variable from its average”? Differences are squared to get rid of the negative sign and punish large deviances a little more. We will use the greek letter \\(\\sigma\\) (“sigma”) for variance \\((\\sigma^{2})\\) and standard deviation \\((\\sigma)\\)\nThe formula is:\n\\[\\begin{align}\n    Var(X) = \\sigma_{X}^{2}\n    &= E[(X - \\mu_{X})^{2}] \\\\\n    &= (x_{1} - \\mu_{X})^{2}p_{1} + (x_{2} - \\mu_{X})^{2}p_{2} + \\cdots + (x_{n} - \\mu_{X})^{2}p_{n} \\\\\n    &= \\sum_{i = 1}^{n} (x_{i} - \\mu_{X})^{2}p_{i}\n\\end{align}\\]\nNote that because of the square and the fact that probabilities \\(p_{i}\\) are never negative, the variance of a RV can never be a negative number"
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#rules",
    "href": "lectures/01-Random-Variables/010-compile.html#rules",
    "title": "Random Variables",
    "section": "Rules",
    "text": "Rules\nSome important rules about the way variance works. Let \\(X\\) and \\(Y\\) be random variables and let \\(b\\) be a constant.\n\nThe variance of the sum of two RVs is the sum of their variances plus two times their covariance: \\[\nVar(X + Y) = Var(X) + Var(Y) + 2Cov(X,Y)\n\\]\nConstants can pass outside of a variance if you square them: \\[\nVar(bX) = b^{2}Var(X)\n\\]\nThe variance of a constant is 0: \\[\nVar(b) = 0\n\\]\nThe variance of a RV plus a constant is the variance of that random variable: \\[\nVar(X + b) = Var(X)\n\\]"
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#definition-1",
    "href": "lectures/01-Random-Variables/010-compile.html#definition-1",
    "title": "Random Variables",
    "section": "Definition",
    "text": "Definition\nThe covariance of two random variables \\((\\sigma_{XY})\\) is a measure of the linear association between those variables. For example, since people who are taller are generally heavier, we would say that the random variables height and weight have a positive covariance. On the other hand, if large values for one random variable tend to correspond to small values in the other, we would say the two variables have a negative covariance. Two variables are independent have a covariance of 0.\nThe formula is:\n\\[\n    Cov(X,Y) = \\sigma_{XY} = E[(X - \\mu_{X})(Y - \\mu_{Y})]\n\\]\nNotice that the covariance of a random variable \\(X\\) with itself is the variance of \\(X\\)"
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#rules-1",
    "href": "lectures/01-Random-Variables/010-compile.html#rules-1",
    "title": "Random Variables",
    "section": "Rules",
    "text": "Rules\nSome important rules about the way variance works. Let \\(X\\),\\(Y\\), and \\(Z\\) be random variables and let \\(b\\) be a constant.\n\nThe covariance of a random variable with a constant is 0 \\[\nCov(X,b) = 0\n\\]\nThe covariance of a random variable with itself is its variance: \\[\nCov(X,X) = Var(X)\n\\]\nConstants can come outside of the covariance: \\[\nCov(X,bY) = bCov(X,Y)\n\\]\nIf \\(Z\\) is a third random variable, we write: \\[\nCov(X,Y + Z) = Cov(X,Y) + Cov(X,Z)\n\\]"
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#definition-2",
    "href": "lectures/01-Random-Variables/010-compile.html#definition-2",
    "title": "Random Variables",
    "section": "Definition",
    "text": "Definition\nAn issue with covariance is that the covariance between two random variables depends on the units those variables are measured in. That’s where correlation comes in:\nCorrelation is another measure of linear association that has the benefit of being dimensionless because the units in the numerator cancel with the units in the denominator.\nIt is also the case that the correlation between two variables is always between -1 and 1. Where correlation = 1, the two variables have a perfect positive linear relationsihp, and when correlation = -1, the two variables have a perfect negative linear relationship.\nWe will use the greek letter \\(\\rho\\) (“rho”) to refer to the correlation between two RVs. The formula is:\n\\[\\begin{align*}\n    \\rho_{XY} =\n    \\dfrac{\n        \\sigma_{XY}\n    }{\n        \\sqrt{\\sigma_{X}^{2}\\sigma_{Y}^{2}}\n    }\n\\end{align*}\\]"
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#probabilities-of-continuous-rvs",
    "href": "lectures/01-Random-Variables/010-compile.html#probabilities-of-continuous-rvs",
    "title": "Random Variables",
    "section": "Probabilities of Continuous RVs",
    "text": "Probabilities of Continuous RVs\nWhen the variable can take on an infinite number of possible values, the probability it takes on any given value must be zero.\nThe variable takes so many values that we cannot count all possibilities, so the probability of any one particular value is zero.\nWe can use probability density functions (PDFs) to help describe continuous RVs of which there are many but we will give emphasis to two:\n\nUniform Distribution\nNormal Distribution"
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#distributions",
    "href": "lectures/01-Random-Variables/010-compile.html#distributions",
    "title": "Random Variables",
    "section": "Distributions",
    "text": "Distributions\nA distribution is a function that represents all outcomes of a random variable and the corresponding probabilities. It is:\n\nA summary that describes the spread of data points in a set\nEssential for making inferences and assumptions from data\n\nKey Takeaway: The shape of a distribution provides valuable information of the data"
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#uniform-distribution",
    "href": "lectures/01-Random-Variables/010-compile.html#uniform-distribution",
    "title": "Random Variables",
    "section": "Uniform Distribution",
    "text": "Uniform Distribution\nThe probability density function of a variable uniformly distributed between 0 and 2 is\n\\[\\begin{align*}\n    f(x) =\n        \\begin{cases}\n        \\dfrac{1}{2} & \\text{if } 0 \\leq x \\leq 2 \\\\\n        0   & \\text{otherwise }\n        \\end{cases}\n\\end{align*}\\]"
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#uniform-distribution-1",
    "href": "lectures/01-Random-Variables/010-compile.html#uniform-distribution-1",
    "title": "Random Variables",
    "section": "Uniform Distribution",
    "text": "Uniform Distribution\nBy definition, the area under \\(f(x)\\) is equal to 1.\nThe shaded area illustrates the probability of the event \\(1 \\leq X \\leq 1.5\\).\n\\[\n    P(1 \\leq X \\leq 1.5) = (1.5 - 1) \\times 0.5 = 0.25\n\\]"
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#normal-distribution",
    "href": "lectures/01-Random-Variables/010-compile.html#normal-distribution",
    "title": "Random Variables",
    "section": "Normal Distribution",
    "text": "Normal Distribution\nThis is commonly called a “bell curve”. It is:\n\nSymmetric: Mean and median occur at the same point (i.e. no skew)\nLow-probability events are in the tails\nHigh-probability events are near the center"
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#normal-distribution-1",
    "href": "lectures/01-Random-Variables/010-compile.html#normal-distribution-1",
    "title": "Random Variables",
    "section": "Normal Distribution",
    "text": "Normal Distribution\nThe shaded area illustrates the probability of the event \\(-2 \\leq X \\leq 2\\) occurring\n\nTo “find the area under the curve” we use integral calculus (or, in practice ).\n\n\\[\n    P(-2 \\leq X \\leq 2) \\approx 0.95\n\\]"
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#normal-distribution-2",
    "href": "lectures/01-Random-Variables/010-compile.html#normal-distribution-2",
    "title": "Random Variables",
    "section": "Normal Distribution",
    "text": "Normal Distribution\nContinuous distribution where \\(x_{i}\\) takes the value of any real number \\((\\mathbb{R})\\)\n\nThe domain spans the entire real line\nCentered on the distribution mean \\(\\mu\\)\n\nA couple of important rules to recall:\n\nThe probability that the random variable takes a value \\(x_{i}\\) is 0 for any \\(x_{i} \\in \\mathbb{R}\\)\nThe probability that the random variable falls between \\([x_{i},x_{j}]\\) range, where \\(x_{i} \\neq x_{j}\\), is the area under \\(p(x)\\) between those two values.\n\nThe area highlighted in the previous graph represents \\(p(x) = 0.95\\). The values \\({-1.96,1.95}\\) represent the 95% confidence interval for \\(\\mu\\)"
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#primary-differences-in-expected-values-by-rv-type",
    "href": "lectures/01-Random-Variables/010-compile.html#primary-differences-in-expected-values-by-rv-type",
    "title": "Random Variables",
    "section": "Primary Differences in Expected Values by RV Type",
    "text": "Primary Differences in Expected Values by RV Type\nTo find the expected value or variance of a continuous random variable instead of a discrete random variable, we just swap integrals for sums and the PDF \\(f(X)\\) for \\(p_{i}\\):\n\n\n\n\n\n\n\n\n\n\\(E[X]\\)\n\\(Var(X) = E[(X - \\mu_{X})^{2}]\\)\n\n\n\n\nDiscrete\n\\(\\sum_{i=1}^{n} x_{i}p_{i}\\)\n\\(\\sum_{i=1}^{n} (x_{i} - \\mu_{X})^{2} p_{i}\\)\n\n\nContinuous\n\\(\\int X f(X) dX\\)\n\\(\\int (X - \\mu_{x})^{2} f(X) dX\\)"
  },
  {
    "objectID": "documents/readings/index.html",
    "href": "documents/readings/index.html",
    "title": "Readings",
    "section": "",
    "text": "View PDF \n  \n     Download"
  },
  {
    "objectID": "documents/readings/index.html#math-rules",
    "href": "documents/readings/index.html#math-rules",
    "title": "Readings",
    "section": "",
    "text": "View PDF \n  \n     Download"
  },
  {
    "objectID": "documents/readings/index.html#review",
    "href": "documents/readings/index.html#review",
    "title": "Readings",
    "section": "Review",
    "text": "Review\n\n  View PDF \n  \n     Download"
  },
  {
    "objectID": "documents/readings/index.html#ch.01---simple-regression-analysis",
    "href": "documents/readings/index.html#ch.01---simple-regression-analysis",
    "title": "Readings",
    "section": "Ch.01 - Simple Regression Analysis",
    "text": "Ch.01 - Simple Regression Analysis\n\n  View PDF \n  \n     Download"
  },
  {
    "objectID": "documents/readings/index.html#ch.02---coefficient-hypothesis-testing",
    "href": "documents/readings/index.html#ch.02---coefficient-hypothesis-testing",
    "title": "Readings",
    "section": "Ch.02 - Coefficient & Hypothesis Testing",
    "text": "Ch.02 - Coefficient & Hypothesis Testing\n\n  View PDF \n  \n     Download"
  },
  {
    "objectID": "documents/readings/index.html#ch.03---multiple-regression-analysis",
    "href": "documents/readings/index.html#ch.03---multiple-regression-analysis",
    "title": "Readings",
    "section": "Ch.03 - Multiple Regression Analysis",
    "text": "Ch.03 - Multiple Regression Analysis\n\n  View PDF \n  \n     Download"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "documents/problem-sets/index.html",
    "href": "documents/problem-sets/index.html",
    "title": "Assignments",
    "section": "",
    "text": "View PDF \n  \n     Download \n  \n  \n    \n  \n\n\n\n\n\n  View PDF \n  \n     Download"
  },
  {
    "objectID": "documents/problem-sets/index.html#problem-sets",
    "href": "documents/problem-sets/index.html#problem-sets",
    "title": "Assignments",
    "section": "",
    "text": "View PDF \n  \n     Download \n  \n  \n    \n  \n\n\n\n\n\n  View PDF \n  \n     Download"
  },
  {
    "objectID": "documents/problem-sets/index.html#quiz",
    "href": "documents/problem-sets/index.html#quiz",
    "title": "Assignments",
    "section": "Quiz",
    "text": "Quiz\n\nQuiz 01 on Canvas\n\n\nQuiz 02 on Canvas\n\n\nQuiz 03\n\n\nQuiz 04"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "EC320 - Introduction to Econometrics",
    "section": "",
    "text": "Hello! Welcome to EC 320. I am Jose Rojas-Fallas, a 4th Year PhD student in the Econ department. And welcome to the course website! I have begun to use a website as a main method of course delivery as it is more intuitive (and easier to manage) than Canvas. For this course, Canvas is a glorified assignment submission site and gradebook. You will find everything you need on this site by navigating the tabs above.\nThat said: as you can clearly see, it is still largely incomplete. I aim to be as transparent as possible with students and their course so allow me to quickly explain why and how I intend for this course to run:\n\nThis is my first time teaching this course so I am still putting all the content together in a way that I like to teach it.\nI will have a complete “Week 01” content by Tuesday evening. I am currently traveling for a conference so my time has unfortunately been drawn away. But you will not be negatively impacted.\nWe will be doing alternate weeks of “theory/math” and “R”. The goal is to understand the reason why things work and then learn how to make them work.\nA lot, and I mean a lot, of econometrics is about learning by doing. What I mean by that is that you should always attempt the math or to code things even if you do not have a clear picture of how to do things. R may become frustrating but it is a worthwhile skill to pick up. To this day I still have to refer to older code I’ve used for simple things such as making a graph.\nLLMs, like ChatGPT, are a great tool that can assist you but it still requires you to understand the output and be able to properly implement it. Do not over rely on it."
  },
  {
    "objectID": "index.html#hello",
    "href": "index.html#hello",
    "title": "EC320 - Introduction to Econometrics",
    "section": "",
    "text": "Hello! Welcome to EC 320. I am Jose Rojas-Fallas, a 4th Year PhD student in the Econ department. And welcome to the course website! I have begun to use a website as a main method of course delivery as it is more intuitive (and easier to manage) than Canvas. For this course, Canvas is a glorified assignment submission site and gradebook. You will find everything you need on this site by navigating the tabs above.\nThat said: as you can clearly see, it is still largely incomplete. I aim to be as transparent as possible with students and their course so allow me to quickly explain why and how I intend for this course to run:\n\nThis is my first time teaching this course so I am still putting all the content together in a way that I like to teach it.\nI will have a complete “Week 01” content by Tuesday evening. I am currently traveling for a conference so my time has unfortunately been drawn away. But you will not be negatively impacted.\nWe will be doing alternate weeks of “theory/math” and “R”. The goal is to understand the reason why things work and then learn how to make them work.\nA lot, and I mean a lot, of econometrics is about learning by doing. What I mean by that is that you should always attempt the math or to code things even if you do not have a clear picture of how to do things. R may become frustrating but it is a worthwhile skill to pick up. To this day I still have to refer to older code I’ve used for simple things such as making a graph.\nLLMs, like ChatGPT, are a great tool that can assist you but it still requires you to understand the output and be able to properly implement it. Do not over rely on it."
  },
  {
    "objectID": "index.html#syllabus",
    "href": "index.html#syllabus",
    "title": "EC320 - Introduction to Econometrics",
    "section": "Syllabus",
    "text": "Syllabus\n\n  Summer 2025 Syllabus PDF\n  \n     Download"
  },
  {
    "objectID": "index.html#summer-2025-class-details",
    "href": "index.html#summer-2025-class-details",
    "title": "EC320 - Introduction to Econometrics",
    "section": "Summer 2025 Class Details",
    "text": "Summer 2025 Class Details\n\n\nInstructor: Jose Rojas-Fallas\nOffice Hours: Tues/Thurs 02:00 to 04:00 pm\n\nEmail: jrojas2@uoregon.edu\nZoom Room\n\n\n\nCourse Grade Breakdown\nFor more details, read the Syllabus above\n\n\n\nAssignment\nGrade Weight\n\n\n\n\nProblem Sets (x4)\n20%\n\n\nQuizzes (x4)\n20%\n\n\nMidterm Exam\n30%\n\n\nFinal Exam\n30%\n\n\n\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "lectures/01-Random-Variables/011-probs.html",
    "href": "lectures/01-Random-Variables/011-probs.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "A Random Variable is any variable whose value cannot be predicted exactly. For example:\n\nThe message you get in a fortune cookie\nThe amount of time spent searching for your keys\nThe number of likes you get on a social media post\nThe number of customers that enter a store in a day\n\nAll of these are random variables.\nSome random variables are discrete and some are continuous"
  },
  {
    "objectID": "lectures/01-Random-Variables/011-probs.html#random-variables",
    "href": "lectures/01-Random-Variables/011-probs.html#random-variables",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "A Random Variable is any variable whose value cannot be predicted exactly. For example:\n\nThe message you get in a fortune cookie\nThe amount of time spent searching for your keys\nThe number of likes you get on a social media post\nThe number of customers that enter a store in a day\n\nAll of these are random variables.\nSome random variables are discrete and some are continuous"
  },
  {
    "objectID": "lectures/01-Random-Variables/011-probs.html#discrete-and-continuous-rv",
    "href": "lectures/01-Random-Variables/011-probs.html#discrete-and-continuous-rv",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Discrete and Continuous RV",
    "text": "Discrete and Continuous RV\nWhat’s the difference?\n\n\nDiscrete\n\nCounted\nTake on a small number of possible values\nEx: Number of M&Ms in your bag\n\n\nContinuous\n\nMeasured\nCan take on an infinite number of possible values\nEx: How heavy your bag is\n\n\n\nVariables can also be categorical instead of numeric. They may represent qualitative data that can be divided into categories or groups. For now, we will lump them in with discrete variables"
  },
  {
    "objectID": "lectures/01-Random-Variables/011-probs.html#discrete-probability-distributions",
    "href": "lectures/01-Random-Variables/011-probs.html#discrete-probability-distributions",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Discrete Probability Distributions",
    "text": "Discrete Probability Distributions\nConsider the event of a dice roll. This action produces a discrete random variable.\nIt could take on values 1 to 6 and, if it is a fair die, it takes on each of those values with equali probability \\(1/6\\).\nOur notation will be:\n\n\\(X\\) is the random variable, \\(x_{i}\\) is a potential outcome for \\(X\\), and each potential outcome \\(x_{i}\\) happens with probability \\(p_{i}\\)\n\n\n\n\n\\(x_{i}\\)\n1\n2\n3\n4\n5\n6\n\n\n\n\n\n\\(p_{i}\\)\n1/6\n1/6\n1/6\n1/6\n1/6\n1/6"
  },
  {
    "objectID": "lectures/01-Random-Variables/011-probs.html#discrete-probability-distributions-1",
    "href": "lectures/01-Random-Variables/011-probs.html#discrete-probability-distributions-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Discrete Probability Distributions",
    "text": "Discrete Probability Distributions\nConsider another random variable \\(X\\) to be the sum of two dice rolls. In the table below, the first row represents the potential outcomes for the first roll and the first column represents the potential outcomes for the second roll. The values inside the table represent the potential outcomes for \\(X\\) (the sum)\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n2\n3\n4\n5\n6\n7\n8\n\n\n3\n4\n5\n6\n7\n8\n9\n\n\n4\n5\n6\n7\n8\n9\n10\n\n\n5\n6\n7\n8\n9\n10\n11\n\n\n6\n7\n8\n9\n10\n11\n12\n\n\n\n\nEach of the cells occur with equal probability. So that X = 2 has probability 1/36. X = 3 has probability 2/36, as it can occur in two ways."
  },
  {
    "objectID": "lectures/01-Random-Variables/013-variance.html",
    "href": "lectures/01-Random-Variables/013-variance.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "The variance of a random variable measures its dispersion. It asks “on average, how far is the variable from its average”? Differences are squared to get rid of the negative sign and punish large deviances a little more. We will use the greek letter \\(\\sigma\\) (“sigma”) for variance \\((\\sigma^{2})\\) and standard deviation \\((\\sigma)\\)\nThe formula is:\n\\[\\begin{align}\n    Var(X) = \\sigma_{X}^{2}\n    &= E[(X - \\mu_{X})^{2}] \\\\\n    &= (x_{1} - \\mu_{X})^{2}p_{1} + (x_{2} - \\mu_{X})^{2}p_{2} + \\cdots + (x_{n} - \\mu_{X})^{2}p_{n} \\\\\n    &= \\sum_{i = 1}^{n} (x_{i} - \\mu_{X})^{2}p_{i}\n\\end{align}\\]\nNote that because of the square and the fact that probabilities \\(p_{i}\\) are never negative, the variance of a RV can never be a negative number"
  },
  {
    "objectID": "lectures/01-Random-Variables/013-variance.html#definition",
    "href": "lectures/01-Random-Variables/013-variance.html#definition",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "The variance of a random variable measures its dispersion. It asks “on average, how far is the variable from its average”? Differences are squared to get rid of the negative sign and punish large deviances a little more. We will use the greek letter \\(\\sigma\\) (“sigma”) for variance \\((\\sigma^{2})\\) and standard deviation \\((\\sigma)\\)\nThe formula is:\n\\[\\begin{align}\n    Var(X) = \\sigma_{X}^{2}\n    &= E[(X - \\mu_{X})^{2}] \\\\\n    &= (x_{1} - \\mu_{X})^{2}p_{1} + (x_{2} - \\mu_{X})^{2}p_{2} + \\cdots + (x_{n} - \\mu_{X})^{2}p_{n} \\\\\n    &= \\sum_{i = 1}^{n} (x_{i} - \\mu_{X})^{2}p_{i}\n\\end{align}\\]\nNote that because of the square and the fact that probabilities \\(p_{i}\\) are never negative, the variance of a RV can never be a negative number"
  },
  {
    "objectID": "lectures/01-Random-Variables/013-variance.html#rules",
    "href": "lectures/01-Random-Variables/013-variance.html#rules",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Rules",
    "text": "Rules\nSome important rules about the way variance works. Let \\(X\\) and \\(Y\\) be random variables and let \\(b\\) be a constant.\n\nThe variance of the sum of two RVs is the sum of their variances plus two times their covariance: \\[\nVar(X + Y) = Var(X) + Var(Y) + 2Cov(X,Y)\n\\]\nConstants can pass outside of a variance if you square them: \\[\nVar(bX) = b^{2}Var(X)\n\\]\nThe variance of a constant is 0: \\[\nVar(b) = 0\n\\]\nThe variance of a RV plus a constant is the variance of that random variable: \\[\nVar(X + b) = Var(X)\n\\]"
  },
  {
    "objectID": "lectures/01-Random-Variables/015-correlation.html",
    "href": "lectures/01-Random-Variables/015-correlation.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "An issue with covariance is that the covariance between two random variables depends on the units those variables are measured in. That’s where correlation comes in:\nCorrelation is another measure of linear association that has the benefit of being dimensionless because the units in the numerator cancel with the units in the denominator.\nIt is also the case that the correlation between two variables is always between -1 and 1. Where correlation = 1, the two variables have a perfect positive linear relationsihp, and when correlation = -1, the two variables have a perfect negative linear relationship.\nWe will use the greek letter \\(\\rho\\) (“rho”) to refer to the correlation between two RVs. The formula is:\n\\[\\begin{align*}\n    \\rho_{XY} =\n    \\dfrac{\n        \\sigma_{XY}\n    }{\n        \\sqrt{\\sigma_{X}^{2}\\sigma_{Y}^{2}}\n    }\n\\end{align*}\\]"
  },
  {
    "objectID": "lectures/01-Random-Variables/015-correlation.html#definition",
    "href": "lectures/01-Random-Variables/015-correlation.html#definition",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "An issue with covariance is that the covariance between two random variables depends on the units those variables are measured in. That’s where correlation comes in:\nCorrelation is another measure of linear association that has the benefit of being dimensionless because the units in the numerator cancel with the units in the denominator.\nIt is also the case that the correlation between two variables is always between -1 and 1. Where correlation = 1, the two variables have a perfect positive linear relationsihp, and when correlation = -1, the two variables have a perfect negative linear relationship.\nWe will use the greek letter \\(\\rho\\) (“rho”) to refer to the correlation between two RVs. The formula is:\n\\[\\begin{align*}\n    \\rho_{XY} =\n    \\dfrac{\n        \\sigma_{XY}\n    }{\n        \\sqrt{\\sigma_{X}^{2}\\sigma_{Y}^{2}}\n    }\n\\end{align*}\\]"
  },
  {
    "objectID": "lectures/03-Estimators-01/031-estimators.html",
    "href": "lectures/03-Estimators-01/031-estimators.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Let’s define some concepts first:\nEstimand\n\nQuantity that is to be estimated in a statistical analysis\n\nEstimator\n\nA rule (or formula) for estimating an unknown population parameter given a sample of data\n\nEstimate\n\nA specific numerical value that we obtain from the smaple data by applying the estimator"
  },
  {
    "objectID": "lectures/03-Estimators-01/031-estimators.html#estimators",
    "href": "lectures/03-Estimators-01/031-estimators.html#estimators",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Let’s define some concepts first:\nEstimand\n\nQuantity that is to be estimated in a statistical analysis\n\nEstimator\n\nA rule (or formula) for estimating an unknown population parameter given a sample of data\n\nEstimate\n\nA specific numerical value that we obtain from the smaple data by applying the estimator"
  },
  {
    "objectID": "lectures/03-Estimators-01/031-estimators.html#estimators-example",
    "href": "lectures/03-Estimators-01/031-estimators.html#estimators-example",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Estimators Example",
    "text": "Estimators Example\nSuppose we want to know the average height of the population in the US\n\nWe have a sample of 1 million Americans\n\nSo then we can identify our Estimand, Estimator, and Estimate\n\nEstimand: The population mean \\((\\mu)\\)\nEstimator: The sample mean \\((\\bar{X})\\)\n\n\\[\n    \\bar{X} = \\dfrac{1}{n} \\sum_{i=1}^{n} X_{i}\n\\]\n\nEstimate: The sample mean \\((\\hat{\\mu} = 5'6'')\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/031-estimators.html#properties-of-estimators",
    "href": "lectures/03-Estimators-01/031-estimators.html#properties-of-estimators",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Properties of Estimators",
    "text": "Properties of Estimators\nThere are many ways to estimate things and they all have their benefits and costs.\nImagine we want to estimate an unknown parameter \\(\\mu\\), and we know the distributions of three competing estimators.\nWhich one should we use?"
  },
  {
    "objectID": "lectures/03-Estimators-01/031-estimators.html#properties-of-estimators---unbiasedness",
    "href": "lectures/03-Estimators-01/031-estimators.html#properties-of-estimators---unbiasedness",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Properties of Estimators - Unbiasedness",
    "text": "Properties of Estimators - Unbiasedness\nWe ask: What properties make an estimator reliable?\nAnswer (1): Unbiasedness\nOn average, does the estimator tend toward the correct value?\n\nFormally: Does the mean of the estimator’s distribution equal the parameter it estimates?\n\\[\n    \\text{Bias}_{\\mu} (\\hat{\\mu}) = E[\\hat{\\mu}] - \\mu\n\\]"
  },
  {
    "objectID": "lectures/03-Estimators-01/031-estimators.html#properties-of-estimators-1",
    "href": "lectures/03-Estimators-01/031-estimators.html#properties-of-estimators-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Properties of estimators",
    "text": "Properties of estimators\nQuestion What properties make an estimator reliable?\nA01: Unbiasedness\n\n\nUnbiased estimator: \\(E\\left[ \\hat{\\mu} \\right] = \\mu\\)\n\n\n\n\n\n\n\n\n\n\nBiased estimator \\(E\\left[ \\hat{\\mu} \\right] \\neq \\mu\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/031-estimators.html#properties-of-estimators---efficiency",
    "href": "lectures/03-Estimators-01/031-estimators.html#properties-of-estimators---efficiency",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Properties of Estimators - Efficiency",
    "text": "Properties of Estimators - Efficiency\nWe ask: What properties make an estimator reliable?\nAnswer (1): Efficiency (Low Variance)\nThe central tendencies (means) of competing distribution are not the only things that matter. We also care about the variance of an estimator.\n\\[\n    Var(\\hat{\\mu}) = E \\left[ (\\hat{\\mu} - E[\\hat{\\mu}])^{2} \\right]\n\\]\nLower variance estimators estimate closer to the mean in each sample"
  },
  {
    "objectID": "lectures/03-Estimators-01/031-estimators.html#properties-of-estimators---efficiency-1",
    "href": "lectures/03-Estimators-01/031-estimators.html#properties-of-estimators---efficiency-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Properties of Estimators - Efficiency",
    "text": "Properties of Estimators - Efficiency\nImagine low variance to be similar to accuracy \\(\\rightarrow\\) tighter estimates"
  },
  {
    "objectID": "lectures/03-Estimators-01/031-estimators.html#the-bias-variance-tradeoff",
    "href": "lectures/03-Estimators-01/031-estimators.html#the-bias-variance-tradeoff",
    "title": "EC 320 - Intro. Econometrics",
    "section": "The Bias-Variance Tradeoff",
    "text": "The Bias-Variance Tradeoff\nMuch like everything, there are tradeoffs from gaining one thing over another.\nShould we be willing to take a bit of bias to reduce the variance?\nIn economics/causal inference, we emphasize unbiasedness"
  },
  {
    "objectID": "lectures/03-Estimators-01/031-estimators.html#unbiased-estimators",
    "href": "lectures/03-Estimators-01/031-estimators.html#unbiased-estimators",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Unbiased estimators",
    "text": "Unbiased estimators\nIn addition to the sample mean, there are other unbiased estimators we will often use\n\n\nSample variance estimates the variance \\(\\sigma^{2}\\)\n\n\n\nSample covariance setimates the covariance \\(\\sigma_{XY}\\)\n\n\n\nSample correlation estimates the pop. correlation coefficient \\(\\rho_{XY}\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/031-estimators.html#sample-variance",
    "href": "lectures/03-Estimators-01/031-estimators.html#sample-variance",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Sample Variance",
    "text": "Sample Variance\nThe sample variance, \\(S_{X}^{2}\\), is an unbiased estimator of the population variance\n\n\\[\n    S_{X}^{2} = \\dfrac{1}{n - 1} \\sum_{i=1}^{n} (X_{i} - \\bar{X})^{2}.\n\\]"
  },
  {
    "objectID": "lectures/03-Estimators-01/031-estimators.html#sample-covariance",
    "href": "lectures/03-Estimators-01/031-estimators.html#sample-covariance",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Sample Covariance",
    "text": "Sample Covariance\nThe sample covariance, \\(S_{XY}\\), is an unbiaed estimator of the population covariance\n\n\\[\n    S_{XY} = \\dfrac{1}{n-1} \\sum_{i=1}^{n} (X_{i} - \\bar{X})(Y_{i} - \\bar{Y}).\n\\]"
  },
  {
    "objectID": "lectures/03-Estimators-01/031-estimators.html#sample-correlation",
    "href": "lectures/03-Estimators-01/031-estimators.html#sample-correlation",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Sample Correlation",
    "text": "Sample Correlation\nSample correlation, \\(r_{XY}\\), is an unbiased estimator of the population correlation coefficient\n\n\\[\n    r_{XY} = \\dfrac{S_{XY}}{\\sqrt{S_{X}^{2}}\\sqrt{S_{Y}^{2}}}.\n\\]"
  },
  {
    "objectID": "lectures/03-Estimators-01/033-linear-ex.html#ex.-effect-of-police-on-crime",
    "href": "lectures/03-Estimators-01/033-linear-ex.html#ex.-effect-of-police-on-crime",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Ex. Effect of police on crime",
    "text": "Ex. Effect of police on crime\n\nEmpirical question:\n\nDoes the number of on-campus police officers affect campus crime rates? If so, by how much?\n\n\n. . .\n\nAlways plot your data first"
  },
  {
    "objectID": "lectures/03-Estimators-01/033-linear-ex.html#ex.-effect-of-police-on-crime-1",
    "href": "lectures/03-Estimators-01/033-linear-ex.html#ex.-effect-of-police-on-crime-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Ex. Effect of police on crime",
    "text": "Ex. Effect of police on crime\nThe scatter plot suggest that a weak positive relationship exists\n\nA sample correlation of 0.14 confirms this\n\n\n. . .\nBut correlation does not imply causation\n. . .\n\nLets estimate a statistical model"
  },
  {
    "objectID": "lectures/03-Estimators-01/033-linear-ex.html#ex.-effect-of-police-on-crime-2",
    "href": "lectures/03-Estimators-01/033-linear-ex.html#ex.-effect-of-police-on-crime-2",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Ex. Effect of police on crime",
    "text": "Ex. Effect of police on crime\nWe express the relationship between a dependent variable and an independent variable as linear:\n\\[\n{\\text{Crime}_i} = \\beta_0 + \\beta_1 \\text{Police}_i + u_i.\n\\]\n\n\\(\\beta_0\\) is the intercept or constant.\n\\(\\beta_1\\) is the slope coefficient.\n\\(u_i\\) is an error term or disturbance term."
  },
  {
    "objectID": "lectures/03-Estimators-01/033-linear-ex.html#ex.-effect-of-police-on-crime-3",
    "href": "lectures/03-Estimators-01/033-linear-ex.html#ex.-effect-of-police-on-crime-3",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Ex. Effect of police on crime",
    "text": "Ex. Effect of police on crime\nThe intercept tells us the expected value of \\(\\text{Crime}_i\\) when \\(\\text{Police}_i = 0\\).\n\\[\n\\text{Crime}_i = {\\color{#BF616A} \\beta_{0}} + \\beta_1\\text{Police}_i + u_i\n\\]\nUsually not the focus of an analysis."
  },
  {
    "objectID": "lectures/03-Estimators-01/033-linear-ex.html#ex.-effect-of-police-on-crime-4",
    "href": "lectures/03-Estimators-01/033-linear-ex.html#ex.-effect-of-police-on-crime-4",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Ex. Effect of police on crime",
    "text": "Ex. Effect of police on crime\nThe slope coefficient tells us the expected change in \\(\\text{Crime}_i\\) when \\(\\text{Police}_i\\) increases by one.\n\\[\n\\text{Crime}_i = \\beta_0 + {\\color{#BF616A} \\beta_1} \\text{Police}_i + u_i\n\\]\n“A one-unit increase in \\(\\text{Police}_i\\) is associated with a \\(\\color{#BF616A}{\\beta_1}\\)-unit increase in \\(\\text{Crime}_i\\).”\n. . .\nInterpretation of this parameter is crucial\n. . .\nUnder certain (strong) assumptions1, \\(\\color{#BF616A}{\\beta_1}\\) is the effect of \\(X_i\\) on \\(Y_i\\).\n\nOtherwise, it’s the association of \\(X_i\\) with \\(Y_i\\)."
  },
  {
    "objectID": "lectures/03-Estimators-01/033-linear-ex.html#ex.-effect-of-police-on-crime-5",
    "href": "lectures/03-Estimators-01/033-linear-ex.html#ex.-effect-of-police-on-crime-5",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Ex. Effect of police on crime",
    "text": "Ex. Effect of police on crime\nThe error term reminds us that \\(\\text{Police}_i\\) does not perfectly explain \\(Y_i\\).\n\\[\n\\text{Crime}_i = \\beta_0 + \\beta_1\\text{Police}_i + {\\color{#BF616A} u_i}\n\\]\nRepresents all other factors that explain \\(\\text{Crime}_i\\).\n\nUseful mnemonic: pretend that \\(u\\) stands for “unobserved” or “unexplained.”"
  },
  {
    "objectID": "lectures/03-Estimators-01/033-linear-ex.html#ex.-effect-of-police-on-crime-6",
    "href": "lectures/03-Estimators-01/033-linear-ex.html#ex.-effect-of-police-on-crime-6",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Ex. Effect of police on crime",
    "text": "Ex. Effect of police on crime\nHow might we apply the simple linear regression model to our question about the effect of on-campus police on campus crime?\n\\[\n\\text{Crime}_i = \\beta_0 + \\beta_1\\text{Police}_i + u_i.\n\\]\n\n\\(\\beta_0\\) is the crime rate for colleges without police.\n\\(\\beta_1\\) is the increase in the crime rate for an additional police officer per 1000 students."
  },
  {
    "objectID": "lectures/03-Estimators-01/033-linear-ex.html#ex.-effect-of-police-on-crime-7",
    "href": "lectures/03-Estimators-01/033-linear-ex.html#ex.-effect-of-police-on-crime-7",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Ex. Effect of police on crime",
    "text": "Ex. Effect of police on crime\nHow might we apply the simple linear regression model to our question?\n\\[\n\\text{Crime}_i = \\beta_0 + \\beta_1\\text{Police}_i + u_i\n\\]\n\\(\\beta_0\\) and \\(\\beta_1\\) are the unobserved population parameters we want\n. . .\n\nWe estimate\n\n\\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) generate predictions of \\(\\text{Crime}_i\\) called \\(\\widehat{\\text{Crime}_i}\\).\nWe call the predictions of the dependent variable fitted values.\n\n. . .\n\nTogether, these trace a line: \\(\\widehat{\\text{Crime}_i} = \\hat{\\beta_0} + \\hat{\\beta_1}\\text{Police}_i\\)."
  },
  {
    "objectID": "lectures/03-Estimators-01/033-linear-ex.html#section-1",
    "href": "lectures/03-Estimators-01/033-linear-ex.html#section-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "So, the question becomes, how do I pick \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/033-linear-ex.html#section-2",
    "href": "lectures/03-Estimators-01/033-linear-ex.html#section-2",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Let’s take some guesses: \\(\\hat{\\beta_0} = 60\\) and \\(\\hat{\\beta}_{1} = -7\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/033-linear-ex.html#section-3",
    "href": "lectures/03-Estimators-01/033-linear-ex.html#section-3",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Let’s take some guesses: \\(\\hat{\\beta_0} = 30\\) and \\(\\hat{\\beta}_{1} = 0\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/033-linear-ex.html#section-4",
    "href": "lectures/03-Estimators-01/033-linear-ex.html#section-4",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Let’s take some guesses: \\(\\hat{\\beta_0} = 15.6\\) and \\(\\hat{\\beta}_{1} = 7.94\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/033-linear-ex.html#residuals",
    "href": "lectures/03-Estimators-01/033-linear-ex.html#residuals",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Residuals",
    "text": "Residuals\nUsing \\(\\hat{\\beta}_{0}\\) and \\(\\hat{\\beta}_{1}\\) to make \\(\\hat{y}_{i}\\) generates misses.\n\n\n\n \\(\\hat{\\beta_0} = 60 \\;\\) Guess\n\n \\(\\hat{\\beta_0} = 30 \\;\\) Guess\n\n \\(\\hat{\\beta_0} = 15 \\;\\) Guess"
  },
  {
    "objectID": "lectures/03-Estimators-01/033-linear-ex.html#residuals-sum-of-squares-rss",
    "href": "lectures/03-Estimators-01/033-linear-ex.html#residuals-sum-of-squares-rss",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Residuals Sum of Squares (RSS)",
    "text": "Residuals Sum of Squares (RSS)\nWhat if we picked an estimator that minimizes the residuals?\nWhy do we not minimize:\n\\[\n    \\sum_{i=1}^{n} \\hat{u}_{i}^{2}\n\\]\nso that the estimator makes fewer big misses?\nThis estimator, the residual sum of squares (RSS), is convenient because squared numbers are never negative so we can minimze an absolut sum of the residuals\nRSS will give bigger penalties to bigger residuals"
  },
  {
    "objectID": "lectures/03-Estimators-01/033-linear-ex.html#minimizing-rss",
    "href": "lectures/03-Estimators-01/033-linear-ex.html#minimizing-rss",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Minimizing RSS",
    "text": "Minimizing RSS\nWe could test thousands of guesses of \\(\\beta_0\\) and \\(\\beta_1\\) an pick the pair the has the smallest RSS\nWe could painstakingly do that, and eventually figure out which one fits best.\nOr… We could just do a little math"
  },
  {
    "objectID": "lectures/03-Estimators-01/033-linear-ex.html#footnotes",
    "href": "lectures/03-Estimators-01/033-linear-ex.html#footnotes",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAssumptions regarding the error term↩︎"
  },
  {
    "objectID": "lectures/03-Estimators-01/035-ols-interpretation.html",
    "href": "lectures/03-Estimators-01/035-ols-interpretation.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "There are two stages of interpretation of a regression equation\n\nInterpret regression estimates into words\nDeciding whether this interpretation should be taken at face value\n\n\nBoth stages are important, but for now, we will focus on the first\nLet’s revisit our crime example"
  },
  {
    "objectID": "lectures/03-Estimators-01/035-ols-interpretation.html#interpretation",
    "href": "lectures/03-Estimators-01/035-ols-interpretation.html#interpretation",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "There are two stages of interpretation of a regression equation\n\nInterpret regression estimates into words\nDeciding whether this interpretation should be taken at face value\n\n\nBoth stages are important, but for now, we will focus on the first\nLet’s revisit our crime example"
  },
  {
    "objectID": "lectures/03-Estimators-01/035-ols-interpretation.html#ex-effect-of-police-on-crime",
    "href": "lectures/03-Estimators-01/035-ols-interpretation.html#ex-effect-of-police-on-crime",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Ex: Effect of Police on Crime",
    "text": "Ex: Effect of Police on Crime\nUsing the OLS formulas, we get \\(\\hat{\\beta}_{0} = 18.41\\) and \\(\\hat{\\beta}_{1} = 1.76\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/035-ols-interpretation.html#coefficient-interpretation",
    "href": "lectures/03-Estimators-01/035-ols-interpretation.html#coefficient-interpretation",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Coefficient Interpretation",
    "text": "Coefficient Interpretation\nHow do I interpret \\(\\hat{\\beta}_{0} = 18.41\\) and \\(\\hat{\\beta}_{1} = 1.76\\)?\nThe general interpration of the intercept is the estimated value of \\(y_{i}\\) when \\(x_{i} = 0\\)\nAnd the general interpretation of the slope parameter is the estimated change \\(y_{i}\\) for the marginal increase \\(x_{i}\\)\n. . .\nFirst, it is important to understand the units:\n\n\\(\\widehat{\\text{Crime}}_{i}\\) is measured as a crime rate, the number of crimes per 1,000 students on campus\n\\(\\text{Police}_{i}\\) is also measured as a rate, the number of police officers per 1,000 students on campus"
  },
  {
    "objectID": "lectures/03-Estimators-01/035-ols-interpretation.html#coefficient-interpretation-1",
    "href": "lectures/03-Estimators-01/035-ols-interpretation.html#coefficient-interpretation-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Coefficient Interpretation",
    "text": "Coefficient Interpretation\nUsing OLS gives us the fitted line\n\\[\n\\widehat{\\text{Crime}_i} = \\hat{\\beta}_1 + \\hat{\\beta}_2\\text{Police}_i.\n\\]\nWhat does \\(\\hat{\\beta_0}\\) = \\(18.41\\) tell us? Without any police on campus, the crime rate is \\(18.41\\) per 1,000 people on campus\n. . .\nWhat does \\(\\hat{\\beta_1}\\) = \\(1.76\\) tell us? For each additional police officer per 1,000, there is an associated increase in the crime rate by \\(1.76\\) crimes per 1,000 people on campus.\n. . .\nDoes this mean that police cause crime? Probably not.\nThis is where deciding if the interpretation should be taken at face value. It now becomes your job to bring reason to the values."
  },
  {
    "objectID": "lectures/04-Estimators-02/041-ols-properties.html",
    "href": "lectures/04-Estimators-02/041-ols-properties.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "There are three important OLS properties\n\n\nThe point \\((\\bar{x}, \\bar{y})\\) is always on the regression line\n\n\n\nResiduals sum to zero: \\(\\sum_{i}^{n} \\hat{u}_{i} = 0\\)\n\n\n\nThe sample covariance between the independent variable and the residuals is zero: \\(\\sum_{i}^{n} x_{i}\\hat{u}_{i} = 0\\)"
  },
  {
    "objectID": "lectures/04-Estimators-02/041-ols-properties.html#important-properties",
    "href": "lectures/04-Estimators-02/041-ols-properties.html#important-properties",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "There are three important OLS properties\n\n\nThe point \\((\\bar{x}, \\bar{y})\\) is always on the regression line\n\n\n\nResiduals sum to zero: \\(\\sum_{i}^{n} \\hat{u}_{i} = 0\\)\n\n\n\nThe sample covariance between the independent variable and the residuals is zero: \\(\\sum_{i}^{n} x_{i}\\hat{u}_{i} = 0\\)"
  },
  {
    "objectID": "lectures/04-Estimators-02/041-ols-properties.html#property-1---proof",
    "href": "lectures/04-Estimators-02/041-ols-properties.html#property-1---proof",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Property 1 - Proof",
    "text": "Property 1 - Proof\nThe point \\((\\bar{x}, \\bar{y})\\) is always on the regression line\n\nStart with the regression line: \\(\\hat{y}_{i} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1} x_{i}\\)\nRecall that \\(\\hat{\\beta}_{0} = \\bar{y} - \\hat{\\beta}_{1}\\bar{x}\\)\nPlug that in \\(\\hat{\\beta}_{0}\\) and substitute \\(\\bar{x}\\) for \\(x_{i}\\):\n\n\\[\\begin{align*}\n    \\hat{y}_{i} &= \\bar{y} - \\hat{\\beta}_{1}\\bar{x} + \\hat{\\beta}_{1} \\bar{x} \\\\\n    \\hat{y}_{i} &= \\bar{y}\n\\end{align*}\\]"
  },
  {
    "objectID": "lectures/04-Estimators-02/041-ols-properties.html#property-2---proof",
    "href": "lectures/04-Estimators-02/041-ols-properties.html#property-2---proof",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Property 2 - Proof",
    "text": "Property 2 - Proof\nResiduals sum to zero: \\(\\sum_{i}^{n} \\hat{u}_{i} = 0\\)\n\nRecall a couple of things we have derived:\n\n\\[\n    \\hat{y}_{i} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1}x_{i} \\;\\; \\text{and} \\;\\; \\hat{u}_{i} = y_{i} - \\hat{y}_{i}\n\\]\n\nThe sum of residuals is:\n\n\\[\n    \\sum_{i} \\hat{u}_{i} = \\sum_{i} (y_{i} - \\hat{y}_{i}) = \\sum_{i} y_{i} - \\sum \\hat{y}_{i}\n\\]\n\nRecall the fact that \\(\\sum_{i} y_{i} = n\\bar{y}\\) and also:\n\n\\[\\begin{align*}\n    \\sum_{i} \\hat{y}_{i} &= \\sum_{i} (\\hat{\\beta}_{0} + \\hat{\\beta}_{1}x_{i})\n    = n \\hat{\\beta}_{0} + \\hat{\\beta}_{1} \\sum_{i} x_{i} \\\\\n    &= n (\\bar{y}_{i} - \\hat{\\beta}_{1}\\bar{x}) + \\hat{\\beta}_{1} n\\bar{x} = n\\bar{y}_{i}\n\\end{align*}\\]\n\nSo:\n\n\\[\n    \\sum_{i} \\hat{u}_{i} = n\\bar{y}_{i} - n\\bar{y}_{i} = 0\n\\]"
  },
  {
    "objectID": "lectures/04-Estimators-02/041-ols-properties.html#property-3---proof",
    "href": "lectures/04-Estimators-02/041-ols-properties.html#property-3---proof",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Property 3 - Proof",
    "text": "Property 3 - Proof\nThe sample covariance between the independent variable and the residuals is zero: \\(\\sum_{i}^{n} x_{i}\\hat{u}_{i} = 0\\)\n\nStart with our residuals: \\(\\hat{u}_{i} = y_{i} - \\hat{\\beta}_{0} - \\hat{\\beta}_{1}x_{i}\\)\nMultiply both sides by \\(x_{i}\\) and sum them:\n\n\\[\n    \\sum_{i} x_{i}\\hat{u}_{i} = \\sum_{i} x_{i}y_{i} - \\hat{\\beta}_{0}\\sum_{i} x_{i} - \\hat{\\beta}_{1}\\sum_{i} x_{i}^{2}\n\\]\n\nRecall from our \\(\\hat{\\beta}_{1}\\) derivation that \\(\\sum_{i} x_{i}y_{i} = \\hat{\\beta}_{0}\\sum_{i} x_{i} + \\hat{\\beta}_{1}\\sum_{i} x_{i}^{2}\\)\n\nSo: \\(\\sum_{i}^{n} x_{i}\\hat{u}_{i} = \\hat{\\beta}_{0}\\sum_{i} x_{i} + \\hat{\\beta}_{1}\\sum_{i} x_{i}^{2} - \\hat{\\beta}_{0}\\sum_{i} x_{i} - \\hat{\\beta}_{1}\\sum_{i} x_{i}^{2} = 0\\)"
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "The most important assumptions concern the error term \\(u_{i}\\).\nImportant: An error \\(u_{i}\\) and a residual \\(\\hat{u}_{i}\\) are related, but different.\nTake for example, a model of the effects of education on wages.\n\nError:\n\nDifference between the wage of a worker with 11 years of education and the expected wage with 11 years of education\n\nResidual:\n\nDifference between the wage of a worker with 11 years of education and the average wage of workers with 11 years of education\n\n. . .\n\nPopulation vs. Sample"
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#residuals-vs-errors",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#residuals-vs-errors",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "The most important assumptions concern the error term \\(u_{i}\\).\nImportant: An error \\(u_{i}\\) and a residual \\(\\hat{u}_{i}\\) are related, but different.\nTake for example, a model of the effects of education on wages.\n\nError:\n\nDifference between the wage of a worker with 11 years of education and the expected wage with 11 years of education\n\nResidual:\n\nDifference between the wage of a worker with 11 years of education and the average wage of workers with 11 years of education\n\n. . .\n\nPopulation vs. Sample"
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#residuals-vs-errors-1",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#residuals-vs-errors-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Residuals vs Errors",
    "text": "Residuals vs Errors\nA residual tells us how a worker’s wages comapre to the average wages of workers in the sample with the same level of education"
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#residuals-vs-errors-2",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#residuals-vs-errors-2",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Residuals vs Errors",
    "text": "Residuals vs Errors\nA residual tells us how a worker’s wages comapre to the average wages of workers in the sample with the same level of education"
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#residuals-vs-errors-3",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#residuals-vs-errors-3",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Residuals vs Errors",
    "text": "Residuals vs Errors\nAn error tells us how a worker’s wages compare to the expected wages of workers in the population with the same level of education"
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#classical-assumptions-of-ols",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#classical-assumptions-of-ols",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Classical Assumptions of OLS",
    "text": "Classical Assumptions of OLS\nA1. Linearity: The population relationship is linear in parameters with an additive error term\nA2. Sample Variation: There is variation in \\(X\\)\nA3. Exogeneity: The \\(X\\) variable is exogenous\nA4. Homosekdasticity: The error term has the same variance for each value of the independent variable\nA5. Non-Autocorrelation: The values of error terms have independent distributions\nA6. Normality: The population error term is normally distributed with mean zero and variance \\(\\sigma^{2}\\)"
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#a1.-linearity",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#a1.-linearity",
    "title": "EC 320 - Intro. Econometrics",
    "section": "A1. Linearity",
    "text": "A1. Linearity\n\nThe population relationship is linear in parameters with an additive error term\n\nExamples\n\n\\(\\text{Wage}_i = \\beta_1 + \\beta_2 \\text{Experience}_i + u_i\\)\n\n. . .\n\n\\(\\log(\\text{Happiness}_i) = \\beta_1 + \\beta_2 \\log(\\text{Money}_i) + u_i\\)\n\n. . .\n\n\\(\\sqrt{\\text{Convictions}_i} = \\beta_1 + \\beta_2 (\\text{Early Childhood Lead Exposure})_i + u_i\\)\n\n. . .\n\n\\(\\log(\\text{Earnings}_i) = \\beta_1 + \\beta_2 \\text{Education}_i + u_i\\)"
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#a1.-linearity-1",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#a1.-linearity-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "A1. Linearity",
    "text": "A1. Linearity\n\nThe population relationship is linear in parameters with an additive error term.\n\nViolations\n\n\\(\\text{Wage}_i = (\\beta_1 + \\beta_2 \\text{Experience}_i)u_i\\)\n\n. . .\n\n\\(\\text{Consumption}_i = \\frac{1}{\\beta_1 + \\beta_2 \\text{Income}_i} + u_i\\)\n\n. . .\n\n\\(\\text{Population}_i = \\frac{\\beta_1}{1 + e^{\\beta_2 + \\beta_3 \\text{Food}_i}} + u_i\\)\n\n. . .\n\n\\(\\text{Batting Average}_i = \\beta_1 (\\text{Wheaties Consumption})_i^{\\beta_2} + u_i\\)"
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#a2.-sample-variation",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#a2.-sample-variation",
    "title": "EC 320 - Intro. Econometrics",
    "section": "A2. Sample Variation",
    "text": "A2. Sample Variation\n\nThere is variation in \\(X\\).\n\nExample"
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#a2.-sample-variation-1",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#a2.-sample-variation-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "A2. Sample Variation",
    "text": "A2. Sample Variation\n\nThere is variation in \\(X\\).\n\nViolation\n\n\n\n\n\nWe will see later that variation matters for inference as well"
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#a3.-exogeneity",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#a3.-exogeneity",
    "title": "EC 320 - Intro. Econometrics",
    "section": "A3. Exogeneity",
    "text": "A3. Exogeneity\n\nThe \\(X\\) variable is exogenous\n\nWe can write this as:\n\\[\n    \\mathbb{E}[(u|X)] = 0\n\\]\nWhich essentially says that the expected value of the errors term, conditional on the variable \\(X\\) is 0. The assignment of \\(X\\) is effectively random.\nA significant implication of this is no selection bias or omitted variable bias"
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#a3.-exogeneity-1",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#a3.-exogeneity-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "A3. Exogeneity",
    "text": "A3. Exogeneity\n\nThe \\(X\\) variable is exogenous\n\n\\[\n    \\mathbb{E}[(u|X)] = 0\n\\]\nExample\nIn the labor market, an important component of \\(u\\) is unobserved ability\n\n\\(\\mathbb{E}(u|\\text{Education} = 12) = 0\\) and \\(\\mathbb{E}(u|\\text{Education} = 20) = 0\\)\n\\(\\mathbb{E}(u|\\text{Education} = 0) = 0\\) and \\(\\mathbb{E}(u|\\text{Education} = 40) = 0\\)\n\nnote: This is an assumption that does not necessarily hold true in real life, but with enough observations we can comfortably assume something like this"
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#a3.-exogeneity-2",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#a3.-exogeneity-2",
    "title": "EC 320 - Intro. Econometrics",
    "section": "A3. Exogeneity",
    "text": "A3. Exogeneity\n\n\nValid Exogeneity\n\\[\n    \\mathbb{E}[(u|X)] = 0\n\\]\n\n\n\n\n\n\nInvalid Exogeneity\n\\[\n    \\mathbb{E}[(u|X)] \\neq 0\n\\]"
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#interlude-unbiasedness-of-ols",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#interlude-unbiasedness-of-ols",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Interlude: Unbiasedness of OLS",
    "text": "Interlude: Unbiasedness of OLS\nWhen can we trust OLS?\nIn estimators, the concept of bias means that the expected value of the estimate is different from the true population parameter.\nGraphically we have:\n\n\nUnbiased estimator: \\(\\mathop{\\mathbb{E}}\\left[ \\hat{\\beta} \\right] = \\beta\\)\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\nBiased estimator: \\(\\mathop{\\mathbb{E}}\\left[ \\hat{\\beta} \\right] \\neq \\beta\\)"
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#is-ols-unbiased",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#is-ols-unbiased",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Is OLS Unbiased?",
    "text": "Is OLS Unbiased?\nWe require our first 3 assumptions for unbaised OLS estimator\nA1. Linearity: The population relationship is linear in parameters with an additive error term\nA2. Sample Variation: There is variation in \\(X\\)\nA3. Exogeneity: The \\(X\\) variable is exogenous\nAnd we can mathematically prove it!"
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#proving-unbiasedness-of-ols",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#proving-unbiasedness-of-ols",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Proving Unbiasedness of OLS",
    "text": "Proving Unbiasedness of OLS\nSuppose we have the following model\n\\[\n    y_{i} = \\beta_{1} + \\beta_{2}x_{i} + u_{i}\n\\]\n. . .\nThe slope parameter follows as:\n\\[\n\\hat{\\beta}_2 = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum(x_i - \\bar{x})^2}\n\\]\n. . .\n(As shown in section 2.3 in ItE) that the estimator \\(\\hat{\\beta_2}\\), can be broken up into a nonrandom and a random component:\n\nProving unbiasedness of simple OLS\nSubstitute for \\(y_i\\):\n\\[\n\\hat{\\beta}_2 = \\frac{\\sum((\\beta_1 + \\beta_2x_i + u_i) - \\bar{y})(x_i - \\bar{x})}{\\sum(x_i - \\bar{x})^2}\n\\]\n. . .\nSubstitute \\(\\bar{y} = \\beta_1 + \\beta_2\\bar{x}\\):\n\\[\n\\hat{\\beta}_2 = \\frac{\\sum(u_i(x_i - \\bar{x}))}{\\sum(x_i - \\bar{x})^2} + \\frac{\\sum(\\beta_2x_i(x_i - \\bar{x}))}{\\sum(x_i - \\bar{x})^2}\n\\]\n. . .\nThe non-random component, \\(\\beta_2\\), is factored out:\n\\[\n\\hat{\\beta}_2 = \\frac{\\sum(u_i(x_i - \\bar{x}))}{\\sum(x_i - \\bar{x})^2} + \\beta_2\\frac{\\sum(x_i(x_i - \\bar{x}))}{\\sum(x_i - \\bar{x})^2}\n\\]\n\nProving unbiasedness of simple OLS\nObserve that the second term is equal to 1. Thus, we have:\n\\[\n\\hat{\\beta}_2 = \\beta_2 + \\frac{\\sum(u_i(x_i - \\bar{x}))}{\\sum(x_i - \\bar{x})^2}\n\\]\n. . .\nTaking the expectation,\n\\[\n\\mathbb{E}[\\hat{\\beta_2}] = \\mathbb{E}[\\beta] + \\mathbb{E} \\left[\\frac{\\sum \\hat{u_i} (x_i - \\bar{x})}{\\sum(x_i - \\bar{x})^2} \\right]\n\\]\n. . .\nBy Rules 01 and 02 of expected value and A3:\n\\[\n\\begin{equation*}\n  \\mathbb{E}[\\hat{\\beta_2}] = \\beta + \\frac{\\sum \\mathbb{E}[\\hat{u_i}] (x_i - \\bar{x})}{\\sum(x_i - \\bar{x})^2} = \\beta\n\\end{equation*}\n\\]"
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#required-assumptions",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#required-assumptions",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Required Assumptions",
    "text": "Required Assumptions\nA1. Linearity: The population relationship is linear in parameters with an additive error term.\nA2. Sample Variation: There is variation in \\(X\\).\nA3. Exogeniety: The \\(X\\) variable is exogenous\n\nA3 implies random sampling.\n\n\nResult: OLS is unbiased."
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#classical-assumptions-of-ols-1",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#classical-assumptions-of-ols-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Classical Assumptions of OLS",
    "text": "Classical Assumptions of OLS\nA1. Linearity: The population relationship is linear in parameters with an additive error term.\nA2. Sample Variation: There is variation in \\(X\\).\nA3. Exogeniety: The \\(X\\) variable is exogenous\n. . .\n \n\nThe following 2 assumptions are not required for unbiasedness…\n\n\nBut they are important for an efficient estimator\n\n\nLet’s talk about why variance matters"
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#why-variance-matters",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#why-variance-matters",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Why variance matters",
    "text": "Why variance matters\nUnbiasedness tells us that OLS gets it right, on average. But we can’t tell whether our sample is “typical.”\n\n. . .\nVariance tells us how far OLS can deviate from the population mean.\n\nHow tight is OLS centered on its expected value?\nThis determines the efficiency of our estimator."
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#why-variance-matters-1",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#why-variance-matters-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Why variance matters",
    "text": "Why variance matters\nUnbiasedness tells us that OLS gets it right, on average. But we can’t tell whether our sample is “typical.”\n\nThe smaller the variance, the closer OLS gets, on average, to the true population parameters on any sample.\n\nGiven two unbiased estimators, we want the one with smaller variance.\nIf two more assumptions are satisfied, we are using the most efficient linear estimator."
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#classical-assumptions-of-ols-2",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#classical-assumptions-of-ols-2",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Classical Assumptions of OLS",
    "text": "Classical Assumptions of OLS\nA1. Linearity: The population relationship is linear in parameters with an additive error term.\nA2. Sample Variation: There is variation in \\(X\\).\nA3. Exogeniety: The \\(X\\) variable is exogenous\n. . .\nA4. Homoskedasticity: The error term has the same variance for each value of the independent variable"
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#a4.-homoskedasticity",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#a4.-homoskedasticity",
    "title": "EC 320 - Intro. Econometrics",
    "section": "A4. Homoskedasticity",
    "text": "A4. Homoskedasticity\n\nThe error term has the same variance for each value of the independent variable \\(x_{i}\\)\n\n\\[\n    Var(u|X) = \\sigma^{2}.\n\\]\nExample:"
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#a4.-homoskedasticity-1",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#a4.-homoskedasticity-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "A4. Homoskedasticity",
    "text": "A4. Homoskedasticity\n\nThe error term has the same variance for each value of the independent variable \\(x_{i}\\)\n\n\\[\n    Var(u|X) = \\sigma^{2}.\n\\]\nViolation:"
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#a4.-homoskedasticity-2",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#a4.-homoskedasticity-2",
    "title": "EC 320 - Intro. Econometrics",
    "section": "A4. Homoskedasticity",
    "text": "A4. Homoskedasticity\n\nThe error term has the same variance for each value of the independent variable \\(x_{i}\\)\n\n\\[\n    Var(u|X) = \\sigma^{2}.\n\\]\nViolation:"
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#heteroskedasticity-example",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#heteroskedasticity-example",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Heteroskedasticity Example",
    "text": "Heteroskedasticity Example\nSuppose we study the following relationship:\n\\[\n\\text{Luxury Expenditure}_i = \\beta_1 + \\beta_2 \\text{Income}_i + u_i\n\\]\n\nAs income increases, variation in luxury expenditures increase\n\nVariance of \\(u_i\\) is likely larger for higher-income households\nPlot of the residuals against the household income would likely reveal a funnel-shaped pattern"
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#section",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#section",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Common test for heteroskedasticity… Plot the residuals across \\(X\\)"
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#classical-assumptions-of-ols-3",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#classical-assumptions-of-ols-3",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Classical Assumptions of OLS",
    "text": "Classical Assumptions of OLS\nA1. Linearity: The population relationship is linear in parameters with an additive error term.\nA2.Sample Variation: There is variation in \\(X\\).\nA3. Exogeniety: The \\(X\\) variable is exogenous\nA4. Homoskedasticity: The error term has the same variance for each value of the independent variable\n. . .\nA5. Non-autocorrelation: The values of error terms have independent distributions"
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#a5.-non-autocorrelation",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#a5.-non-autocorrelation",
    "title": "EC 320 - Intro. Econometrics",
    "section": "A5. Non-Autocorrelation",
    "text": "A5. Non-Autocorrelation\n\nThe values of error terms have independent distributions1\n\n\\[\nE[u_i u_j]=0, \\forall i \\text{ s.t. } i \\neq j\n\\]\n. . .\nOr…\n\\[\n\\begin{align*}\n\\mathop{\\text{Cov}}(u_i, u_j) &= E[(u_i - \\mu_u)(u_j - \\mu_u)]\\\\\n                              &= E[u_i u_j] = E[u_i] E[u_j]  = 0, \\text{where } i \\neq j\n\\end{align*}\n\\]"
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#a5.-non-autocorrelation-1",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#a5.-non-autocorrelation-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "A5. Non-Autocorrelation",
    "text": "A5. Non-Autocorrelation\n\nThe values of error terms have independent distributions\n\n\\[\nE[u_i u_j]=0, \\forall i \\text{ s.t. } i \\neq j\n\\]\n\nImplies no systematic association between pairs of individual \\(u_i\\)\nAlmost always some unobserved correlation across individuals2\nReferred to as clustering problem.\nAn easy solution exists where we can adjust our standard errors"
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#section-1",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#section-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Let’s take a moment to talk about the variance of the OLS estimator\n\n\\[\n    Var(\\hat{\\beta}_{1}) = \\dfrac{\n        \\sigma^{2}\n        }{\n        \\sum (x_{i} - \\bar{x})^{2}\n        }\n\\]"
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#classical-assumptions-of-ols-4",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#classical-assumptions-of-ols-4",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Classical Assumptions of OLS",
    "text": "Classical Assumptions of OLS\nA1. Linearity: The population relationship is linear in parameters with an additive error term.\nA2. Sample Variation: There is variation in \\(X\\).\nA3. Exogeniety: The \\(X\\) variable is exogenous\nA4. Homoskedasticity: The error term has the same variance for each value of the independent variable\nA5. Non-autocorrelation: The values of error terms have independent distributions\n\nIf A4 and A5 are satisfied, along with A1, A2, and A3, then we are using the most efficient linear estimator"
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#classical-assumptions-of-ols-5",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#classical-assumptions-of-ols-5",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Classical Assumptions of OLS",
    "text": "Classical Assumptions of OLS\nA1. Linearity: The population relationship is linear in parameters with an additive error term.\nA2. Sample Variation: There is variation in \\(X\\).\nA3. Exogeniety: The \\(X\\) variable is exogenous\nA4. Homoskedasticity: The error term has the same variance for each value of the independent variable\nA5. Non-autocorrelation: The values of error terms have independent distributions\n. . .\nA6. Normality The population error term in normally distributed with mean zero and variance \\(\\sigma^{2}\\)"
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#a6.-normality",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#a6.-normality",
    "title": "EC 320 - Intro. Econometrics",
    "section": "A6. Normality",
    "text": "A6. Normality\n\nThe population error term in normally distributed with mean zero and variance \\(\\sigma^{2}\\)\n\nAlso known as:\n\\[\n    u \\sim N(0,\\sigma^{2})\n\\]\nWhere \\(\\sim\\) means distributed by and \\(N\\) stands for normal distribution\nHowever, A6 is not required for efficiency nor unbiasedness"
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#footnotes",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#footnotes",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNotes: \\(\\forall i = \\text{for all} \\: i\\), \\(\\text{s.t.} = \\text{such that}\\), \\(i \\neq j \\: \\text{means} \\: i \\: \\text{is not equal to} \\: j\\)↩︎\n(e.g. common correlation in unobservables among individuals within a given US state)↩︎"
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#ols",
    "href": "lectures/05-Inference/050-compile.html#ols",
    "title": "Inference",
    "section": "OLS",
    "text": "OLS\nUp to now, we have been focusing on OLS considering:\n\nHow we model regressions with this estimator\nHow the estimator is derived and what properties it demonstrates\nHow the classical assumptions make the estimator BLUE\n\n\nWe have mostly ignored drawing conclusions about the true population parameters from the estimates of the sample data\n\nThis is inference"
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#ols-1",
    "href": "lectures/05-Inference/050-compile.html#ols-1",
    "title": "Inference",
    "section": "OLS",
    "text": "OLS\nThus far we have fit an OLS model to find an answer to the following questions:\n\nHow much does an additional year of schooling increase earnings?\nDoes the number of police officers affect campus crime rates?\n\n\nUp to now, we have not discussed our confidence in our fitted relationship\nEven if all 6 Assumptions hold, sample selection might generate the incorrect conclusions in a completely unbiased, coincidental fashion."
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#sampling-distribution",
    "href": "lectures/05-Inference/050-compile.html#sampling-distribution",
    "title": "Inference",
    "section": "Sampling distribution",
    "text": "Sampling distribution\n\nThe probability distribution of the OLS estimators obtained from repeatedly drawing random samples of the same size from a population and fitting point estimates each time.\n\nProvides information about their variability, accuracy, and precision across different samples.\n\n\nPoint estimates\n\nThe fitted values of the OLS estimator (e.g., \\(\\hat{\\beta}_0, \\hat{\\beta}_1\\))"
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#sampling-distribution-properties",
    "href": "lectures/05-Inference/050-compile.html#sampling-distribution-properties",
    "title": "Inference",
    "section": "Sampling distribution properties",
    "text": "Sampling distribution properties\n1. Unbiasedness: If the Gauss-Markov assumptions hold, the OLS estimators are unbiased (i.e., \\(E(\\hat{\\beta}_0) = \\beta_0\\) and \\(E(\\hat{\\beta}_1) = \\beta_1\\))\n\n2. Variance: The variance of the OLS estimators describes their dispersion around the true population parameters.\n\n\n3. Normality: If the errors are normally distributed or the sample size is large enough, by the Central Limit Theorem, the sampling distribution of the OLS estimators will be approximately normal.1\n\nUseful for making inferences, constructing confidence intervals, and performing hypothesis tests using the t-distribution."
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#sampling-distribution-1",
    "href": "lectures/05-Inference/050-compile.html#sampling-distribution-1",
    "title": "Inference",
    "section": "Sampling distribution",
    "text": "Sampling distribution\nThe sampling distribution of \\(\\hat{\\beta}\\) to conduct hypothesis tests.\nUse all 6 classical assumptions to show that OLS is normally distributed:\n\\[\n\\hat{\\beta} \\sim \\mathop{N}\\left( \\beta, \\frac{\\sigma^2}{\\sum_{i=1}^n (X_i - \\bar{X})^2} \\right)\n\\]\n\nLet’s look at a simulation"
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#inference-1",
    "href": "lectures/05-Inference/050-compile.html#inference-1",
    "title": "Inference",
    "section": "Inference",
    "text": "Inference\nOur current workflow consists of:\n\n\nGet data (points with \\(X\\) and \\(Y\\) values)\nRegress \\(Y\\) on \\(X\\) (aka \\(Y = \\beta_{0} + \\beta_{1}X\\))\nPlot the point estimates \\((\\hat{\\beta}_{0},\\hat{\\beta}_{1})\\) and report them\n\n\n\nBut when do we learn something? We are still missing a step.\n\nFor \\(\\hat{\\beta}_{1}\\), can we rule out a previously hypothesized values?\nHow condifent should we be in the precision of our estimates?\n\nWe need to be careful about our sample being atypical \\(\\Rightarrow\\) uncertainty"
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#section-1",
    "href": "lectures/05-Inference/050-compile.html#section-1",
    "title": "Inference",
    "section": "",
    "text": "However, there is a problem.\n\n \nRecall the variance of the point estimate \\(\\hat{\\beta_1}\\) \\[\n\\mathop{\\text{Var}}(\\hat{\\beta}_1) = \\frac{\\sigma^2}{\\sum_{i=1}^n (X_i - \\bar{X})^2}\n\\]\nThe problem is that \\(\\color{#BF616A}{\\sigma^2}\\) is unobserved. So what do we do? Estimate it."
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#estimating-error-variance",
    "href": "lectures/05-Inference/050-compile.html#estimating-error-variance",
    "title": "Inference",
    "section": "Estimating error variance",
    "text": "Estimating error variance\nWe can estimate the variance of \\(u_i\\) (\\(\\color{#BF616A}{\\sigma^2}\\)) using the sum of squared residuals (RSS):\n\\[\ns^2_u = \\dfrac{\\sum_i \\hat{u}_i^2}{n - k}\n\\]\nwhere \\(n\\) is the number of observations and \\(k\\) is the number of regression parameters. (In a simple linear regression, \\(k=2\\))\n\nIf the assumptions from Gauss-Markov hold, then \\(s^2_u\\) is an unbiased estimator of \\(\\sigma^2\\).\n\n\nIn essence, we are learning from our prediction errors"
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#ols-variance",
    "href": "lectures/05-Inference/050-compile.html#ols-variance",
    "title": "Inference",
    "section": "OLS Variance",
    "text": "OLS Variance\nWith \\(s^2_u = \\dfrac{\\sum_i \\hat{u}_i^2}{n - k}\\), we can calculate the estimated variance of \\(\\hat{\\beta}_1\\)\n\\[\n\\mathop{\\text{Var}}(\\hat{\\beta}_1) = \\frac{s^2_u}{\\sum_{i=1}^n (X_i - \\bar{X})^2}\n\\]\n\nTaking the square root, we get the standard error of the OLS estimator:\n\\[\n\\mathop{\\widehat{\\text{SE}}} \\left( \\hat{\\beta}_1 \\right) = \\sqrt{ \\frac{s^2_u}{\\sum_{i=1}^n (X_i - \\bar{X})^2} }\n\\]\nThe standard error is the standard deviation of the sampling distribution."
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#inference-2",
    "href": "lectures/05-Inference/050-compile.html#inference-2",
    "title": "Inference",
    "section": "Inference",
    "text": "Inference\nAfter deriving the distribution of \\(\\hat{\\beta}_1\\)1, we have two (related) options for formal statistical inference (learning) about our unknown parameter \\(\\beta_1\\):\n\n\nHypothesis testing: Determine whether there is statistically significant evidence to reject a hypothesized value or range of values.\nConfidence intervals: Use the estimate and its standard error to create an interval that will generally2 contain the true parameter.\n\nHint: It’s normal with mean \\(\\beta_1\\) and variance \\(\\frac{\\sigma^2}{\\sum_{i=1}^n (X_i - \\bar{X})^2}\\).E.g., similarly constructed 95% confidence intervals will contain the true parameter 95% of the time."
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#hypothesis-tests",
    "href": "lectures/05-Inference/050-compile.html#hypothesis-tests",
    "title": "Inference",
    "section": "Hypothesis Tests",
    "text": "Hypothesis Tests\nSystematic procedure that gives us evidence to hang our hat on. Starting with a Null hypothesis (\\(H_0\\)) and an Alternative hypothesis (\\(H_1\\))\n\\[\n\\begin{align*}\nH_0:& \\beta_1 = 0 \\\\\nH_1:& \\beta_1 \\neq 0\n\\end{align*}\n\\]\n\nIn the context of the wage regression:\n\\[\n\\text{Wage}_i = \\beta_0 + \\beta_1 \\cdot \\text{Education}_i + u_i\n\\]\n\n\\(H_0\\): Education has no effect on wage\n\n\n\\(H_1\\): Education has an effect on wage"
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#possible-outcomes",
    "href": "lectures/05-Inference/050-compile.html#possible-outcomes",
    "title": "Inference",
    "section": "Possible outcomes",
    "text": "Possible outcomes\nWithin this structure, four possible outcomes exist:\n\n\n1. We fail to reject the null hypothesis and the null is true.\n\n\nEx. Education has no effect on wage and, correctly, we fail to reject \\(H_0\\)."
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#possible-outcomes-1",
    "href": "lectures/05-Inference/050-compile.html#possible-outcomes-1",
    "title": "Inference",
    "section": "Possible outcomes",
    "text": "Possible outcomes\nWithin this structure, four possible outcomes exist:\n\n1. We fail to reject the null hypothesis and the null is true.\n2. We reject the null hypothesis and the null is false.\n\nEx. Education has an effect on wage and, correctly, we reject \\(H_0\\)."
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#possible-outcomes-2",
    "href": "lectures/05-Inference/050-compile.html#possible-outcomes-2",
    "title": "Inference",
    "section": "Possible outcomes",
    "text": "Possible outcomes\nWithin this structure, four possible outcomes exist:\n\n1. We fail to reject the null hypothesis and the null is true.\n2. We reject the null hypothesis and the null is false.\n3. We reject the null hypothesis, but the null is actually true.\n\nEx. Education has no effect on wage, but we incorrectly reject \\(H_0\\).\nThis is an error. Defined as a Type I error."
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#possible-outcomes-3",
    "href": "lectures/05-Inference/050-compile.html#possible-outcomes-3",
    "title": "Inference",
    "section": "Possible outcomes",
    "text": "Possible outcomes\nWithin this structure, four possible outcomes exist:\n\n1. We fail to reject the null hypothesis and the null is true.\n2. We reject the null hypothesis and the null is false.\n3. We reject the null hypothesis, but the null is actually true.\n4. We fail to reject the null hypothesis, but the null is actually false.\n\nEx. Education has an effect on wage, but we incorrectly fail to reject \\(H_0\\).\nThis is an error. Defined as a Type II error."
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#possible-outcomes-4",
    "href": "lectures/05-Inference/050-compile.html#possible-outcomes-4",
    "title": "Inference",
    "section": "Possible outcomes",
    "text": "Possible outcomes\nWithin this structure, four possible outcomes exist:\n\n1. We fail to reject the null hypothesis and the null is true.\n2. We reject the null hypothesis and the null is false.\n3. We reject the null hypothesis, but the null is actually true.1\n4. We fail to reject the null hypothesis, but the null is actually false.2\nType I errorType II error"
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#hypothesis-tests-1",
    "href": "lectures/05-Inference/050-compile.html#hypothesis-tests-1",
    "title": "Inference",
    "section": "Hypothesis Tests",
    "text": "Hypothesis Tests\nGoal: Make a statement about \\(\\beta_1\\) using information on \\(\\hat{\\beta}_1\\).\n\n\\(\\hat{\\beta}_1\\) is random—it could be anything, even if \\(\\beta_1 = 0\\) is true.\n\nBut if \\(\\beta_1 = 0\\) is true, then \\(\\hat{\\beta}_1\\) is unlikely to take values far from zero.\nAs the standard error shrinks, we are even less likely to observe “extreme” values of \\(\\hat{\\beta}_1\\) (assuming \\(\\beta_1 = 0\\)).\n\n\n\nHypothesis testing takes extreme values of \\(\\hat{\\beta}_1\\) as evidence against the null hypothesis, but it will weight them by information about variance the estimated variance of \\(\\hat{\\beta}_1\\)."
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#hypothesis-tests-2",
    "href": "lectures/05-Inference/050-compile.html#hypothesis-tests-2",
    "title": "Inference",
    "section": "Hypothesis Tests",
    "text": "Hypothesis Tests\n\n\\(H_0\\): \\(\\beta_1 = 0\\)\n\n\n\\(H_1\\): \\(\\beta \\neq 0\\)\n\nTo conduct the test, we calculate a \\(t\\)-statistic1:\n\\[\nt = \\frac{\\hat{\\beta}_1 - \\beta_1^0}{\\mathop{\\hat{\\text{SE}}} \\left( \\hat{\\beta}_1 \\right)}\n\\]\nDistributed by a \\(t\\)-distribution with \\(n-2\\) degrees of freedom2.\n\\(\\beta_1^0\\) is the value of \\(\\beta_1\\) in our null hypothesis (e.g., \\(\\beta_1^0 = 0\\)).represents the number of independent values in a sample that are free to vary when estimating statistical parameters."
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#hypothesis-testing-1",
    "href": "lectures/05-Inference/050-compile.html#hypothesis-testing-1",
    "title": "Inference",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\nNormal distribution vs. \\(t\\) distribution\n\nA normal distribution has the same shape for any sample size.\nThe shape of the t distribution depends the degrees of freedom.\n\n\n\nDegrees of freedom = 5."
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#hypothesis-testing-2",
    "href": "lectures/05-Inference/050-compile.html#hypothesis-testing-2",
    "title": "Inference",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\nNormal distribution vs. \\(t\\) distribution\n\nA normal distribution has the same shape for any sample size.\nThe shape of the t distribution depends the degrees of freedom.\n\n\n\nDegrees of freedom = 50."
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#hypothesis-testing-3",
    "href": "lectures/05-Inference/050-compile.html#hypothesis-testing-3",
    "title": "Inference",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\nNormal distribution vs. \\(t\\) distribution\n\nA normal distribution has the same shape for any sample size.\nThe shape of the t distribution depends the degrees of freedom.\n\n\n\nDegrees of freedom = 500."
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#hypothesis-testing-4",
    "href": "lectures/05-Inference/050-compile.html#hypothesis-testing-4",
    "title": "Inference",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\nTwo sided t Tests\nTo conduct a t test, compare the \\(t\\) statistic to the appropriate critical value of the t distribution.\n\nTo find the critical value in a t table, we need the degrees of freedom and the significance level \\(\\alpha\\).\n\nReject (\\(\\text{H}_0\\)) at the \\(\\alpha \\cdot 100\\)-percent level if\n\\[\n\\left| t \\right| = \\left| \\dfrac{\\hat{\\mu} - \\mu_0}{\\mathop{\\text{SE}}(\\hat{\\mu})} \\right| &gt; t_\\text{crit}.\n\\]"
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#hypothesis-tests-3",
    "href": "lectures/05-Inference/050-compile.html#hypothesis-tests-3",
    "title": "Inference",
    "section": "Hypothesis Tests",
    "text": "Hypothesis Tests\nNext, we use the \\(\\color{#434C5E}{t}\\)-statistic to calculate a \\(\\color{#B48EAD}{p}\\)-value.\n\nDescribes the probability of seeing a \\(\\color{#434C5E}{t}\\)-statistic as extreme as the one we observe if the null hypothesis is actually true.\n\nBut…we still need some benchmark to compare our \\(\\color{#B48EAD}{p}\\)-value against."
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#hypothesis-tests-4",
    "href": "lectures/05-Inference/050-compile.html#hypothesis-tests-4",
    "title": "Inference",
    "section": "Hypothesis Tests",
    "text": "Hypothesis Tests\nWe worry mostly about false positives, so we conduct hypothesis tests based on the probability of making a Type I error1.\n\nHow? We select a significance level, \\(\\color{#434C5E}{\\alpha}\\), that specifies our tolerance for false positives (i.e., the probability of Type I error we choose to live with).\n\n\n\n\n\n\n\n\nWe reject the null hypothesis, but the null is actually true."
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#hypothesis-tests-5",
    "href": "lectures/05-Inference/050-compile.html#hypothesis-tests-5",
    "title": "Inference",
    "section": "Hypothesis Tests",
    "text": "Hypothesis Tests\nWe then compare \\(\\color{#434C5E}{\\alpha}\\) to the \\(\\color{#B48EAD}{p}\\)-value of our test.\n\nIf the \\(\\color{#B48EAD}{p}\\)-value is less than \\(\\color{#434C5E}{\\alpha}\\), then we reject the null hypothesis at the \\(\\color{#434C5E}{\\alpha}\\cdot100\\) percent level.\nIf the \\(\\color{#B48EAD}{p}\\)-value is greater than \\(\\color{#434C5E}{\\alpha}\\), then we fail to reject the null hypothesis at the \\(\\color{#434C5E}{\\alpha}\\cdot100\\) percent level.1\n\nNote: Fail to reject \\(\\neq\\) accept."
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#hypothesis-tests-6",
    "href": "lectures/05-Inference/050-compile.html#hypothesis-tests-6",
    "title": "Inference",
    "section": "Hypothesis Tests",
    "text": "Hypothesis Tests\n\\(\\color{#B48EAD}{p}\\)-values are difficult to calculate by hand.\nAlternative: Compare \\(\\color{#434C5E}{t}\\)-statistic to critical values from the \\({\\color{#434C5E} t}\\)-distribution."
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#hypothesis-tests-7",
    "href": "lectures/05-Inference/050-compile.html#hypothesis-tests-7",
    "title": "Inference",
    "section": "Hypothesis Tests",
    "text": "Hypothesis Tests\nNotation: \\(t_{1-\\alpha/2, n-2}\\) or \\(t_\\text{crit}\\).\n\nFind in a \\(t\\)-table using \\(\\color{#434C5E}{\\alpha}\\) and \\(n-2\\) degrees of freedom.\n\nCompare the the critical value to your \\(t\\)-statistic:\n\nIf \\(|t| &gt; |t_{1-\\alpha/2, n-2}|\\), then reject the null.\nIf \\(|t| &lt; |t_{1-\\alpha/2, n-2}|\\), then fail to reject the null."
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#two-sided-tests",
    "href": "lectures/05-Inference/050-compile.html#two-sided-tests",
    "title": "Inference",
    "section": "Two-sided tests",
    "text": "Two-sided tests\nBased on a critical value of \\(t_{1-\\alpha/2, n-2} = t_{0.975, 100} = 1.98\\) we can identify a rejection region on the \\(\\color{#434C5E}{t}\\)-distribution.\n\n\nIf our \\(\\color{#434C5E}{t}\\)-statistic is in the rejection region, then we reject the null hypothesis at the 5 percent level."
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#one-sided-tests",
    "href": "lectures/05-Inference/050-compile.html#one-sided-tests",
    "title": "Inference",
    "section": "One-sided tests",
    "text": "One-sided tests\nWe might be confident in a parameter being non-negative/non-positive.\nOne-sided tests assume that the parameter of interest is either greater than/less than \\(H_0\\).\n\nOption 1 \\(H_0\\): \\(\\beta_1 = 0\\) vs. \\(H_1\\): \\(\\beta_1 &gt; 0\\)\nOption 2 \\(H_0\\): \\(\\beta_1 = 0\\) vs. \\(H_1\\): \\(\\beta_1 &lt; 0\\)\n\n\nIf this assumption is reasonable, then our rejection region changes.\n\nSame \\(\\alpha\\)."
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#one-sided-tests-1",
    "href": "lectures/05-Inference/050-compile.html#one-sided-tests-1",
    "title": "Inference",
    "section": "One-sided tests",
    "text": "One-sided tests\nLeft-tailed: Based on a critical value of \\(t_{1-\\alpha, n-2} = t_{0.95, 100} = 1.66\\), we can identify a rejection region on the \\(t\\)-distribution.\n\n\nIf our \\(t\\) statistic is in the rejection region, then we reject the null hypothesis at the 5 percent level."
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#one-sided-tests-2",
    "href": "lectures/05-Inference/050-compile.html#one-sided-tests-2",
    "title": "Inference",
    "section": "One-sided tests",
    "text": "One-sided tests\nRight-tailed: Based on a critical value of \\(t_{1-\\alpha, n-2} = t_{0.95, 100} = 1.66\\), we can identify a rejection region on the \\(t\\)-distribution.\n\n\nIf our \\(t\\) statistic is in the rejection region, then we reject the null hypothesis at the 5 percent level."
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#confidence-intervals-1",
    "href": "lectures/05-Inference/050-compile.html#confidence-intervals-1",
    "title": "Inference",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nUntil now, we have considered point estimates of population parameters.\n\nSometimes a range of values is more interesting/honest.\n\n\nWe can construct \\((1-\\alpha)\\cdot100\\)-percent level confidence intervals for \\(\\beta_1\\)\n\\[\n\\hat{\\beta}_1 \\pm t_{1-\\alpha/2, n-2} \\, \\mathop{\\hat{\\text{SE}}} \\left( \\hat{\\beta}_1 \\right)\n\\]\n\n\n\\(t_{1-\\alpha/2,n-2}\\) denotes the \\(1-\\alpha/2\\) quantile of a \\(t\\) distribution with \\(n-2\\) degrees of freedom."
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#confidence-intervals-2",
    "href": "lectures/05-Inference/050-compile.html#confidence-intervals-2",
    "title": "Inference",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nQ: Where does the confidence interval formula come from?\n\nA: Formula is a result from the rejection condition of a two-sided test.\nReject \\(H_0\\) if\n\\[\n|t| &gt; t_\\text{crit}\n\\]\n\n\nThe test condition implies that we:\nFail to reject \\(H_0\\) if\n\\[\n|t| \\leq t_\\text{crit}\n\\]\nor, \\[\n-t_\\text{crit} \\leq t \\leq t_\\text{crit}\n\\]"
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#confidence-intervals-3",
    "href": "lectures/05-Inference/050-compile.html#confidence-intervals-3",
    "title": "Inference",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nReplacing \\(t\\) with its formula gives:\nFail to reject \\(H_0\\) if\n\\[-t_\\text{crit} \\leq \\frac{\\hat{\\beta}_1 - \\beta_1^0}{\\mathop{\\hat{\\text{SE}}} \\left( \\hat{\\beta}_1 \\right)} \\leq t_\\text{crit}\n\\]\n\nStandard errors are always positive, so the inequalities do not flip when we multiply by \\(\\mathop{\\hat{\\text{SE}}} \\left( \\hat{\\beta}_1 \\right)\\):\nFail to reject \\(H_0\\) if \\[\n-t_\\text{crit} \\mathop{\\hat{\\text{SE}}} \\left( \\hat{\\beta}_1 \\right) \\leq \\hat{\\beta}_1 - \\beta_1^0\\leq t_\\text{crit} \\mathop{\\hat{\\text{SE}}} \\left( \\hat{\\beta}_1 \\right)\n\\]"
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#confidence-intervals-4",
    "href": "lectures/05-Inference/050-compile.html#confidence-intervals-4",
    "title": "Inference",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nSubtracting \\(\\hat{\\beta}_1\\) yields\nFail to reject \\(H_0\\) if \\[\n-\\hat{\\beta}_1 -t_\\text{crit} \\mathop{\\hat{\\text{SE}}} \\left( \\hat{\\beta}_1 \\right) \\leq - \\beta_1^0 \\leq - \\hat{\\beta}_1 + t_\\text{crit} \\mathop{\\hat{\\text{SE}}} \\left( \\hat{\\beta}_1 \\right)\n\\]\n\nMultiplying by -1 and rearranging gives\nFail to reject \\(H_0\\) if\n\\[\n\\hat{\\beta}_1 - t_\\text{crit} \\mathop{\\hat{\\text{SE}}} \\left( \\hat{\\beta}_1 \\right) \\leq \\beta_1^0 \\leq \\hat{\\beta}_1 + t_\\text{crit} \\mathop{\\hat{\\text{SE}}} \\left( \\hat{\\beta}_1 \\right)\n\\]"
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#confidence-intervals-5",
    "href": "lectures/05-Inference/050-compile.html#confidence-intervals-5",
    "title": "Inference",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nReplacing \\(\\beta_1^0\\) with \\(\\beta_1\\) and dropping the test condition yields the interval:\n\\[\n\\hat{\\beta}_1 - t_\\text{crit} \\mathop{\\hat{\\text{SE}}} \\left( \\hat{\\beta}_1 \\right) \\leq \\beta_1 \\leq \\hat{\\beta}_1 + t_\\text{crit} \\mathop{\\hat{\\text{SE}}} \\left( \\hat{\\beta}_1 \\right)\n\\]\nwhich is equivalent to\n\\[\n\\hat{\\beta}_1 \\pm t_\\text{crit} \\, \\mathop{\\hat{\\text{SE}}} \\left( \\hat{\\beta}_1 \\right)\n\\]"
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#confidence-intervals-6",
    "href": "lectures/05-Inference/050-compile.html#confidence-intervals-6",
    "title": "Inference",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nMain insight:\n\nIf a 95 percent confidence interval contains zero, then we fail to reject the null hypothesis at the 5 percent level.\nIf a 95 percent confidence interval does not contain zero, then we reject the null hypothesis at the 5 percent level.\n\n \nGenerally, a \\((1- \\alpha) \\cdot 100\\) percent confidence interval embeds a two-sided test at the \\(\\alpha \\cdot 100\\) level."
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#confidence-intervals-ex.",
    "href": "lectures/05-Inference/050-compile.html#confidence-intervals-ex.",
    "title": "Inference",
    "section": "Confidence intervals Ex.",
    "text": "Confidence intervals Ex.\n\n\n\n\n\n\n\n\n\n\n\n\n\n95% confidence interval for \\(\\beta_1\\) is:\n\\[\n0.567 \\pm 1.98 \\times 0.0793 = \\left[ 0.410,\\, 0.724 \\right]\n\\]"
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#confidence-intervals-7",
    "href": "lectures/05-Inference/050-compile.html#confidence-intervals-7",
    "title": "Inference",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nWe have a confidence interval for \\(\\beta_1\\), i.e., \\(\\left[ 0.410,\\, 0.724 \\right]\\)\n\nWhat does it mean?\n\n\nInformally: The confidence interval gives us a region (interval) in which we can place some trust (confidence) for containing the parameter.\n\n\nMore formally: If we repeatedly sample from our population and construct confidence intervals for each of these samples, then \\((1-\\alpha) \\cdot100\\) percent of our intervals (e.g., 95%) will contain the population parameter somewhere in the interval."
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#confidence-intervals-8",
    "href": "lectures/05-Inference/050-compile.html#confidence-intervals-8",
    "title": "Inference",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nGoing back to our simulation…\n\nWe drew 10,000 samples (each of size \\(n = 30\\)) from our population and estimated our regression model for each sample:\n\\[\nY_i = \\hat{\\beta}_1 + \\hat{\\beta}_1 X_i + \\hat{u}_i\n\\]\n\n(repeated 10,000 times)\n\n\n\nThe true parameter values are \\(\\beta_0 = 0\\) and \\(\\beta_1 = 0.5\\)\n\n\nLet’s estimate 95% confidence intervals for each of these intervals…"
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#confidence-intervals-9",
    "href": "lectures/05-Inference/050-compile.html#confidence-intervals-9",
    "title": "Inference",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nFrom our previous simulation, 97.7% of 95% confidence intervals contain the true parameter value of \\(\\beta_1\\)."
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#ex.-association-of-police-with-crime",
    "href": "lectures/05-Inference/050-compile.html#ex.-association-of-police-with-crime",
    "title": "Inference",
    "section": "Ex. Association of police with crime",
    "text": "Ex. Association of police with crime\nYou can instruct tidy to return a 95 percent confidence interval for the association of campus police with campus crime:"
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#ex.-association-of-police-with-crime-1",
    "href": "lectures/05-Inference/050-compile.html#ex.-association-of-police-with-crime-1",
    "title": "Inference",
    "section": "Ex. Association of police with crime",
    "text": "Ex. Association of police with crime\n\nFour confidence intervals for the same coefficient."
  },
  {
    "objectID": "lectures/05-Inference/052-inference.html",
    "href": "lectures/05-Inference/052-inference.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Our current workflow consists of:\n\n\nGet data (points with \\(X\\) and \\(Y\\) values)\nRegress \\(Y\\) on \\(X\\) (aka \\(Y = \\beta_{0} + \\beta_{1}X\\))\nPlot the point estimates \\((\\hat{\\beta}_{0},\\hat{\\beta}_{1})\\) and report them\n\n\n. . .\nBut when do we learn something? We are still missing a step.\n\nFor \\(\\hat{\\beta}_{1}\\), can we rule out a previously hypothesized values?\nHow condifent should we be in the precision of our estimates?\n\nWe need to be careful about our sample being atypical \\(\\Rightarrow\\) uncertainty"
  },
  {
    "objectID": "lectures/05-Inference/052-inference.html#inference",
    "href": "lectures/05-Inference/052-inference.html#inference",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Our current workflow consists of:\n\n\nGet data (points with \\(X\\) and \\(Y\\) values)\nRegress \\(Y\\) on \\(X\\) (aka \\(Y = \\beta_{0} + \\beta_{1}X\\))\nPlot the point estimates \\((\\hat{\\beta}_{0},\\hat{\\beta}_{1})\\) and report them\n\n\n. . .\nBut when do we learn something? We are still missing a step.\n\nFor \\(\\hat{\\beta}_{1}\\), can we rule out a previously hypothesized values?\nHow condifent should we be in the precision of our estimates?\n\nWe need to be careful about our sample being atypical \\(\\Rightarrow\\) uncertainty"
  },
  {
    "objectID": "lectures/05-Inference/052-inference.html#section",
    "href": "lectures/05-Inference/052-inference.html#section",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "However, there is a problem.\n. . .\n \nRecall the variance of the point estimate \\(\\hat{\\beta_1}\\) \\[\n\\mathop{\\text{Var}}(\\hat{\\beta}_1) = \\frac{\\sigma^2}{\\sum_{i=1}^n (X_i - \\bar{X})^2}\n\\]\nThe problem is that \\(\\color{#BF616A}{\\sigma^2}\\) is unobserved. So what do we do? Estimate it."
  },
  {
    "objectID": "lectures/05-Inference/052-inference.html#estimating-error-variance",
    "href": "lectures/05-Inference/052-inference.html#estimating-error-variance",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Estimating error variance",
    "text": "Estimating error variance\nWe can estimate the variance of \\(u_i\\) (\\(\\color{#BF616A}{\\sigma^2}\\)) using the sum of squared residuals (RSS):\n\\[\ns^2_u = \\dfrac{\\sum_i \\hat{u}_i^2}{n - k}\n\\]\nwhere \\(n\\) is the number of observations and \\(k\\) is the number of regression parameters. (In a simple linear regression, \\(k=2\\))\n. . .\nIf the assumptions from Gauss-Markov hold, then \\(s^2_u\\) is an unbiased estimator of \\(\\sigma^2\\).\n. . .\nIn essence, we are learning from our prediction errors"
  },
  {
    "objectID": "lectures/05-Inference/052-inference.html#ols-variance",
    "href": "lectures/05-Inference/052-inference.html#ols-variance",
    "title": "EC 320 - Intro. Econometrics",
    "section": "OLS Variance",
    "text": "OLS Variance\nWith \\(s^2_u = \\dfrac{\\sum_i \\hat{u}_i^2}{n - k}\\), we can calculate the estimated variance of \\(\\hat{\\beta}_1\\)\n\\[\n\\mathop{\\text{Var}}(\\hat{\\beta}_1) = \\frac{s^2_u}{\\sum_{i=1}^n (X_i - \\bar{X})^2}\n\\]\n. . .\nTaking the square root, we get the standard error of the OLS estimator:\n\\[\n\\mathop{\\widehat{\\text{SE}}} \\left( \\hat{\\beta}_1 \\right) = \\sqrt{ \\frac{s^2_u}{\\sum_{i=1}^n (X_i - \\bar{X})^2} }\n\\]\nThe standard error is the standard deviation of the sampling distribution."
  },
  {
    "objectID": "lectures/05-Inference/052-inference.html#inference-1",
    "href": "lectures/05-Inference/052-inference.html#inference-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Inference",
    "text": "Inference\nAfter deriving the distribution of \\(\\hat{\\beta}_1\\)1, we have two (related) options for formal statistical inference (learning) about our unknown parameter \\(\\beta_1\\):\n\n\nHypothesis testing: Determine whether there is statistically significant evidence to reject a hypothesized value or range of values.\nConfidence intervals: Use the estimate and its standard error to create an interval that will generally2 contain the true parameter."
  },
  {
    "objectID": "lectures/05-Inference/052-inference.html#footnotes",
    "href": "lectures/05-Inference/052-inference.html#footnotes",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHint: It’s normal with mean \\(\\beta_1\\) and variance \\(\\frac{\\sigma^2}{\\sum_{i=1}^n (X_i - \\bar{X})^2}\\).↩︎\nE.g., similarly constructed 95% confidence intervals will contain the true parameter 95% of the time.↩︎"
  },
  {
    "objectID": "lectures/05-Inference/054-conf-intervals.html",
    "href": "lectures/05-Inference/054-conf-intervals.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Until now, we have considered point estimates of population parameters.\n\nSometimes a range of values is more interesting/honest.\n\n. . .\nWe can construct \\((1-\\alpha)\\cdot100\\)-percent level confidence intervals for \\(\\beta_1\\)\n\\[\n\\hat{\\beta}_1 \\pm t_{1-\\alpha/2, n-2} \\, \\mathop{\\hat{\\text{SE}}} \\left( \\hat{\\beta}_1 \\right)\n\\]\n. . .\n\\(t_{1-\\alpha/2,n-2}\\) denotes the \\(1-\\alpha/2\\) quantile of a \\(t\\) distribution with \\(n-2\\) degrees of freedom."
  },
  {
    "objectID": "lectures/05-Inference/054-conf-intervals.html#confidence-intervals",
    "href": "lectures/05-Inference/054-conf-intervals.html#confidence-intervals",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Until now, we have considered point estimates of population parameters.\n\nSometimes a range of values is more interesting/honest.\n\n. . .\nWe can construct \\((1-\\alpha)\\cdot100\\)-percent level confidence intervals for \\(\\beta_1\\)\n\\[\n\\hat{\\beta}_1 \\pm t_{1-\\alpha/2, n-2} \\, \\mathop{\\hat{\\text{SE}}} \\left( \\hat{\\beta}_1 \\right)\n\\]\n. . .\n\\(t_{1-\\alpha/2,n-2}\\) denotes the \\(1-\\alpha/2\\) quantile of a \\(t\\) distribution with \\(n-2\\) degrees of freedom."
  },
  {
    "objectID": "lectures/05-Inference/054-conf-intervals.html#confidence-intervals-1",
    "href": "lectures/05-Inference/054-conf-intervals.html#confidence-intervals-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nQ: Where does the confidence interval formula come from?\n. . .\nA: Formula is a result from the rejection condition of a two-sided test.\nReject \\(H_0\\) if\n\\[\n|t| &gt; t_\\text{crit}\n\\]\n. . .\nThe test condition implies that we:\nFail to reject \\(H_0\\) if\n\\[\n|t| \\leq t_\\text{crit}\n\\]\nor, \\[\n-t_\\text{crit} \\leq t \\leq t_\\text{crit}\n\\]"
  },
  {
    "objectID": "lectures/05-Inference/054-conf-intervals.html#confidence-intervals-2",
    "href": "lectures/05-Inference/054-conf-intervals.html#confidence-intervals-2",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nReplacing \\(t\\) with its formula gives:\nFail to reject \\(H_0\\) if\n\\[-t_\\text{crit} \\leq \\frac{\\hat{\\beta}_1 - \\beta_1^0}{\\mathop{\\hat{\\text{SE}}} \\left( \\hat{\\beta}_1 \\right)} \\leq t_\\text{crit}\n\\]\n. . .\nStandard errors are always positive, so the inequalities do not flip when we multiply by \\(\\mathop{\\hat{\\text{SE}}} \\left( \\hat{\\beta}_1 \\right)\\):\nFail to reject \\(H_0\\) if \\[\n-t_\\text{crit} \\mathop{\\hat{\\text{SE}}} \\left( \\hat{\\beta}_1 \\right) \\leq \\hat{\\beta}_1 - \\beta_1^0\\leq t_\\text{crit} \\mathop{\\hat{\\text{SE}}} \\left( \\hat{\\beta}_1 \\right)\n\\]"
  },
  {
    "objectID": "lectures/05-Inference/054-conf-intervals.html#confidence-intervals-3",
    "href": "lectures/05-Inference/054-conf-intervals.html#confidence-intervals-3",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nSubtracting \\(\\hat{\\beta}_1\\) yields\nFail to reject \\(H_0\\) if \\[\n-\\hat{\\beta}_1 -t_\\text{crit} \\mathop{\\hat{\\text{SE}}} \\left( \\hat{\\beta}_1 \\right) \\leq - \\beta_1^0 \\leq - \\hat{\\beta}_1 + t_\\text{crit} \\mathop{\\hat{\\text{SE}}} \\left( \\hat{\\beta}_1 \\right)\n\\]\n. . .\nMultiplying by -1 and rearranging gives\nFail to reject \\(H_0\\) if\n\\[\n\\hat{\\beta}_1 - t_\\text{crit} \\mathop{\\hat{\\text{SE}}} \\left( \\hat{\\beta}_1 \\right) \\leq \\beta_1^0 \\leq \\hat{\\beta}_1 + t_\\text{crit} \\mathop{\\hat{\\text{SE}}} \\left( \\hat{\\beta}_1 \\right)\n\\]"
  },
  {
    "objectID": "lectures/05-Inference/054-conf-intervals.html#confidence-intervals-4",
    "href": "lectures/05-Inference/054-conf-intervals.html#confidence-intervals-4",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nReplacing \\(\\beta_1^0\\) with \\(\\beta_1\\) and dropping the test condition yields the interval:\n\\[\n\\hat{\\beta}_1 - t_\\text{crit} \\mathop{\\hat{\\text{SE}}} \\left( \\hat{\\beta}_1 \\right) \\leq \\beta_1 \\leq \\hat{\\beta}_1 + t_\\text{crit} \\mathop{\\hat{\\text{SE}}} \\left( \\hat{\\beta}_1 \\right)\n\\]\nwhich is equivalent to\n\\[\n\\hat{\\beta}_1 \\pm t_\\text{crit} \\, \\mathop{\\hat{\\text{SE}}} \\left( \\hat{\\beta}_1 \\right)\n\\]"
  },
  {
    "objectID": "lectures/05-Inference/054-conf-intervals.html#confidence-intervals-5",
    "href": "lectures/05-Inference/054-conf-intervals.html#confidence-intervals-5",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nMain insight:\n\nIf a 95 percent confidence interval contains zero, then we fail to reject the null hypothesis at the 5 percent level.\nIf a 95 percent confidence interval does not contain zero, then we reject the null hypothesis at the 5 percent level.\n\n \nGenerally, a \\((1- \\alpha) \\cdot 100\\) percent confidence interval embeds a two-sided test at the \\(\\alpha \\cdot 100\\) level."
  },
  {
    "objectID": "lectures/05-Inference/054-conf-intervals.html#confidence-intervals-ex.",
    "href": "lectures/05-Inference/054-conf-intervals.html#confidence-intervals-ex.",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Confidence intervals Ex.",
    "text": "Confidence intervals Ex.\n\n\n\n\n\n. . .\n\n\n\n\n\n. . .\n95% confidence interval for \\(\\beta_1\\) is:\n\\[\n0.567 \\pm 1.98 \\times 0.0793 = \\left[ 0.410,\\, 0.724 \\right]\n\\]"
  },
  {
    "objectID": "lectures/05-Inference/054-conf-intervals.html#confidence-intervals-6",
    "href": "lectures/05-Inference/054-conf-intervals.html#confidence-intervals-6",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nWe have a confidence interval for \\(\\beta_1\\), i.e., \\(\\left[ 0.410,\\, 0.724 \\right]\\)\n\nWhat does it mean?\n\n. . .\nInformally: The confidence interval gives us a region (interval) in which we can place some trust (confidence) for containing the parameter.\n. . .\nMore formally: If we repeatedly sample from our population and construct confidence intervals for each of these samples, then \\((1-\\alpha) \\cdot100\\) percent of our intervals (e.g., 95%) will contain the population parameter somewhere in the interval."
  },
  {
    "objectID": "lectures/05-Inference/054-conf-intervals.html#confidence-intervals-7",
    "href": "lectures/05-Inference/054-conf-intervals.html#confidence-intervals-7",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nGoing back to our simulation…\n. . .\nWe drew 10,000 samples (each of size \\(n = 30\\)) from our population and estimated our regression model for each sample:\n\\[\nY_i = \\hat{\\beta}_1 + \\hat{\\beta}_1 X_i + \\hat{u}_i\n\\]\n\n(repeated 10,000 times)\n\n. . .\nThe true parameter values are \\(\\beta_0 = 0\\) and \\(\\beta_1 = 0.5\\)\n. . .\nLet’s estimate 95% confidence intervals for each of these intervals…"
  },
  {
    "objectID": "lectures/05-Inference/054-conf-intervals.html#confidence-intervals-8",
    "href": "lectures/05-Inference/054-conf-intervals.html#confidence-intervals-8",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nFrom our previous simulation, 97.7% of 95% confidence intervals contain the true parameter value of \\(\\beta_1\\)."
  },
  {
    "objectID": "lectures/05-Inference/054-conf-intervals.html#ex.-association-of-police-with-crime",
    "href": "lectures/05-Inference/054-conf-intervals.html#ex.-association-of-police-with-crime",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Ex. Association of police with crime",
    "text": "Ex. Association of police with crime\nYou can instruct tidy to return a 95 percent confidence interval for the association of campus police with campus crime:"
  },
  {
    "objectID": "lectures/05-Inference/054-conf-intervals.html#ex.-association-of-police-with-crime-1",
    "href": "lectures/05-Inference/054-conf-intervals.html#ex.-association-of-police-with-crime-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Ex. Association of police with crime",
    "text": "Ex. Association of police with crime\n\n\n\n\n\nFour confidence intervals for the same coefficient."
  }
]