[
  {
    "objectID": "lectures/index.html",
    "href": "lectures/index.html",
    "title": "Lectures",
    "section": "",
    "text": "Expand\n\n\n\n\n\n\n\n  \n\n\n   Expand\n\n\n\n\n\n  \n\n\n   Expand\n\n\n\n\n\n\n  \n\n\n   Expand\n\n\n\n\n\n  \n\n\n   Expand\n\n\n\n\n\n  \n\n\n   Expand\n\n\n\n\n\n  \n\n\n   Expand\n\n\n\n\n\n  \n\n\n   Expand"
  },
  {
    "objectID": "lectures/index.html#theory",
    "href": "lectures/index.html#theory",
    "title": "Lectures",
    "section": "",
    "text": "Expand\n\n\n\n\n\n\n\n  \n\n\n   Expand\n\n\n\n\n\n  \n\n\n   Expand\n\n\n\n\n\n\n  \n\n\n   Expand\n\n\n\n\n\n  \n\n\n   Expand\n\n\n\n\n\n  \n\n\n   Expand\n\n\n\n\n\n  \n\n\n   Expand\n\n\n\n\n\n  \n\n\n   Expand"
  },
  {
    "objectID": "lectures/index.html#learning-r",
    "href": "lectures/index.html#learning-r",
    "title": "Lectures",
    "section": "Learning R",
    "text": "Learning R\n\nVectors and Pipes\n\n  \n\n\n   Expand\n\n\n\nTibbles and lm\n\n  \n\n\n   Expand\n\n\n\nDplyr\n\n  \n\n\n   Expand\n\n\n\nggplot\n\n  \n\n\n   Expand\n\nDownload Dataset"
  },
  {
    "objectID": "lectures/12-ggplot/120-compile.html#preview",
    "href": "lectures/12-ggplot/120-compile.html#preview",
    "title": "Learning R: ggplot",
    "section": "Preview",
    "text": "Preview\nData manipulation and querying is a very useful skill.\nAn equally important one is being able to visualize the data.\nIn this lecture, you will learn a handful of recipes to draw plots using ggplot\nIn particular, you will learn about:\n\nBar plots\nHistograms\nBox plots (“Box-and-Whisker” plots)\nScatterplots (and how to add a best fit line)"
  },
  {
    "objectID": "lectures/12-ggplot/120-compile.html#but-first",
    "href": "lectures/12-ggplot/120-compile.html#but-first",
    "title": "Learning R: ggplot",
    "section": "But First…",
    "text": "But First…\nWe need some data.\nAs you’ll see, the type of plot you will want to draw depends crucially on the type of data that you have.\n\nSuppose we have a tibble called students that looks like this:\n\n\n\nsex\nstudy_time\ngrade1\nfinal_grade\n\n\n\n\nfemale\n5-10 hrs\n94.9\n95.4\n\n\nmale\n2-5 hrs\n79.6\n73.7\n\n\nfemale\n5-10 hrs\n64.2\n49.1\n\n\n. . .\n. . .\n. . .\n. . .\n\n\n\nWhere study_time is hours per week studying math, grade1 is their first-semester math grade, and final_grade is their final grade in the math course"
  },
  {
    "objectID": "lectures/12-ggplot/120-compile.html#but-first-1",
    "href": "lectures/12-ggplot/120-compile.html#but-first-1",
    "title": "Learning R: ggplot",
    "section": "But First…",
    "text": "But First…\n\n\n\nsex\nstudy_time\ngrade1\nfinal_grade\n\n\n\n\nfemale\n5-10 hrs\n94.9\n95.4\n\n\nmale\n2-5 hrs\n79.6\n73.7\n\n\nfemale\n5-10 hrs\n64.2\n49.1\n\n\n. . .\n. . .\n. . .\n. . .\n\n\n\nWe have some variables which are categorical (sex takes either “male” or “female” and study_time takes on 0-2 hrs, 2-5 hrs, etc.).\nWe have other variables which are numeric, and in particular, continuous (grade1 and final_grade can take any value between 0 and 100)\nThese being different “type” of variables means we use different “recipes” to visualize them"
  },
  {
    "objectID": "lectures/12-ggplot/120-compile.html#data-and-quick-ggplot-syntax",
    "href": "lectures/12-ggplot/120-compile.html#data-and-quick-ggplot-syntax",
    "title": "Learning R: ggplot",
    "section": "Data and quick ggplot() syntax",
    "text": "Data and quick ggplot() syntax\nI will be using the dataset you can find below the lecture, named students\nDownload it, open RStudio and in your terminal load it using:\n\nload(\"./students-data.Rdata\")\n\nThe exact path will change depending on where it is stored on your device. I recommend right-clicking on the file itself and copying the path shown in the properties.\n\nNote: This is not necessary for you to do. It’s just if you want to follow along"
  },
  {
    "objectID": "lectures/12-ggplot/120-compile.html#recipe-1-bar-plots",
    "href": "lectures/12-ggplot/120-compile.html#recipe-1-bar-plots",
    "title": "Learning R: ggplot",
    "section": "Recipe 1: Bar Plots",
    "text": "Recipe 1: Bar Plots\ngeom_bar()\nQ: How many hours do students spend studying?\n\nDistrubtion of a single discrete variable \\(\\rightarrow\\) Use a Bar Plot\n\n\nstudents %&gt;% \n    ggplot(aes(x = study_time)) + \n    geom_bar()\n\n\nHere I pipe the tibble into the function ggplot()\nggplot() needs an aesthetic mapping. You need to tell it which variables in your dataset map to which visual aesthetic in the plot.\nAfter the ggplot() call, add any extra layer with +\ngeom_bar() draws the bar plot using the previous instructions"
  },
  {
    "objectID": "lectures/12-ggplot/120-compile.html#recipe-2-histograms",
    "href": "lectures/12-ggplot/120-compile.html#recipe-2-histograms",
    "title": "Learning R: ggplot",
    "section": "Recipe 2: Histograms",
    "text": "Recipe 2: Histograms\ngeom_histogram()\nQ: What is the grade distribution?\n\nDistribution of a single continuous variable \\(\\rightarrow\\) use a histogram\n\n\nThe recipe remains largely the same: Pipe the data into ggplot(), then ggplot() needs an aesthetic mapping which is wrapped in aes().\nThen use the specific geom_histogram() to draw the histogram.\n\nstudents %&gt;%\n    ggplot(aes(x = final_grade)) + \n    geom_histogram()"
  },
  {
    "objectID": "lectures/12-ggplot/120-compile.html#recipe-3-box-plots",
    "href": "lectures/12-ggplot/120-compile.html#recipe-3-box-plots",
    "title": "Learning R: ggplot",
    "section": "Recipe 3: Box Plots",
    "text": "Recipe 3: Box Plots\ngeom_boxplot()\nQ. Do students who study more (discrete) earn higher grades (continuous)?\n\nBegin by piping the data into ggplot()\nProvide ggplot() the aesthetic wrapped in aes()\nThis time we have 2 variables: one for each axis\n\n\nstudents %&gt;%\n    ggplot(aes(x = study_time, y = final_grade)) + \n    geom_boxplot()"
  },
  {
    "objectID": "lectures/12-ggplot/120-compile.html#recipe-4-scatterplots",
    "href": "lectures/12-ggplot/120-compile.html#recipe-4-scatterplots",
    "title": "Learning R: ggplot",
    "section": "Recipe 4: Scatterplots",
    "text": "Recipe 4: Scatterplots\ngeom_point()\nQ. How well does a student’s first-semester grade predict their final grade in a (high school) class?\n\nstudents %&gt;%\n    ggplot(aes(x = grade1, y = final_grade)) + \n    geom_point()"
  },
  {
    "objectID": "lectures/12-ggplot/120-compile.html#recipe-5-bar-plots",
    "href": "lectures/12-ggplot/120-compile.html#recipe-5-bar-plots",
    "title": "Learning R: ggplot",
    "section": "Recipe 5: Bar Plots",
    "text": "Recipe 5: Bar Plots\ngeom_bar()\nQ. Do females report studying for longer than males?\nRelationship between time studied and sex\nWe can do this in two different ways:\n\n\nMake a bar plot for each category (sex) which creates separate plots\n\n\nstudents %&gt;% \n    ggplot(aes(x = study_time)) +\n    geom_bar() +\n    facet_wrap(~ sex) # ~ indicates a formula is coming"
  },
  {
    "objectID": "lectures/12-ggplot/120-compile.html#recipe-5-bar-plots-1",
    "href": "lectures/12-ggplot/120-compile.html#recipe-5-bar-plots-1",
    "title": "Learning R: ggplot",
    "section": "Recipe 5: Bar Plots",
    "text": "Recipe 5: Bar Plots\ngeom_bar()\nQ. Do females report studying for longer than males?\nRelationship between time studied and sex\nWe can do this in two different ways:\n\n\nTry a fill aesthetic mapping to color in the bars using seperate colors for males and females\n\n\nstudents %&gt;%\n    ggplot(aes(x = study_time, fill = sex)) + # Be sure to include the fill inside the aes()\n    geom_bar(position = 'dodge') # Use position = \"dodge\" to set bars next to each other. The default is to stack them on top of each other"
  },
  {
    "objectID": "lectures/12-ggplot/120-compile.html#a-few-more-tools",
    "href": "lectures/12-ggplot/120-compile.html#a-few-more-tools",
    "title": "Learning R: ggplot",
    "section": "A Few More Tools",
    "text": "A Few More Tools\nSay we want to show a scatterplot and have it differentiate amongst categories using different colors. It is very similar to fill =, but instead we use color =.\n\nlibrary(gapminder)\n\ngapminder %&gt;%\n    ggplot(aes(x = gdpPercap, y = lifeExp, color = continent)) + \n    geom_point()"
  },
  {
    "objectID": "lectures/12-ggplot/120-compile.html#a-few-more-tools-1",
    "href": "lectures/12-ggplot/120-compile.html#a-few-more-tools-1",
    "title": "Learning R: ggplot",
    "section": "A Few More Tools",
    "text": "A Few More Tools"
  },
  {
    "objectID": "lectures/12-ggplot/120-compile.html#summary",
    "href": "lectures/12-ggplot/120-compile.html#summary",
    "title": "Learning R: ggplot",
    "section": "Summary",
    "text": "Summary\n\nAesthetic mappings get wrapped in aes() and map variables in your tibble to aesthetics in your plot like which variable gets drawn on the x-axis, which goes on the y-axis, and which variable is represented in color\nGeoms are added to the plot using + as layers"
  },
  {
    "objectID": "lectures/12-ggplot/120-compile.html#practice-download-worksheet-04-from-the-site",
    "href": "lectures/12-ggplot/120-compile.html#practice-download-worksheet-04-from-the-site",
    "title": "Learning R: ggplot",
    "section": "Practice: Download “Worksheet 04” From the Site",
    "text": "Practice: Download “Worksheet 04” From the Site\nThis worksheet will help you learn coding by doing. You will:\n\nPractice aesthetic mappings and all the recipes you’ve seen in this section"
  },
  {
    "objectID": "lectures/11-Dplyr/110-compile.html#preview",
    "href": "lectures/11-Dplyr/110-compile.html#preview",
    "title": "Learning R: Dplyr",
    "section": "Preview",
    "text": "Preview\nSo now you know how to build a dataset and run a regresion.\nBut sometimes you want to ask your dataset questions, like:\n\nWhich student got the highest final grade?\nHow much, on average, do “A” students study?\nOut of all the students who finished the first semester with a “B”, how many of them turned it around to earn an “A” for their final grade?\n\n\nIn this lecture you will learn how to answer these questions and more using a toolset included in the tidyverse called dplyr. The name stands for data pliers and the idea is tha tyou should reach for your data pliers when you want to manipulate your data."
  },
  {
    "objectID": "lectures/11-Dplyr/110-compile.html#dplyr-functions",
    "href": "lectures/11-Dplyr/110-compile.html#dplyr-functions",
    "title": "Learning R: Dplyr",
    "section": "Dplyr Functions",
    "text": "Dplyr Functions\ndplyr is an amazing tool because, with just 7 main functions, you can answer just about any question you could possibly have about your data\n\n\nIf you have any data science experience, you will quickly realize that dplyr is nothing new.\nIt is actually just SQL, a language for querying databases that has been around since the ’70s.\nWe will not be write any SQL but you should know that by learning dplyr, you are also learning SQL.\nThe graphics for the functions were created by Colleen O’Briant"
  },
  {
    "objectID": "lectures/11-Dplyr/110-compile.html#practice-download-worksheet-03-from-the-site",
    "href": "lectures/11-Dplyr/110-compile.html#practice-download-worksheet-03-from-the-site",
    "title": "Learning R: Dplyr",
    "section": "Practice: Download “Worksheet 03” From the Site",
    "text": "Practice: Download “Worksheet 03” From the Site\nThis worksheet will help you learn coding by doing. You will:\n\nPractice how to do use all of these functions"
  },
  {
    "objectID": "lectures/10-Tibbles-and-lm/101-tibbles.html",
    "href": "lectures/10-Tibbles-and-lm/101-tibbles.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "They are tidyverse spreadsheets\nData is still being held in vectors (column vectors specifically), but the rows of a tibble also hold meaning.\nThe rows are the observations while the columns are the variables\nLet’s look at an example"
  },
  {
    "objectID": "lectures/10-Tibbles-and-lm/101-tibbles.html#what-is-a-tibble",
    "href": "lectures/10-Tibbles-and-lm/101-tibbles.html#what-is-a-tibble",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "They are tidyverse spreadsheets\nData is still being held in vectors (column vectors specifically), but the rows of a tibble also hold meaning.\nThe rows are the observations while the columns are the variables\nLet’s look at an example"
  },
  {
    "objectID": "lectures/10-Tibbles-and-lm/101-tibbles.html#ex-daily-weather",
    "href": "lectures/10-Tibbles-and-lm/101-tibbles.html#ex-daily-weather",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Ex Daily Weather",
    "text": "Ex Daily Weather\nLet’s write down each day’s high temp, low temp, and rainfall. In words the data is:\n\nJan 01, 2023: We had a high of \\(46^{\\circ}\\), a low of \\(37^{\\circ}\\) and \\(0.07\\) in. of rain\nJan 02, 2023: We had a high of \\(46^{\\circ}\\), a low of \\(35^{\\circ}\\) and \\(0.00\\) in. of rain\nJan 03, 2023: We had a high of \\(47^{\\circ}\\), a low of \\(34^{\\circ}\\) and \\(0.08\\) in. of rain\n\n\n\nWhat should the observations (rows) be?\n\nEach day we went outside and observed the weather. So each day should have its own row.\n\nWhat are the variables (columns) we observe?\n\nThe date, the high temp, the low temp, rainfall"
  },
  {
    "objectID": "lectures/10-Tibbles-and-lm/101-tibbles.html#ex.-daily-weather",
    "href": "lectures/10-Tibbles-and-lm/101-tibbles.html#ex.-daily-weather",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Ex. Daily Weather",
    "text": "Ex. Daily Weather\nSo we want our tibble to look like:\n\n\n\nDate\nHigh Temp\nLow Temp\nRainfall\n\n\n\n\n1/1/23\n46\n37\n0.07\n\n\n1/2/23\n46\n35\n0.00\n\n\n1/3/23\n47\n34\n0.08\n\n\n\n\nThe mantra “observations as rows, variables as columns” is what we call the tidied data format\nThere are tons of ways you could format your data, but the tidyverse is compatible with only this way.\nLuckily, it turns out to be very experssible"
  },
  {
    "objectID": "lectures/10-Tibbles-and-lm/101-tibbles.html#ex.-daily-weather-1",
    "href": "lectures/10-Tibbles-and-lm/101-tibbles.html#ex.-daily-weather-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Ex. Daily Weather",
    "text": "Ex. Daily Weather\nHere’s the code to construct our tibble:\n\ntibble(\n    date = as.Date(c(\"2023-01-01\", \"2023-01-02\", \"2023-01-03\")),\n    high_temp = c(46, 46, 47),\n    low_temp = c(37, 35, 36),\n    rainfall = c(0.07, 0.00, 0.08)\n)\n\n# A tibble: 3 × 4\n  date       high_temp low_temp rainfall\n  &lt;date&gt;         &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 2023-01-01        46       37     0.07\n2 2023-01-02        46       35     0   \n3 2023-01-03        47       36     0.08\n\n\nHow it works:\n\nUse the function tibble()\ntibble() takes a list of vectors created with c() that become variable columns\nEach varible column has a name"
  },
  {
    "objectID": "lectures/10-Tibbles-and-lm/101-tibbles.html#rules-for-tibbles",
    "href": "lectures/10-Tibbles-and-lm/101-tibbles.html#rules-for-tibbles",
    "title": "EC 320 - Intro. Econometrics",
    "section": "2 Rules for Tibbles",
    "text": "2 Rules for Tibbles\n1. Each Column Must be Named\n\nIf you try to define a column without giving it a name, tibble() generatses one for you (try it out yourself)\nWhen naming variables, avoid spaces between words\n\n2. Each Column Must Have the Same Number of Rows\n\nIf you try to define a column that is shorter than the others, tibble() will throw an error.\n\nException: If you define a column with only one element, tibble() will repeat it to make it the same length as the other columns."
  },
  {
    "objectID": "lectures/10-Tibbles-and-lm/101-tibbles.html#data-types",
    "href": "lectures/10-Tibbles-and-lm/101-tibbles.html#data-types",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Data Types",
    "text": "Data Types\n\nThere are 3 types of data that you will come across\n\nCross-sectional\nTime Series\nPanel\n\nImportantly, we cannot model different data types the same way.\nIn this class we build models of exclusively cross-sectional data."
  },
  {
    "objectID": "lectures/10-Tibbles-and-lm/101-tibbles.html#cross-sectional-data",
    "href": "lectures/10-Tibbles-and-lm/101-tibbles.html#cross-sectional-data",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Cross-Sectional Data",
    "text": "Cross-Sectional Data\n\nCross-sectional Data is data about many individuals at (around) the same time. Here, “individuals” might refer to people, or households, companies, cities, states, countries, etc.\n\n\ntibble(\n    name = c(\"Kris\", \"Kourtney\", \"Kim\"),\n    study_time = c(\"&lt; 2hrs\", \"2-5hrs\", \"&lt; 2hrs\"),\n    final_grade = c(69.4, 89.7, 66.3)\n)\n\n# A tibble: 3 × 3\n  name     study_time final_grade\n  &lt;chr&gt;    &lt;chr&gt;            &lt;dbl&gt;\n1 Kris     &lt; 2hrs            69.4\n2 Kourtney 2-5hrs            89.7\n3 Kim      &lt; 2hrs            66.3\n\n\nThe name “cross-sectional” comes from the fact that the data is a cross-section of some population. It is a snapshot in time of a smaple of individuals."
  },
  {
    "objectID": "lectures/10-Tibbles-and-lm/101-tibbles.html#time-series-data",
    "href": "lectures/10-Tibbles-and-lm/101-tibbles.html#time-series-data",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Time Series Data",
    "text": "Time Series Data\n\nTime Series Data is data that follows one specific individual (or household, company, city, state, country, etc.) over time. So this dataset would be a time series if it reports quiz grades over the course of a term for one student:\n\n\ntibble(\n    assignment = c(\"Quiz 01\", \"Quiz 02\", \"Quiz 03\", \"Quiz 04\", \"Quiz 05\"),\n    study_hours = c(4,3,2,3,8),\n    grade = c(75, 74, 69, 77, 89)\n)\n\n# A tibble: 5 × 3\n  assignment study_hours grade\n  &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt;\n1 Quiz 01              4    75\n2 Quiz 02              3    74\n3 Quiz 03              2    69\n4 Quiz 04              3    77\n5 Quiz 05              8    89"
  },
  {
    "objectID": "lectures/10-Tibbles-and-lm/101-tibbles.html#panel-data",
    "href": "lectures/10-Tibbles-and-lm/101-tibbles.html#panel-data",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Panel Data",
    "text": "Panel Data\nPanel Data describes many individuals over many periods of time. For example, many students’ scores over the course of a term would be panel data\n\ntibble(\n    name = rep(c(\"Kris\", \"Kourtney\", \"Kim\"), each = 3),\n    assignment = rep(c(\"Quiz 01\", \"Quiz 02\", \"Quiz 03\"), times = 3),\n    grade = c(75, 78, 66, 95, 97, 90, 62, 66, 54)\n)\n\n# A tibble: 9 × 3\n  name     assignment grade\n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt;\n1 Kris     Quiz 01       75\n2 Kris     Quiz 02       78\n3 Kris     Quiz 03       66\n4 Kourtney Quiz 01       95\n5 Kourtney Quiz 02       97\n6 Kourtney Quiz 03       90\n7 Kim      Quiz 01       62\n8 Kim      Quiz 02       66\n9 Kim      Quiz 03       54"
  },
  {
    "objectID": "lectures/09-Fundamental-Prob-Causal-Inf/093-rct.html",
    "href": "lectures/09-Fundamental-Prob-Causal-Inf/093-rct.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Problem: Selection bias precludes all else equal comparisons.\n\nTo make causal statements, we need to shut down the bias term.\n\n. . .\nPotential solution: Conduct an experiment.\n\nHow? Random assignment of treatment\nHence the name, randomized control trial (RCT).\n\n. . .\nGroups will need to be large enough\n\nFollowing the LLN, as we increase \\(n\\) of both groups, our randomly assigned treatment estimate is more likely to be representative of the population mean."
  },
  {
    "objectID": "lectures/09-Fundamental-Prob-Causal-Inf/093-rct.html#selection-bias",
    "href": "lectures/09-Fundamental-Prob-Causal-Inf/093-rct.html#selection-bias",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Problem: Selection bias precludes all else equal comparisons.\n\nTo make causal statements, we need to shut down the bias term.\n\n. . .\nPotential solution: Conduct an experiment.\n\nHow? Random assignment of treatment\nHence the name, randomized control trial (RCT).\n\n. . .\nGroups will need to be large enough\n\nFollowing the LLN, as we increase \\(n\\) of both groups, our randomly assigned treatment estimate is more likely to be representative of the population mean."
  },
  {
    "objectID": "lectures/09-Fundamental-Prob-Causal-Inf/093-rct.html#randomized-control-trials",
    "href": "lectures/09-Fundamental-Prob-Causal-Inf/093-rct.html#randomized-control-trials",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Randomized control trials",
    "text": "Randomized control trials\nEx. Effect of de-worming on attendance\n\nMotivation: Intestinal worms are common among children in less-developed countries. The parasitic symptoms disrupt human capital acquisition by keeping children home.\n\nPolicy Question: Do school-based de-worming interventions provide a cost-effective way to increase school attendance?"
  },
  {
    "objectID": "lectures/09-Fundamental-Prob-Causal-Inf/093-rct.html#randomized-control-trials-1",
    "href": "lectures/09-Fundamental-Prob-Causal-Inf/093-rct.html#randomized-control-trials-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Randomized control trials",
    "text": "Randomized control trials\nEx. Effect of de-worming on attendance\n\nResearch Question: How much do de-worming interventions increase school attendance?\nQ: Could we simply compare average attendance among children with and without access to de-worming medication?\n. . .\nA: If we’re after the causal effect, probably not."
  },
  {
    "objectID": "lectures/09-Fundamental-Prob-Causal-Inf/093-rct.html#randomized-control-trials-2",
    "href": "lectures/09-Fundamental-Prob-Causal-Inf/093-rct.html#randomized-control-trials-2",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Randomized control trials",
    "text": "Randomized control trials\nEx. Effect of de-worming on attendance\n\nResearch Question: How much do de-worming interventions increase school attendance?\nQ: Why not?\n. . .\nA: Selection bias – Families with access to de-worming medication probably have healthier children for other reasons (e.g. wealth, access to clean drinking water).1"
  },
  {
    "objectID": "lectures/09-Fundamental-Prob-Causal-Inf/093-rct.html#randomized-control-trials-3",
    "href": "lectures/09-Fundamental-Prob-Causal-Inf/093-rct.html#randomized-control-trials-3",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Randomized Control Trials",
    "text": "Randomized Control Trials\nEx. Effect of de-worming on attendance\n. . .\nImagine an RCT where we have two groups of villages:\n\nTreatment: Where children get de-worming medication in school.\nControl: Where children don’t get de-worming medication in school (status quo).\n\n. . .\nBy randomizing, we will, on average, include all kinds of villages in both groups\n\npoor vs. less poor\naccess to clean water vs. contaminated water\nhospital vs. no hospital"
  },
  {
    "objectID": "lectures/09-Fundamental-Prob-Causal-Inf/093-rct.html#section",
    "href": "lectures/09-Fundamental-Prob-Causal-Inf/093-rct.html#section",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "54 villages"
  },
  {
    "objectID": "lectures/09-Fundamental-Prob-Causal-Inf/093-rct.html#section-1",
    "href": "lectures/09-Fundamental-Prob-Causal-Inf/093-rct.html#section-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "54 villages of varying levels of development"
  },
  {
    "objectID": "lectures/09-Fundamental-Prob-Causal-Inf/093-rct.html#section-2",
    "href": "lectures/09-Fundamental-Prob-Causal-Inf/093-rct.html#section-2",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "54 villages of varying levels of development plus randomly assigned treatment"
  },
  {
    "objectID": "lectures/09-Fundamental-Prob-Causal-Inf/093-rct.html#section-3",
    "href": "lectures/09-Fundamental-Prob-Causal-Inf/093-rct.html#section-3",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "54 villages of varying levels of development plus randomly assigned treatment"
  },
  {
    "objectID": "lectures/09-Fundamental-Prob-Causal-Inf/093-rct.html#section-4",
    "href": "lectures/09-Fundamental-Prob-Causal-Inf/093-rct.html#section-4",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "54 villages of varying levels of development plus randomly assigned treatment"
  },
  {
    "objectID": "lectures/09-Fundamental-Prob-Causal-Inf/093-rct.html#section-5",
    "href": "lectures/09-Fundamental-Prob-Causal-Inf/093-rct.html#section-5",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "54 villages of varying levels of development plus randomly assigned treatment"
  },
  {
    "objectID": "lectures/09-Fundamental-Prob-Causal-Inf/093-rct.html#section-6",
    "href": "lectures/09-Fundamental-Prob-Causal-Inf/093-rct.html#section-6",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "54 villages of varying levels of development plus randomly assigned treatment"
  },
  {
    "objectID": "lectures/09-Fundamental-Prob-Causal-Inf/093-rct.html#randomized-control-trials-4",
    "href": "lectures/09-Fundamental-Prob-Causal-Inf/093-rct.html#randomized-control-trials-4",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Randomized Control Trials",
    "text": "Randomized Control Trials\nEx. Effect of de-worming on attendance\nWe can estimate the causal effect of de-worming on school attendance by comparing the average attendance rates in the treatment group (💊) with those in the control group (no 💊).\n\\[\n\\begin{align}\n  \\overline{\\text{Attendance}}_\\text{Treatment} - \\overline{\\text{Attendance}}_\\text{Control}\n\\end{align}\n\\]\n. . .\nAlternatively, we can use the regression\n. . .\n\\[\n\\begin{align}\n  \\text{Attendance}_i = \\beta_0 + \\beta_1 \\text{Treatment}_i + u_i \\tag{1}\n\\end{align}\n\\]\nwhere \\(\\text{Treatment}_i\\) is a binary variable (=1 if village \\(i\\) received the de-worming treatment)."
  },
  {
    "objectID": "lectures/09-Fundamental-Prob-Causal-Inf/093-rct.html#randomized-control-trials-5",
    "href": "lectures/09-Fundamental-Prob-Causal-Inf/093-rct.html#randomized-control-trials-5",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Randomized Control Trials",
    "text": "Randomized Control Trials\nEx. Effect of de-worming on attendance\n\\[\n\\begin{align}\n  \\text{Attendance}_i = \\beta_0 + \\beta_1 \\text{Treatment}_i + u_i \\tag{1}\n\\end{align}\n\\]\nwhere \\(\\text{Treatment}_i\\) is a binary variable (=1 if village \\(i\\) received the de-worming treatment).\nQ: Should trust the results of Eq. \\((1)\\)? Why?\n. . .\nA: On average, randomly assigning treatment should balance treatment and control across the other dimensions that affect school attendance.\nBut we must always be cautious\n\nRandomization can go wrong!"
  },
  {
    "objectID": "lectures/09-Fundamental-Prob-Causal-Inf/093-rct.html#footnotes",
    "href": "lectures/09-Fundamental-Prob-Causal-Inf/093-rct.html#footnotes",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCan’t make an all else equal comparison. Biased and/or spurious results.↩︎"
  },
  {
    "objectID": "lectures/09-Fundamental-Prob-Causal-Inf/091-prologue.html",
    "href": "lectures/09-Fundamental-Prob-Causal-Inf/091-prologue.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Policy: In 2017, the University of Oregon started requiring first-year students to live on campus.\n. . .\nRationale: First-year students who live on campus fare better:\n\n80 percent more likely to graduate in four years.\nSecond-year retention rate 5 percentage points higher.\nGPAs 0.13 points higher, on average.\n\n. . .\nDo these comparisons suggest the policy improves student outcomes?\n. . .\nDo they describe the effect of living on campus?\n. . .\nDo they describe something else?"
  },
  {
    "objectID": "lectures/09-Fundamental-Prob-Causal-Inf/091-prologue.html#statistics-inform-policy",
    "href": "lectures/09-Fundamental-Prob-Causal-Inf/091-prologue.html#statistics-inform-policy",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Policy: In 2017, the University of Oregon started requiring first-year students to live on campus.\n. . .\nRationale: First-year students who live on campus fare better:\n\n80 percent more likely to graduate in four years.\nSecond-year retention rate 5 percentage points higher.\nGPAs 0.13 points higher, on average.\n\n. . .\nDo these comparisons suggest the policy improves student outcomes?\n. . .\nDo they describe the effect of living on campus?\n. . .\nDo they describe something else?"
  },
  {
    "objectID": "lectures/09-Fundamental-Prob-Causal-Inf/091-prologue.html#other-things-equal",
    "href": "lectures/09-Fundamental-Prob-Causal-Inf/091-prologue.html#other-things-equal",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Other Things Equal",
    "text": "Other Things Equal\nThe UO’s interpretation of those comparisons warrants skepticism.\n\nThe decision to live on campus is probably related to family wealth and interest in school.\nFamily wealth and interest in school are related to academic achievement.\n\n. . .\n\nWhy might I be worried?\n\nThe difference in outcomes between those on and off campus is not an all else equal comparison.\n\n\nUpshot: Can’t attribute the difference in outcomes solely to living on campus.\n\nLiving on campus (D) increases student welfare (Y)\n\n\n\n\n\n\nLiving on campus (D) increases student welfare (Y)\n\n\n\n\n\nBut parental income (W) impacts both student welfare (Y) and living on campus (D). Failing to account for parental income will bias comparisons"
  },
  {
    "objectID": "lectures/09-Fundamental-Prob-Causal-Inf/091-prologue.html#all-else-equal",
    "href": "lectures/09-Fundamental-Prob-Causal-Inf/091-prologue.html#all-else-equal",
    "title": "EC 320 - Intro. Econometrics",
    "section": "All else equal",
    "text": "All else equal\n\nCeteris paribus, all else held constant, etc.\n\n\n. . .\nA high bar\nWhen all else equal, statistical comparisons detect causal relationships.\n(Micro)economics has developed a comparative advantage in understanding where all else equal comparisons can and cannot be made.\n\nAnyone can retort “correlation doesn’t necessarily imply causation.”\nUnderstanding why is difficult, but useful for learning from data."
  },
  {
    "objectID": "lectures/08-Categorical-Variables/083-interactions.html",
    "href": "lectures/08-Categorical-Variables/083-interactions.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Regression coefficients describe average effects. But for whom does on average mean?\n\n. . .\nAverages can mask heterogeneous effects that differ by group or by the level of another variable.\n\n. . .\nWe can use interaction terms to model heterogeneous effects, accommodating complexity and nuance by going beyond “the effect of \\(X\\) on \\(Y\\) is \\(\\beta_1\\).”"
  },
  {
    "objectID": "lectures/08-Categorical-Variables/083-interactions.html#motivation",
    "href": "lectures/08-Categorical-Variables/083-interactions.html#motivation",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Regression coefficients describe average effects. But for whom does on average mean?\n\n. . .\nAverages can mask heterogeneous effects that differ by group or by the level of another variable.\n\n. . .\nWe can use interaction terms to model heterogeneous effects, accommodating complexity and nuance by going beyond “the effect of \\(X\\) on \\(Y\\) is \\(\\beta_1\\).”"
  },
  {
    "objectID": "lectures/08-Categorical-Variables/083-interactions.html#interaction-terms",
    "href": "lectures/08-Categorical-Variables/083-interactions.html#interaction-terms",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Interaction Terms",
    "text": "Interaction Terms\nStarting point: \\(Y_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + u_i\\)\n\n\n\n\\(X_{1i}\\) is the variable of interest\n\n\n\n\\(X_{2i}\\) is a control variable\n\n\n\n. . .\nA richer model: Interactions test whether \\(X_{2i}\\) moderates the effect of \\(X_{1i}\\)\n\\[\nY_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{1i} \\cdot X_{2i} + u_i\n\\]\n. . .\nInterpretation: The partial derivative of \\(Y_i\\) with respect to \\(X_{1i}\\) is the marginal effect of \\(X_1\\) on \\(Y_i\\):\n\\[\n\\color{#81A1C1}{\\dfrac{\\partial Y}{\\partial X_1} = \\beta_1 + \\beta_3 X_{2i}}\n\\]\n. . .\nThe effect of \\(X_1\\) depends on the level of \\(X_2\\) 🤯"
  },
  {
    "objectID": "lectures/08-Categorical-Variables/081-categorical-vars.html",
    "href": "lectures/08-Categorical-Variables/081-categorical-vars.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Goal Make quantitative statements about qualitative information.\n\ne.g., race, gender, being employed, living in Oregon, etc.\n\n. . .\nApproach. Construct binary variables.\n\na.k.a. dummy variables or indicator variables.\nValue equals 1 if observation is in the category or 0 if otherwise.\n\n. . .\nRegression implications.\n\nChange the interpretation of the intercept.\nChange the interpretations of the slope parameters."
  },
  {
    "objectID": "lectures/08-Categorical-Variables/081-categorical-vars.html#categorical-variables",
    "href": "lectures/08-Categorical-Variables/081-categorical-vars.html#categorical-variables",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Goal Make quantitative statements about qualitative information.\n\ne.g., race, gender, being employed, living in Oregon, etc.\n\n. . .\nApproach. Construct binary variables.\n\na.k.a. dummy variables or indicator variables.\nValue equals 1 if observation is in the category or 0 if otherwise.\n\n. . .\nRegression implications.\n\nChange the interpretation of the intercept.\nChange the interpretations of the slope parameters."
  },
  {
    "objectID": "lectures/08-Categorical-Variables/081-categorical-vars.html#continuous-variables",
    "href": "lectures/08-Categorical-Variables/081-categorical-vars.html#continuous-variables",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Continuous Variables",
    "text": "Continuous Variables\nConsider the relationship\n\\[\n\\text{Pay}_i = \\beta_0 + \\beta_1 \\text{School}_i + u_i\n\\]\nwhere\n\n\\(\\text{Pay}_i\\) is a continuous variable measuring an individual’s pay\n\\(\\text{School}_i\\) is a continuous variable that measures years of education\n\n. . .\nInterpretation\n\n\\(\\beta_0\\): \\(y\\)-intercept, i.e., \\(\\text{Pay}\\) when \\(\\text{School} = 0\\)\n\\(\\beta_1\\): expected increase in \\(\\text{Pay}\\) for a one-unit increase in \\(\\text{School}\\)\n\n\nConsider the relationship\n\\[\n\\text{Pay}_i = \\beta_0 + \\beta_1 \\text{School}_i + u_i\n\\]\nDerive the slope’s interpretation.\n\\(\\mathop{\\mathbb{E}}\\left[ \\text{Pay} | \\text{School} = \\ell + 1 \\right] - \\mathop{\\mathbb{E}}\\left[ \\text{Pay} | \\text{School} = \\ell \\right]\\)\n. . .\n \\(\\quad = \\mathop{\\mathbb{E}}\\left[ \\beta_0 + \\beta_1 (\\ell + 1) + u \\right] - \\mathop{\\mathbb{E}}\\left[ \\beta_0 + \\beta_1 \\ell + u \\right]\\)\n. . .\n \\(\\quad = \\left[ \\beta_0 + \\beta_1 (\\ell + 1) \\right] - \\left[ \\beta_0 + \\beta_1 \\ell \\right]\\)\n. . .\n \\(\\quad = \\beta_0 - \\beta_0 + \\beta_1 \\ell - \\beta_1 \\ell + \\beta_1\\) \\(\\: = \\beta_1\\).\nExpected increase in pay for an additional year of schooling"
  },
  {
    "objectID": "lectures/08-Categorical-Variables/081-categorical-vars.html#continuous-variables-1",
    "href": "lectures/08-Categorical-Variables/081-categorical-vars.html#continuous-variables-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Continuous Variables",
    "text": "Continuous Variables\nConsider the relationship\n\\[\n\\text{Pay}_i = \\beta_0 + \\beta_1 \\text{School}_i + u_i\n\\]\nAlternative derivation:\nDifferentiate the model with respect to schooling:\n\\[\n\\dfrac{\\partial \\text{Pay}}{\\partial \\text{School}} = \\beta_1\n\\]\nExpected increase in pay for an additional year of schooling\n\nIf we have multiple explanatory variables, e.g.,\n\\[\n\\text{Pay}_i = \\beta_0 + \\beta_1 \\text{School}_i + \\beta_2 \\text{Ability}_i + u_i\n\\]\nthen the interpretation changes slightly.\n. . .\n\\(\\mathop{\\mathbb{E}}\\left[ \\text{Pay} | \\text{School} = \\ell + 1 \\land \\text{Ability} = \\alpha \\right] - \\mathop{\\mathbb{E}}\\left[ \\text{Pay} | \\text{School} = \\ell \\land \\text{Ability} = \\alpha \\right]\\)\n. . .\n \\(\\quad = \\mathop{\\mathbb{E}}\\left[ \\beta_0 + \\beta_1 (\\ell + 1) + \\beta_2 \\alpha + u \\right] - \\mathop{\\mathbb{E}}\\left[ \\beta_0 + \\beta_1 \\ell + \\beta_2 \\alpha + u \\right]\\)\n. . .\n \\(\\quad = \\left[ \\beta_0 + \\beta_1 (\\ell + 1) + \\beta_2 \\alpha \\right] - \\left[ \\beta_0 + \\beta_1 \\ell + \\beta_2 \\alpha \\right]\\)\n. . .\n \\(\\quad = \\beta_0 - \\beta_0 + \\beta_1 \\ell - \\beta_1 \\ell + \\beta_1 + \\beta_2 \\alpha - \\beta_2 \\alpha\\) \\(\\: = \\beta_1\\)\n. . .\nThe slope gives the expected increase in pay for an additional year of schooling, holding ability constant."
  },
  {
    "objectID": "lectures/08-Categorical-Variables/081-categorical-vars.html#continuous-variables-2",
    "href": "lectures/08-Categorical-Variables/081-categorical-vars.html#continuous-variables-2",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Continuous Variables",
    "text": "Continuous Variables\nIf we have multiple explanatory variables, e.g.,\n\\[\n\\text{Pay}_i = \\beta_0 + \\beta_1 \\text{School}_i + \\beta_2 \\text{Ability}_i + u_i\n\\]\nthen the interpretation changes slightly.\n. . .\nAlternative derivation\nDifferentiate the model with respect to schooling:\n\\[\n\\dfrac{\\partial\\text{Pay}}{\\partial\\text{School}} = \\beta_1\n\\]\nThe slope gives the expected increase in pay for an additional year of schooling, holding ability constant."
  },
  {
    "objectID": "lectures/08-Categorical-Variables/081-categorical-vars.html#categorical-variables-1",
    "href": "lectures/08-Categorical-Variables/081-categorical-vars.html#categorical-variables-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Categorical Variables",
    "text": "Categorical Variables\nConsider the relationship\n\\[\n\\text{Pay}_i = \\beta_0 + \\beta_1 \\text{Female}_i + u_i\n\\]\nwhere \\(\\text{Pay}_i\\) is a continuous variable measuring an individual’s pay and \\(\\text{Female}_i\\) is a binary variable equal to \\(1\\) when \\(i\\) is female.\n. . .\nInterpretation of \\(\\beta_0\\)\n\\(\\beta_0\\) is the expected \\(\\text{Pay}\\) for males (i.e., when \\(\\text{Female} = 0\\)):\n. . .\n\\[\n\\mathop{\\mathbb{E}}\\left[ \\text{Pay} | \\text{Male} \\right] = \\mathop{\\mathbb{E}}\\left[ \\beta_0 + \\beta_1\\times 0 + u_i \\right] = \\mathop{\\mathbb{E}}\\left[ \\beta_0 + 0 + u_i \\right] = \\beta_0\n\\]"
  },
  {
    "objectID": "lectures/08-Categorical-Variables/081-categorical-vars.html#categorical-variables-2",
    "href": "lectures/08-Categorical-Variables/081-categorical-vars.html#categorical-variables-2",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Categorical Variables",
    "text": "Categorical Variables\nConsider the relationship\n\\[\n\\text{Pay}_i = \\beta_0 + \\beta_1 \\text{Female}_i + u_i\n\\]\nwhere \\(\\text{Pay}_i\\) is a continuous variable measuring an individual’s pay and \\(\\text{Female}_i\\) is a binary variable equal to \\(1\\) when \\(i\\) is female.\n. . .\nInterpretation of \\(\\beta_1\\)\n\\(\\beta_1\\) is the expected difference in \\(\\text{Pay}\\) between females and males:\n. . .\n\\(\\mathop{\\mathbb{E}}\\left[ \\text{Pay} | \\text{Female} \\right] - \\mathop{\\mathbb{E}}\\left[ \\text{Pay} | \\text{Male} \\right]\\)\n. . .\n\\(\\quad = \\mathop{\\mathbb{E}}\\left[ \\beta_0 + \\beta_1\\times 1 + u_i \\right] - \\mathop{\\mathbb{E}}\\left[ \\beta_0 + \\beta_1\\times 0 + u_i \\right]\\)\n. . .\n\\(\\quad = \\mathop{\\mathbb{E}}\\left[ \\beta_0 + \\beta_1 + u_i \\right] - \\mathop{\\mathbb{E}}\\left[ \\beta_0 + 0 + u_i \\right]\\)\n. . .\n\\(\\quad = \\beta_0 + \\beta_1 - \\beta_0\\) \\(\\quad = \\beta_1\\)"
  },
  {
    "objectID": "lectures/08-Categorical-Variables/081-categorical-vars.html#categorical-variables-3",
    "href": "lectures/08-Categorical-Variables/081-categorical-vars.html#categorical-variables-3",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Categorical Variables",
    "text": "Categorical Variables\nConsider the relationship\n\\[\n\\text{Pay}_i = \\beta_0 + \\beta_1 \\text{Female}_i + u_i\n\\]\nwhere \\(\\text{Pay}_i\\) is a continuous variable measuring an individual’s pay and \\(\\text{Female}_i\\) is a binary variable equal to \\(1\\) when \\(i\\) is female.\nInterpretation\n\\(\\beta_0 + \\beta_1\\): is the expected \\(\\text{Pay}\\) for females:\n\\(\\mathop{\\mathbb{E}}\\left[ \\text{Pay} | \\text{Female} \\right]\\)\n. . .\n\\(\\quad = \\mathop{\\mathbb{E}}\\left[ \\beta_0 + \\beta_1\\times 1 + u_i \\right]\\)\n. . .\n\\(\\quad = \\mathop{\\mathbb{E}}\\left[ \\beta_0 + \\beta_1 + u_i \\right]\\)\n. . .\n\\(\\quad = \\beta_0 + \\beta_1\\)"
  },
  {
    "objectID": "lectures/08-Categorical-Variables/081-categorical-vars.html#categorical-variables-4",
    "href": "lectures/08-Categorical-Variables/081-categorical-vars.html#categorical-variables-4",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Categorical Variables",
    "text": "Categorical Variables\nConsider the relationship\n\\[\n\\text{Pay}_i = \\beta_0 + \\beta_1 \\text{Female}_i + u_i\n\\]\nInterpretation\n\n\\(\\beta_0\\): expected \\(\\text{Pay}\\) for males (i.e., when \\(\\text{Female} = 0\\))\n\\(\\beta_1\\): expected difference in \\(\\text{Pay}\\) between females and males\n\\(\\beta_0 + \\beta_1\\): expected \\(\\text{Pay}\\) for females\nMales are the reference group"
  },
  {
    "objectID": "lectures/08-Categorical-Variables/081-categorical-vars.html#categorical-variables-5",
    "href": "lectures/08-Categorical-Variables/081-categorical-vars.html#categorical-variables-5",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Categorical Variables",
    "text": "Categorical Variables\nConsider the relationship\n\\[\n\\text{Pay}_i = \\beta_0 + \\beta_1 \\text{Female}_i + u_i\n\\]\nNote. If there are no other variables to condition on, then \\(\\hat{\\beta}_1\\) equals the difference in group means, e.g., \\(\\bar{X}_\\text{Female} - \\bar{X}_\\text{Male}\\).\n\n. . .\nNote2. The holding all other variables constant interpretation also applies for categorical variables in multiple regression settings."
  },
  {
    "objectID": "lectures/08-Categorical-Variables/081-categorical-vars.html#categorical-variables-6",
    "href": "lectures/08-Categorical-Variables/081-categorical-vars.html#categorical-variables-6",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Categorical Variables",
    "text": "Categorical Variables\n\\(Y_i = \\beta_0 + \\beta_1 X_i + u_i\\) for binary variable \\(X_i = \\{\\color{#434C5E}{0}, \\, {\\color{#B48EAD}{1}}\\}\\)"
  },
  {
    "objectID": "lectures/08-Categorical-Variables/081-categorical-vars.html#categorical-variables-7",
    "href": "lectures/08-Categorical-Variables/081-categorical-vars.html#categorical-variables-7",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Categorical Variables",
    "text": "Categorical Variables\n\\(Y_i = \\beta_0 + \\beta_1 X_i + u_i\\) for binary variable \\(X_i = \\{\\color{#434C5E}{0}, \\, {\\color{#B48EAD}{1}}\\}\\)"
  },
  {
    "objectID": "lectures/08-Categorical-Variables/081-categorical-vars.html#categorical-variables-8",
    "href": "lectures/08-Categorical-Variables/081-categorical-vars.html#categorical-variables-8",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Categorical Variables",
    "text": "Categorical Variables\n\\(Y_i = \\beta_0 + \\beta_1 X_i + u_i\\) for binary variable \\(X_i = \\{\\color{#434C5E}{0}, \\, {\\color{#B48EAD}{1}}\\}\\)"
  },
  {
    "objectID": "lectures/08-Categorical-Variables/081-categorical-vars.html#multiple-regression",
    "href": "lectures/08-Categorical-Variables/081-categorical-vars.html#multiple-regression",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Multiple Regression",
    "text": "Multiple Regression\n\\(Y_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + u_i \\quad\\) \\(X_1\\) is continuous \\(\\quad X_2\\) is categorical"
  },
  {
    "objectID": "lectures/08-Categorical-Variables/081-categorical-vars.html#multiple-regression-1",
    "href": "lectures/08-Categorical-Variables/081-categorical-vars.html#multiple-regression-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Multiple Regression",
    "text": "Multiple Regression\nThe intercept and categorical variable \\(X_2\\) control for the groups’ means."
  },
  {
    "objectID": "lectures/08-Categorical-Variables/081-categorical-vars.html#multiple-regression-2",
    "href": "lectures/08-Categorical-Variables/081-categorical-vars.html#multiple-regression-2",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Multiple Regression",
    "text": "Multiple Regression\nWith groups’ means removed"
  },
  {
    "objectID": "lectures/08-Categorical-Variables/081-categorical-vars.html#multiple-regression-3",
    "href": "lectures/08-Categorical-Variables/081-categorical-vars.html#multiple-regression-3",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Multiple Regression",
    "text": "Multiple Regression\n\\(\\hat{\\beta}_1\\) estimates the relationship between \\(Y\\) and \\(X_1\\) after controlling for \\(X_2\\)."
  },
  {
    "objectID": "lectures/08-Categorical-Variables/081-categorical-vars.html#multiple-regression-4",
    "href": "lectures/08-Categorical-Variables/081-categorical-vars.html#multiple-regression-4",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Multiple Regression",
    "text": "Multiple Regression\nAnother way to think about it: Regression by group"
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/073-quadratic.html",
    "href": "lectures/07-Non-Linear-Models/073-quadratic.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Let’s talk about a wage regression again. Suppose we would like to estimate the effect of age on earnings. We estimate the following SLR:\n. . .\n\\[\n\\text{Wage}_i = \\beta_0 + \\beta_1 \\text{Age}_i + u_i\n\\]\n. . .\nHowever, maybe we believe that \\(\\text{Wage}_i\\) and \\(\\text{Age}_i\\) have some nonlinear relationship—the effect of an additional year of experience, when age is 27 vs age is 67, might be different. So instead, we might estimate:\n. . .\n\\[\n\\text{Wage}_i = \\beta_0 + \\beta_1 \\text{Age}_i + \\beta_2 \\text{Age}^2_i + u_i\n\\]"
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/073-quadratic.html#quadratic-variables",
    "href": "lectures/07-Non-Linear-Models/073-quadratic.html#quadratic-variables",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Let’s talk about a wage regression again. Suppose we would like to estimate the effect of age on earnings. We estimate the following SLR:\n. . .\n\\[\n\\text{Wage}_i = \\beta_0 + \\beta_1 \\text{Age}_i + u_i\n\\]\n. . .\nHowever, maybe we believe that \\(\\text{Wage}_i\\) and \\(\\text{Age}_i\\) have some nonlinear relationship—the effect of an additional year of experience, when age is 27 vs age is 67, might be different. So instead, we might estimate:\n. . .\n\\[\n\\text{Wage}_i = \\beta_0 + \\beta_1 \\text{Age}_i + \\beta_2 \\text{Age}^2_i + u_i\n\\]"
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/073-quadratic.html#quadratic-variables-1",
    "href": "lectures/07-Non-Linear-Models/073-quadratic.html#quadratic-variables-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Quadratic variables",
    "text": "Quadratic variables\nIn this model:\n\\[\n\\text{Wage}_i = \\beta_0 + \\beta_1 \\text{Age}_i + \\beta_2 \\text{Age}^2_i + u_i\n\\]\nthe effect of \\(\\text{Age}_i\\) on \\(\\text{Wage}_i\\) would be:\n. . .\n\\[\n\\frac{\\partial \\text{Wage}_i}{\\partial \\text{Age}_i} = \\beta_1 + 2\\beta_2 \\text{Age}_i\n\\]"
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/073-quadratic.html#quadratic-regression",
    "href": "lectures/07-Non-Linear-Models/073-quadratic.html#quadratic-regression",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Quadratic regression",
    "text": "Quadratic regression\nRegression Model\n\\[\nY_i = \\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + u_i\n\\]\n. . .\nInterpretation\nSign of \\(\\beta_2\\) indicates whether the relationship is convex (+) or concave (-)\n. . .\nSign of \\(\\beta_1\\)? 🤷\n. . .\nPartial derivative of \\(Y\\) wrt. \\(X\\) is the marginal effect of \\(X\\) on \\(Y\\):\n\\[\n\\color{#B48EAD}{\\dfrac{\\partial Y}{\\partial X} = \\beta_1 + 2 \\beta_2 X}\n\\]\n\nEffect of \\(X\\) depends on the level of \\(X\\)"
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/073-quadratic.html#quadratic-regression-1",
    "href": "lectures/07-Non-Linear-Models/073-quadratic.html#quadratic-regression-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Quadratic Regression",
    "text": "Quadratic Regression\n\n\n\nTerm\nEstimate\nStd. Error\nStatistic\nP-value\n\n\n\n\nIntercept\n30,046\n138\n218\n0\n\n\nX\n158.89\n5.81\n27.3\n2.58e-123\n\n\n\\(X^{2}\\)\n-1.50\n0.0564\n-26.6\n6.19e-118\n\n\n\nWhat is the marginal effect of \\(X\\) on \\(Y\\)?\n. . .\n\\[\n    \\hat{\\dfrac{\\partial Y}{\\partial X}} = \\hat{\\beta}_{1} + 2 \\hat{\\beta}_{2} X =\n    158.89 + 2(-1.50)X =\n    158.89 - 3X\n\\]\nDepends on level of \\(X\\)"
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/073-quadratic.html#quadratic-regression-2",
    "href": "lectures/07-Non-Linear-Models/073-quadratic.html#quadratic-regression-2",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Quadratic Regression",
    "text": "Quadratic Regression\n\n\n\nTerm\nEstimate\nStd. Error\nStatistic\nP-value\n\n\n\n\nIntercept\n30,046\n138\n218\n0\n\n\nX\n158.89\n5.81\n27.3\n2.58e-123\n\n\n\\(X^{2}\\)\n-1.50\n0.0564\n-26.6\n6.19e-118\n\n\n\nWhat is the marginal effect of \\(X\\) on \\(Y\\), when \\(X = 0\\)?\n. . .\n\\[\n    \\widehat{\\dfrac{\\partial \\text{Y}}{\\partial \\text{X}} }\\Bigg|_{\\small \\text{X}=0} = \\hat{\\beta}_{1} = 158.89\n\\]"
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/073-quadratic.html#quadratic-regression-3",
    "href": "lectures/07-Non-Linear-Models/073-quadratic.html#quadratic-regression-3",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Quadratic Regression",
    "text": "Quadratic Regression\n\n\n\nTerm\nEstimate\nStd. Error\nStatistic\nP-value\n\n\n\n\nIntercept\n30,046\n138\n218\n0\n\n\nX\n158.89\n5.81\n27.3\n2.58e-123\n\n\n\\(X^{2}\\)\n-1.50\n0.0564\n-26.6\n6.19e-118\n\n\n\nWhat is the marginal effect of \\(X\\) on \\(Y\\), when \\(X = 2\\)?\n. . .\n\\[\n    \\widehat{\\dfrac{\\partial \\text{Y}}{\\partial \\text{X}} }\\Bigg|_{\\small \\text{X}=2} =\n    \\hat{\\beta}_{1} + 2 \\hat{\\beta}_{2} \\cdot (2) =\n    158.89 - 5.99 =\n    152.9\n\\]"
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/073-quadratic.html#quadratic-regression-4",
    "href": "lectures/07-Non-Linear-Models/073-quadratic.html#quadratic-regression-4",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Quadratic Regression",
    "text": "Quadratic Regression\n\n\n\nTerm\nEstimate\nStd. Error\nStatistic\nP-value\n\n\n\n\nIntercept\n30,046\n138\n218\n0\n\n\nX\n158.89\n5.81\n27.3\n2.58e-123\n\n\n\\(X^{2}\\)\n-1.50\n0.0564\n-26.6\n6.19e-118\n\n\n\nWhat is the marginal effect of \\(X\\) on \\(Y\\), when \\(X = 7\\)?\n. . .\n\\[\n    \\widehat{\\dfrac{\\partial \\text{Y}}{\\partial \\text{X}} }\\Bigg|_{\\small \\text{X}=7} =\n    \\hat{\\beta}_{1} + 2 \\hat{\\beta}_{2} \\cdot (7) =\n    158.89 - 20.98 =\n    137.91\n\\]"
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/073-quadratic.html#fitted-regression-line",
    "href": "lectures/07-Non-Linear-Models/073-quadratic.html#fitted-regression-line",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Fitted Regression Line",
    "text": "Fitted Regression Line"
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/073-quadratic.html#marginal-effect-of-x-on-y",
    "href": "lectures/07-Non-Linear-Models/073-quadratic.html#marginal-effect-of-x-on-y",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Marginal Effect of \\(X\\) on \\(Y\\)",
    "text": "Marginal Effect of \\(X\\) on \\(Y\\)"
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/073-quadratic.html#quadratic-regression-5",
    "href": "lectures/07-Non-Linear-Models/073-quadratic.html#quadratic-regression-5",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Quadratic regression",
    "text": "Quadratic regression\nWhere does the regression \\(\\hat{Y_i} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i + \\hat{\\beta}_2 X_i^2\\) turn?\n\n. . .\nStep 1: Take the derivative and set equal to zero.\n\\[\n\\widehat{\\dfrac{\\partial \\text{Y}}{\\partial \\text{X}} } = \\hat{\\beta}_1 + 2\\hat{\\beta}_2 X = 0\n\\]\n. . .\nStep 1: Solve for \\(X\\).\n\\[\nX = -\\dfrac{\\hat{\\beta}_1}{2\\hat{\\beta}_2}\n\\]\n. . .\nEx. Peak of previous regression occurs at \\(X = 53.02\\)."
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/071-prologue.html#linear-regression",
    "href": "lectures/07-Non-Linear-Models/071-prologue.html#linear-regression",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Linear regression",
    "text": "Linear regression\nSuppose we would like to estimate the degree to which an increase in GDP correlates with Life expectancy. We set up our model as follows:\n\\[\n{\\text{Life Expectancy}_i} = \\beta_0 + \\beta_1 \\text{GDP}_i + u_i\n\\]\n. . .\nUsing the gapminder package in R, we could quickly generate estimates to get at the correlation But first, as always, let’s plot it before running the regression\n\nVisualize the OLS fit? Is \\(\\beta_1\\) positive or negeative?"
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/071-prologue.html#linear-regression-1",
    "href": "lectures/07-Non-Linear-Models/071-prologue.html#linear-regression-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Linear regression",
    "text": "Linear regression\nUsing the gapminder, we could quickly generate estimates for\n\\[\n\\widehat{\\text{Life Expectancy}_i} = \\hat{\\beta_0} + \\hat{\\beta_1} \\cdot \\text{GDP}_i\n\\]\n. . .\n\n\nlibrary(gapminder)\nlibrary(broom)\n\nm1 = lm(lifeExp ~ gdpPercap, data = gapminder) \n\ntidy(m1)\n\n# A tibble: 2 × 5\n  term         estimate std.error statistic   p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept) 54.0      0.315         171.  0        \n2 gdpPercap    0.000765 0.0000258      29.7 3.57e-156\n\n\n\nFitting OLS. But are you satisfied? Can we do better?\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/071-prologue.html#linearity-in-ols",
    "href": "lectures/07-Non-Linear-Models/071-prologue.html#linearity-in-ols",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Linearity in OLS",
    "text": "Linearity in OLS\nUp to this point, we’ve acknowledged OLS as a “linear” estimator.\n. . .\n\nMany economic relationships are nonlinear.\n\ne.g., most production functions, profit, diminishing marginal utility, tax revenue as a function of the tax rate, etc.\n\n. . .\n\nThe “linear” in simple linear regression refers to the linearity of the parameters or coefficients, not the predictors themselves."
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/071-prologue.html#linearity-in-ols-1",
    "href": "lectures/07-Non-Linear-Models/071-prologue.html#linearity-in-ols-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Linearity in OLS",
    "text": "Linearity in OLS\nOLS is flexible and can accommodate a subset of nonlinear relationships.\n\nUnderlying model must be linear-in-parameters.\nNonlinear transformations of variables are okay.\nModeling some nonlinear relationships requires advanced estimation techniques, such as maximum likelihood1\n\n. . .\n\nPut different, independent variables can be a linear combination of the parameters, regardless of any nonlinear transformations"
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/071-prologue.html#linearity",
    "href": "lectures/07-Non-Linear-Models/071-prologue.html#linearity",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Linearity",
    "text": "Linearity\nLinear-in-parameters: Parameters enter model as a weighted sum, where the weights are functions of the variables.\n\nOne of the assumptions required for the unbiasedness of OLS.\n\nLinear-in-variables: Variables enter the model as a weighted sum, where the weights are functions of the parameters.\n\nNot required for the unbiasedness of OLS.\n\n. . .\nThe standard linear regression model satisfies both properties:\n\\[Y_i = \\beta_0 + \\beta_1X_{1i} + \\beta_2X_{2i} + \\dots + \\beta_kX_{ki} + u_i\\]"
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/071-prologue.html#linearity-1",
    "href": "lectures/07-Non-Linear-Models/071-prologue.html#linearity-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Linearity",
    "text": "Linearity\nWhich of the following are an example of linear-in-parameters, linear-in-variables, or neither?\n\n1. \\(Y_i = \\beta_0 + \\beta_1X_{i} + \\beta_2X_{i}^2 + \\dots + \\beta_kX_{i}^k + u_i\\)\n2. \\(Y_i = \\beta_0X_i^{\\beta_1}v_i\\)\n3. \\(Y_i = \\beta_0 + \\beta_1\\beta_2X_{i} + u_i\\)"
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/071-prologue.html#linearity-2",
    "href": "lectures/07-Non-Linear-Models/071-prologue.html#linearity-2",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Linearity",
    "text": "Linearity\nWhich of the following are an example of linear-in-parameters, linear-in-variables, or neither?\n\n1. \\(\\color{#A3BE8C}{Y_i = \\beta_0 + \\beta_1X_{i} + \\beta_2X_{i}^2 + \\dots + \\beta_kX_{i}^k + u_i}\\)\n2. \\(Y_i = \\beta_0X_i^{\\beta_1}v_i\\)\n3. \\(Y_i = \\beta_0 + \\beta_1\\beta_2X_{i} + u_i\\)\n\nModel 1 is linear-in-parameters, but not linear-in-variables."
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/071-prologue.html#linearity-3",
    "href": "lectures/07-Non-Linear-Models/071-prologue.html#linearity-3",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Linearity",
    "text": "Linearity\nWhich of the following are an example of linear-in-parameters, linear-in-variables, or neither?\n\n1. \\(\\color{#A3BE8C}{Y_i = \\beta_0 + \\beta_1X_{i} + \\beta_2X_{i}^2 + \\dots + \\beta_kX_{i}^k + u_i}\\)\n2. \\(\\color{#434C5E}{Y_i = \\beta_0X_i^{\\beta_1}v_i}\\)\n3. \\(Y_i = \\beta_0 + \\beta_1\\beta_2X_{i} + u_i\\)\n\nModel 1 is linear-in-parameters, but not linear-in-variables.\nModel 2 is neither."
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/071-prologue.html#linearity-4",
    "href": "lectures/07-Non-Linear-Models/071-prologue.html#linearity-4",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Linearity",
    "text": "Linearity\nWhich of the following are an example of linear-in-parameters, linear-in-variables, or neither?\n\n1. \\(\\color{#A3BE8C}{Y_i = \\beta_0 + \\beta_1X_{i} + \\beta_2X_{i}^2 + \\dots + \\beta_kX_{i}^k + u_i}\\)\n2. \\(\\color{#434C5E}{Y_i = \\beta_0X_i^{\\beta_1}v_i}\\)\n3. \\(\\color{#B48EAD}{Y_i = \\beta_0 + \\beta_1\\beta_2X_{i} + u_i}\\)\n\nModel 1 is linear-in-parameters, but not linear-in-variables.\nModel 2 is neither.\nModel 3 is linear-in-variables, but not linear-in-parameters."
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/071-prologue.html#footnotes",
    "href": "lectures/07-Non-Linear-Models/071-prologue.html#footnotes",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBeyond the scope of this class.↩︎"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/066-f-tests.html#f-tests",
    "href": "lectures/06-Multiple-Var-Reg/066-f-tests.html#f-tests",
    "title": "EC 320 - Intro. Econometrics",
    "section": "F Tests",
    "text": "F Tests\nt tests allow us to test simple hypotheses involving a single parameter.\n\ne.g., \\(\\beta_1 = 0\\) or \\(\\beta_2 = 1\\).\n\n. . .\nF tests allow us to test hypotheses that involve multiple parameters.\n\ne.g., \\(\\beta_1 = \\beta_2\\) or \\(\\beta_3 + \\beta_4 = 1\\)."
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/066-f-tests.html#f-tests-1",
    "href": "lectures/06-Multiple-Var-Reg/066-f-tests.html#f-tests-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "F Tests",
    "text": "F Tests\nEx. Is money “fungible”?\nEconomists often say that “money is fungible.”\nWe might want to test whether money received as income actually has the same effect on consumption as money received from tax credits.\n\\[\n\\text{Consumption}_i = \\beta_0 + \\beta_1 \\text{Income}_{i} + \\beta_2 \\text{Credit}_i + u_i\n\\]"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/066-f-tests.html#f-tests-2",
    "href": "lectures/06-Multiple-Var-Reg/066-f-tests.html#f-tests-2",
    "title": "EC 320 - Intro. Econometrics",
    "section": "F Tests",
    "text": "F Tests\nEx. Is money “fungible”?\nWe can write our null hypothesis as\n\\[\nH_0:\\: \\beta_1 = \\beta_2 \\iff H_0 :\\: \\beta_1 - \\beta_2 = 0\n\\]\nImposing the null hypothesis gives us a restricted model\n\\[\n\\text{Consumption}_i = \\beta_0 + \\beta_1 \\text{Income}_{i} + \\beta_1 \\text{Credit}_i + u_i\n\\]\n\\[\n\\text{Consumption}_i = \\beta_0 + \\beta_1 \\left( \\text{Income}_{i} + \\text{Credit}_i \\right) + u_i\n\\]"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/066-f-tests.html#f-tests-3",
    "href": "lectures/06-Multiple-Var-Reg/066-f-tests.html#f-tests-3",
    "title": "EC 320 - Intro. Econometrics",
    "section": "F Tests",
    "text": "F Tests\nEx. Is money “fungible”?\nTo test the null hypothesis \\(H_o :\\: \\beta_1 = \\beta_2\\) against \\(H_a :\\: \\beta_1 \\neq \\beta_2\\), we use the \\(F\\) statistic:\n\\[\n\\begin{align}\n  F_{q,\\,n-k-1} = \\dfrac{\\left(\\text{RSS}_r - \\text{RSS}_u\\right)/q}{\\text{RSS}_u/(n-k-1)}\n\\end{align}\n\\]\nwhich (as its name suggests) follows the \\(F\\) distribution with \\(q\\) numerator degrees of freedom and \\(n-k-1\\) denominator degrees of freedom.\nHere, \\(q\\) is the number of restrictions we impose via \\(H_0\\)."
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/066-f-tests.html#f-tests-4",
    "href": "lectures/06-Multiple-Var-Reg/066-f-tests.html#f-tests-4",
    "title": "EC 320 - Intro. Econometrics",
    "section": "F Tests",
    "text": "F Tests\nEx. Is money “fungible”?\nThe term \\(\\text{RSS}_r\\) is the sum of squared residuals (RSS) from our restricted model\n\\[\n\\text{Consumption}_i = \\beta_0 + \\beta_1 \\left( \\text{Income}_{i} + \\text{Credit}_i \\right) + u_i\n\\]\nand \\(\\text{RSS}_u\\) is the sum of squared residuals (RSS) from our unrestricted model\n\\[\n\\text{Consumption}_i = \\beta_0 + \\beta_1 \\text{Income}_{i} + \\beta_2 \\text{Credit}_i + u_i\n\\]"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/066-f-tests.html#f-tests-5",
    "href": "lectures/06-Multiple-Var-Reg/066-f-tests.html#f-tests-5",
    "title": "EC 320 - Intro. Econometrics",
    "section": "F Tests",
    "text": "F Tests\nFinally, we compare our \\(F\\)-statistic to a critical value of \\(F\\) to test the null hypothesis.\nIf \\(F\\) &gt; \\(F_\\text{crit}\\), then reject the null hypothesis at the \\(\\alpha \\times 100\\) percent level.\n\nFind \\(F_\\text{crit}\\) in a table using the desired significance level, numerator degrees of freedom, and denominator degrees of freedom.\n\n. . .\nAside: Why are \\(F\\)-statistics always positive?"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/066-f-tests.html#f-tests-6",
    "href": "lectures/06-Multiple-Var-Reg/066-f-tests.html#f-tests-6",
    "title": "EC 320 - Intro. Econometrics",
    "section": "F Tests",
    "text": "F Tests\nRSS is usually a large cumbersome number.\nAlternative: Calculate the \\(F\\)-statistic using \\(R^2\\).\n\\[\n\\begin{align}\n  F = \\dfrac{\\left(R^2_u - R^2_r\\right)/q}{ (1 - R^2_u)/(n-k-1)}\n\\end{align}\n\\]\n. . .\nWhere does this come from?\n\n\n\\(\\text{TSS} = \\text{RSS} + \\text{ESS}\\)\n\\(R^2 = \\text{ESS}/\\text{TSS}\\)\n\n\\(\\text{RSS}_r = \\text{TSS}(1-R^2_r)\\)\n\\(\\text{RSS}_u = \\text{TSS}(1-R^2_u)\\)"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/064-multi-linear-reg.html",
    "href": "lectures/06-Multiple-Var-Reg/064-multi-linear-reg.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Simple linear regression features one dependent variable and one independent variable:\n\\[\n\\color{#434C5E}{Y_i} = \\beta_0 + \\beta_1 \\color{\"#81A1C1\"}{X_i} + u_i\n\\]\nMultiple linear regression features one dependent variable and multiple independent variables:\n\\[\n\\color{#434C5E}{Y_i} = \\beta_0 + \\beta_1 \\color{\"#81A1C1\"}{X_{1i}} + \\beta_2 \\color{\"#81A1C1\"}{X_{2i}} + \\cdots + \\beta_{k} \\color{\"#81A1C1\"}{X_{ki}} + u_i\n\\]\n. . .\nThis serves more than one purpose. Multiple independent variables improves predictions, avoids OVB, and better explains variation in \\(Y\\)."
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/064-multi-linear-reg.html#multiple-linear-regression",
    "href": "lectures/06-Multiple-Var-Reg/064-multi-linear-reg.html#multiple-linear-regression",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Simple linear regression features one dependent variable and one independent variable:\n\\[\n\\color{#434C5E}{Y_i} = \\beta_0 + \\beta_1 \\color{\"#81A1C1\"}{X_i} + u_i\n\\]\nMultiple linear regression features one dependent variable and multiple independent variables:\n\\[\n\\color{#434C5E}{Y_i} = \\beta_0 + \\beta_1 \\color{\"#81A1C1\"}{X_{1i}} + \\beta_2 \\color{\"#81A1C1\"}{X_{2i}} + \\cdots + \\beta_{k} \\color{\"#81A1C1\"}{X_{ki}} + u_i\n\\]\n. . .\nThis serves more than one purpose. Multiple independent variables improves predictions, avoids OVB, and better explains variation in \\(Y\\)."
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/064-multi-linear-reg.html#simple-linear-regression",
    "href": "lectures/06-Multiple-Var-Reg/064-multi-linear-reg.html#simple-linear-regression",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Simple Linear Regression",
    "text": "Simple Linear Regression"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/064-multi-linear-reg.html#add-dimension-rightarrow-per-student-expenditure",
    "href": "lectures/06-Multiple-Var-Reg/064-multi-linear-reg.html#add-dimension-rightarrow-per-student-expenditure",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Add Dimension \\(\\rightarrow\\) Per Student Expenditure",
    "text": "Add Dimension \\(\\rightarrow\\) Per Student Expenditure"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/064-multi-linear-reg.html#multiple-linear-regression-ex.",
    "href": "lectures/06-Multiple-Var-Reg/064-multi-linear-reg.html#multiple-linear-regression-ex.",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Multiple Linear Regression Ex.",
    "text": "Multiple Linear Regression Ex.\nIf we ignore per student expenditure (aka our original simple regression)\n\\[\n\\text{Scores}_i = \\beta_0 + \\beta_1 \\text{Class Size}_i + u_i\n\\]"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/064-multi-linear-reg.html#multiple-linear-regression-ex.-1",
    "href": "lectures/06-Multiple-Var-Reg/064-multi-linear-reg.html#multiple-linear-regression-ex.-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Multiple Linear Regression Ex.",
    "text": "Multiple Linear Regression Ex.\nControlling for school funding\n\\[\n\\text{Scores}_i = \\beta_0 + \\beta_1 \\text{Class Size}_i + \\text{Expenditure}_i+ u_i\n\\]"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/064-multi-linear-reg.html#ols-estimation",
    "href": "lectures/06-Multiple-Var-Reg/064-multi-linear-reg.html#ols-estimation",
    "title": "EC 320 - Intro. Econometrics",
    "section": "OLS Estimation",
    "text": "OLS Estimation\nResiduals are now defined as:\n. . .\n\\[\n\\hat{u}_i = Y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 X_{1i} - \\hat{\\beta}_2 X_{2i} - \\cdots - \\hat{\\beta}_{k} X_{ki}\n\\]\n. . .\nAs with SLR, OLS minimizes the sum of squared residuals (RSS).\n. . .\n\\[\n\\begin{align*}\n  \\color{#D08770}{RSS} &= \\sum_{i = 1}^{n} (Y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 X_{1i} - \\hat{\\beta}_2 X_{2i} - \\cdots - \\hat{\\beta}_{k} X_{ki})^2 \\\\\n                        &= \\color{#D08770}{\\sum_{i=1}^n \\hat{u}_i^2}\n\\end{align*}\n\\]\nwhich is a familiar expression."
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/064-multi-linear-reg.html#ols-estimation-1",
    "href": "lectures/06-Multiple-Var-Reg/064-multi-linear-reg.html#ols-estimation-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "OLS Estimation",
    "text": "OLS Estimation\nTo obtain point estimates:\n\\[\n\\min_{\\hat{\\beta}_0,\\, \\hat{\\beta}_1,\\, \\dots \\, \\hat{\\beta}_k} \\quad \\color{#D08770}{\\sum_{i=1}^n \\hat{u}_i^2}\n\\]\n\nTake partial derivatives of RSS with respect to each \\(\\hat{\\beta}\\)\nSet each derivative equal to zero\nSolve the system of \\(k+1\\) equations1.\n\n. . .\nThe algebra is cumbersome. We let R do the heavy lifting."
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/064-multi-linear-reg.html#coefficient-interpretation",
    "href": "lectures/06-Multiple-Var-Reg/064-multi-linear-reg.html#coefficient-interpretation",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Coefficient Interpretation",
    "text": "Coefficient Interpretation\nModel\n\\[\n\\color{}{Y_i} = \\beta_0 + \\beta_1 \\color{}{X_{1i}} + \\beta_2 \\color{}{X_{2i}} + \\cdots + \\beta_{k} \\color{}{X_{ki}} + u_i\n\\]\nInterpretation\n\nThe intercept \\(\\hat{\\beta}_0\\) is the average value of \\(Y_i\\) when all of the independent variables are equal to zero.\nSlope parameters \\(\\hat{\\beta}_1, \\dots, \\hat{\\beta}_{k}\\) give us the change in \\(Y_i\\) from a one-unit change in \\(X_j\\), holding the other \\(X\\) variables constant."
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/064-multi-linear-reg.html#algebraic-properties-of-ols",
    "href": "lectures/06-Multiple-Var-Reg/064-multi-linear-reg.html#algebraic-properties-of-ols",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Algebraic properties of OLS",
    "text": "Algebraic properties of OLS\nThe OLS first-order conditions yield the same properties as before.\n\n1. Residuals sum to zero: \\(\\sum_{i=1}^n \\hat{u_i} = 0\\).\n2. The sample covariance \\(X_i\\) and the \\(\\hat{u_i}\\) is zero.\n3. The point \\((\\bar{X_1}, \\bar{X_2}, \\dots, \\bar{X_k}, \\bar{Y})\\) is on the fitted regression “line.”"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/064-multi-linear-reg.html#goodness-of-fit",
    "href": "lectures/06-Multiple-Var-Reg/064-multi-linear-reg.html#goodness-of-fit",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Goodness of fit",
    "text": "Goodness of fit\nFitted values are defined similarly:\n\\[\n\\hat{Y_i} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_{1i} + \\hat{\\beta}_2 X_{2i} + \\cdots + \\hat{\\beta}_{k} X_{ki}\n\\]\nThe formula for \\(R^2\\) is the same as before:\n\\[\nR^2 =\\frac{\\sum(\\hat{Y_i}-\\bar{Y})^2}{\\sum(Y_i-\\bar{Y})^2}\n\\]"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/064-multi-linear-reg.html#goodness-of-fit-1",
    "href": "lectures/06-Multiple-Var-Reg/064-multi-linear-reg.html#goodness-of-fit-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Goodness of fit",
    "text": "Goodness of fit\nWe can describe the variation explain in \\(Y\\) with venn diagrams"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/064-multi-linear-reg.html#goodness-of-fit-2",
    "href": "lectures/06-Multiple-Var-Reg/064-multi-linear-reg.html#goodness-of-fit-2",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Goodness of fit",
    "text": "Goodness of fit\nWe can describe the variation explain in \\(Y\\) with venn diagrams\n\n\n\n\n\n\n\n\n\nAs we add more variables, we are able to explain more “chunks” of the variation in \\(y\\)"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/064-multi-linear-reg.html#section",
    "href": "lectures/06-Multiple-Var-Reg/064-multi-linear-reg.html#section",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Problem: As we add variables to our model, \\(R^2\\) mechanically increases.\n\nLet me show you this problem with a simulation"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/064-multi-linear-reg.html#section-1",
    "href": "lectures/06-Multiple-Var-Reg/064-multi-linear-reg.html#section-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Simulate a dataset of 10,000 observations on \\(y\\) and 1,000 random \\(x_k\\) variables, where\n\\[\ny \\perp x_k \\quad \\forall x_k \\; \\text{s.t.} \\; k = 1, 2, \\dots, 1000\n\\]\n\nWe have 1,000 independent variables that are independent to the dependent variable. Each \\(x_k\\) has no relationship to \\(y\\) whatsoever.\n\n\nProblem: As we add variables to our model, \\(\\color{#314f4f}{R^2}\\) mechanically increases.\nPseudo-code:\n\nGenerate 10,000 obs. on \\(y\\)\nGenerate 10,000 obs. on variables \\(x_1\\) through \\(x_{1000}\\)\n\nRegressions:\n\nLM1: Regress \\(y\\) of \\(x_1\\); record \\(R^2\\)\nLM2: Regress \\(y\\) of \\(x_1\\) and \\(x_2\\); record \\(R^2\\)\n…\nLM1000: Regress \\(y\\) on \\(x_1\\), \\(x_2\\), …, \\(x_{1000}\\); record \\(R^2\\)"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/064-multi-linear-reg.html#section-2",
    "href": "lectures/06-Multiple-Var-Reg/064-multi-linear-reg.html#section-2",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Problem: As we add variables to our model, \\(R^2\\) mechanically increases."
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/064-multi-linear-reg.html#section-3",
    "href": "lectures/06-Multiple-Var-Reg/064-multi-linear-reg.html#section-3",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Problem: As we add variables to our model, \\(R^2\\) mechanically increases.\nOne solution: Penalize for the number of variables, e.g., adjusted \\(R^2\\):"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/064-multi-linear-reg.html#goodness-of-fit-3",
    "href": "lectures/06-Multiple-Var-Reg/064-multi-linear-reg.html#goodness-of-fit-3",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Goodness of fit",
    "text": "Goodness of fit\nProblem: As we add variables to our model, \\(R^2\\) mechanically increases.\nOne solution: Penalize for the number of variables, e.g., adjusted \\(R^2\\):\n\\[\n\\bar{R}^2 = 1 - \\dfrac{\\sum_i \\left( Y_i - \\hat{Y}_i \\right)^2/(n-k-1)}{\\sum_i \\left( Y_i - \\bar{Y} \\right)^2/(n-1)}\n\\]\n\nNote: Adjusted \\(R^2\\) need not be between 0 and 1."
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/064-multi-linear-reg.html#multiple-regression",
    "href": "lectures/06-Multiple-Var-Reg/064-multi-linear-reg.html#multiple-regression",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Multiple regression",
    "text": "Multiple regression\nThere are tradeoffs to remember as we add/remove variables:\n\n\nFewer variables\n\nExplains less variation in \\(y\\)\nProvide simple interpretations and visualizations\nMore worried about omitted-variable bias\n\n\nMore variables\n\nMore likely to find spurious relationships2\nMore difficult interpretation\nThe variance of our point estimates will be bigger\nWe still might have omitted-variable bias"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/064-multi-linear-reg.html#multiple-regression-1",
    "href": "lectures/06-Multiple-Var-Reg/064-multi-linear-reg.html#multiple-regression-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Multiple regression",
    "text": "Multiple regression\nThere are tradeoffs to remember as we add/remove variables:\n\n\nFewer variables\n\nExplains less variation in \\(y\\)\nProvide simple interpretations and visualizations\nMore worried about omitted-variable bias\n\n\nMore variables\n\nMore likely to find spurious relationships3\nMore difficult interpretation\nThe variance of our point estimates will be bigger\nWe still might have omitted-variable bias"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/064-multi-linear-reg.html#footnotes",
    "href": "lectures/06-Multiple-Var-Reg/064-multi-linear-reg.html#footnotes",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n\\(k+1\\) due to the intercept.↩︎\nSpurious meaning statistically significant by coincidence—not reflective of true, population-level relationship↩︎\nSpurious meaning statistically significant by coincidence—not reflective of true, population-level relationship↩︎"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/062-class-size-ex.html",
    "href": "lectures/06-Multiple-Var-Reg/062-class-size-ex.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Empirical question:\n\nWhat improvement do smaller class sizes have on student test scores, if any?"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/062-class-size-ex.html#section",
    "href": "lectures/06-Multiple-Var-Reg/062-class-size-ex.html#section",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Empirical question:\n\nWhat improvement do smaller class sizes have on student test scores, if any?"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/062-class-size-ex.html#ex.-effect-of-class-sizes-on-test-scores",
    "href": "lectures/06-Multiple-Var-Reg/062-class-size-ex.html#ex.-effect-of-class-sizes-on-test-scores",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Ex. Effect of class sizes on test scores",
    "text": "Ex. Effect of class sizes on test scores\nEstimate effect of class size on test scores with the following:\n\\[\n\\text{Scores}_i = \\beta_0 + \\beta_1 \\text{Class Size}_i + u_i\n\\]\n\n. . .\nData: Test performance and class across school districts in MA\n\nScores: 4th grade test scores agg. across reading, math, and science\nClass size: Ratio of number of students to teachers\n\n\n. . .\nAlways plot your data first\n\n\n\n\n\n\nRaw Data\n\n\n\n\n\n\nFitting OLS"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/062-class-size-ex.html#ex.-effect-of-class-sizes-on-test-scores-1",
    "href": "lectures/06-Multiple-Var-Reg/062-class-size-ex.html#ex.-effect-of-class-sizes-on-test-scores-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Ex. Effect of class sizes on test scores",
    "text": "Ex. Effect of class sizes on test scores\nEstimate effect of class size on test scores with the following:\n\\[\n\\text{Scores}_i = \\beta_0 + \\beta_1 \\text{Class Size}_i + u_i\n\\]\n\nQ. How might smaller class sizes influence test scores?\n. . .\nA. More personalized teaching, less classroom disruptions etc.\n. . .\n\nQ. What sign would we expect on \\(\\beta_1\\)?\n. . .\nA.\n\\[\n\\beta_1 &lt; 0\n\\]"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/062-class-size-ex.html#section-1",
    "href": "lectures/06-Multiple-Var-Reg/062-class-size-ex.html#section-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Smaller class sizes (X) increases test scores (Y)"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/062-class-size-ex.html#ex.-effect-of-class-sizes-on-test-scores-2",
    "href": "lectures/06-Multiple-Var-Reg/062-class-size-ex.html#ex.-effect-of-class-sizes-on-test-scores-2",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Ex. Effect of class sizes on test scores",
    "text": "Ex. Effect of class sizes on test scores\nEstimate effect of class size on test scores with the following:\n\\[\n\\text{Scores}_i = \\beta_0 + \\beta_1 \\text{Class Size}_i + u_i\n\\]\n\nQ. Do we think \\(\\beta_1\\) will be a good guess of the underlying population parameter?\n. . .\nA. In \\(u_i\\), several variables are correlated with class size and test scores\nSuch as… school funding, which might affect:\n. . .\n\n\n\nTextbooks\nComputers\n\n\n\nTeacher salary\nAttract high income parents"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/062-class-size-ex.html#section-2",
    "href": "lectures/06-Multiple-Var-Reg/062-class-size-ex.html#section-2",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Smaller class sizes (X) increases test scores (Y)"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/062-class-size-ex.html#section-3",
    "href": "lectures/06-Multiple-Var-Reg/062-class-size-ex.html#section-3",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Smaller class sizes (X) increases test scores (Y) along with greater school funding (U)"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/062-class-size-ex.html#section-4",
    "href": "lectures/06-Multiple-Var-Reg/062-class-size-ex.html#section-4",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Smaller class sizes (X) increases test scores (Y) along with greater school funding (U). And, school funding (U) is correlated with test scores (X).\n\n\n\n\n\n\nAny unobserved variable that connects a backdoor path between class size (X) and test scores (Y) will bias our point estimate of \\(\\beta_1\\)."
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/062-class-size-ex.html#section-5",
    "href": "lectures/06-Multiple-Var-Reg/062-class-size-ex.html#section-5",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Any unobserved variable that connects a backdoor path between class size (X) and test scores (Y) will bias our point estimate of \\(\\beta_1\\). Why?\n\nA1. Linearity\nA2. Sample Variation\nA3. Exogeniety\nA4. Homoskedasticity\nA5. Non-autocorrelation\nA6. Normality"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/062-class-size-ex.html#section-6",
    "href": "lectures/06-Multiple-Var-Reg/062-class-size-ex.html#section-6",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Any unobserved variable that connects a backdoor path between class size (X) and test scores (Y) will bias our point estimate of \\(\\beta_1\\). Why?\n\nA1. Linearity\nA2. Sample Variation\nA3. Exogeniety: The \\(X\\) variable is exogenous\nA4. Homoskedasticity\nA5. Non-autocorrelation\nA6. Normality"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/062-class-size-ex.html#section-7",
    "href": "lectures/06-Multiple-Var-Reg/062-class-size-ex.html#section-7",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Any unobserved variable that connects a backdoor path between class size (X) and test scores (Y) will bias our point estimate of \\(\\beta_1\\). Why?\n\nA. Because is violates the exogeniety assumption\n\\[\n\\mathop{\\mathbb{E}}\\left( u|\\text{Class Size} \\right) \\neq 0\n\\]\nCorrelation between class size and school funding (\\(u_i\\)) is not zero."
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/062-class-size-ex.html#section-8",
    "href": "lectures/06-Multiple-Var-Reg/062-class-size-ex.html#section-8",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Graphically… Valid exogeniety, i.e., \\(\\mathop{\\mathbb{E}}\\left( u \\mid X \\right) = 0\\)"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/062-class-size-ex.html#section-9",
    "href": "lectures/06-Multiple-Var-Reg/062-class-size-ex.html#section-9",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Graphically… Invalid exogeniety, i.e., \\(\\mathop{\\mathbb{E}}\\left( u \\mid X \\right) \\neq 0\\)"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/062-class-size-ex.html#section-10",
    "href": "lectures/06-Multiple-Var-Reg/062-class-size-ex.html#section-10",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "What the actual data looks like:\n\n\n\n\n\n\n\nThis violation has a name. We call it omitted variable bias"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#quick-recap",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#quick-recap",
    "title": "Multiple Variable Regression Analysis",
    "section": "Quick Recap",
    "text": "Quick Recap\nThe Regression Model\nWe can estimate the effect of \\(X\\) on \\(Y\\) by estimating a regression model:\n\\[Y_i = \\beta_0 + \\beta_1 X_i + u_i\\]\n\n\\(Y_i\\) is the outcome variable.\n\\(X_i\\) is the treatment variable (continuous).\n\\(\\beta_0\\) is the intercept parameter. \\(\\mathop{\\mathbb{E}}\\left[ {Y_i | X_i=0} \\right] = \\beta_0\\)\n\\(\\beta_1\\) is the slope parameter, which under the correct causal setting represents marginal change in \\(X_i\\)’s effect on \\(Y_i\\). \\(\\frac{\\partial Y_i}{\\partial X_i} = \\beta_1\\)\n\\(u_i\\) is an error term including all other (omitted) factors affecting \\(Y_i\\)."
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#the-error-term",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#the-error-term",
    "title": "Multiple Variable Regression Analysis",
    "section": "The error term",
    "text": "The error term\n\\(u_i\\) is quite special\n\nConsider the data generating process of variable \\(Y_i\\),\n\n\\(u_i\\) captures all unobserved relationships that explain variation in \\(Y_i\\).\n\n\nSome error will exist in all models, our aim is to minimize error under a set of constraints. This error is the price we are willing to accept for simplified model"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#the-error-term-1",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#the-error-term-1",
    "title": "Multiple Variable Regression Analysis",
    "section": "The error term",
    "text": "The error term\nFive items contribute to the existence of the disturbance term:\n1. Omission of explanatory variables\n2. Aggregation of Variables\n3. Model misspecificiation\n4. Functional misspecificiation\n5. Measurement error"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#running-regressions",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#running-regressions",
    "title": "Multiple Variable Regression Analysis",
    "section": "Running regressions",
    "text": "Running regressions\nUsing an estimator with data on \\(X_i\\) and \\(Y_i\\), we can estimate a fitted regression line:\n\\[\n\\hat{Y}_{i} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1} X_i\n\\]\n\n\\(\\hat{Y}_{i}\\) is the fitted value of \\(Y_i\\).\n\\(\\hat{\\beta}_{0}\\) is the estimated intercept.\n\\(\\hat{\\beta}_{1}\\) is the estimated slope.\n\n\nThis procedure produces misses, known as residuals, \\(Y_{i} - \\hat{Y}_{i}\\)"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#gauss-markov-theorem",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#gauss-markov-theorem",
    "title": "Multiple Variable Regression Analysis",
    "section": "Gauss-Markov Theorem",
    "text": "Gauss-Markov Theorem\n\nOLS is the Best Linear Unbiased Estimator (BLUE) when the following assumptions hold:\n\nA1. Linearity: The population relationship is linear in parameters with an additive error term.\nA2. Sample Variation: There is variation in \\(X\\).\nA3. Exogeniety: The \\(X\\) variable is exogenous\nA4. Homoskedasticity: The error term has the same variance for each value of the independent variable\nA5. Non-autocorrelation: The values of error terms have independent distributions"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#section",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#section",
    "title": "Multiple Variable Regression Analysis",
    "section": "",
    "text": "Consider the following example."
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#section-1",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#section-1",
    "title": "Multiple Variable Regression Analysis",
    "section": "",
    "text": "Empirical question:\n\nWhat improvement do smaller class sizes have on student test scores, if any?"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#ex.-effect-of-class-sizes-on-test-scores-1",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#ex.-effect-of-class-sizes-on-test-scores-1",
    "title": "Multiple Variable Regression Analysis",
    "section": "Ex. Effect of class sizes on test scores",
    "text": "Ex. Effect of class sizes on test scores\nEstimate effect of class size on test scores with the following:\n\\[\n\\text{Scores}_i = \\beta_0 + \\beta_1 \\text{Class Size}_i + u_i\n\\]\n\n\nData: Test performance and class across school districts in MA\n\nScores: 4th grade test scores agg. across reading, math, and science\nClass size: Ratio of number of students to teachers\n\n\n\n\nAlways plot your data first"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#ex.-effect-of-class-sizes-on-test-scores-2",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#ex.-effect-of-class-sizes-on-test-scores-2",
    "title": "Multiple Variable Regression Analysis",
    "section": "Ex. Effect of class sizes on test scores",
    "text": "Ex. Effect of class sizes on test scores\nEstimate effect of class size on test scores with the following:\n\\[\n\\text{Scores}_i = \\beta_0 + \\beta_1 \\text{Class Size}_i + u_i\n\\]\n\nQ. How might smaller class sizes influence test scores?\n\nA. More personalized teaching, less classroom disruptions etc.\n\n\n\nQ. What sign would we expect on \\(\\beta_1\\)?\n\n\nA.\n\\[\n\\beta_1 &lt; 0\n\\]"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#section-2",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#section-2",
    "title": "Multiple Variable Regression Analysis",
    "section": "",
    "text": "Smaller class sizes (X) increases test scores (Y)"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#ex.-effect-of-class-sizes-on-test-scores-3",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#ex.-effect-of-class-sizes-on-test-scores-3",
    "title": "Multiple Variable Regression Analysis",
    "section": "Ex. Effect of class sizes on test scores",
    "text": "Ex. Effect of class sizes on test scores\nEstimate effect of class size on test scores with the following:\n\\[\n\\text{Scores}_i = \\beta_0 + \\beta_1 \\text{Class Size}_i + u_i\n\\]\n\nQ. Do we think \\(\\beta_1\\) will be a good guess of the underlying population parameter?\n\nA. In \\(u_i\\), several variables are correlated with class size and test scores\nSuch as… school funding, which might affect:\n\n\n\n\n\nTextbooks\nComputers\n\n\n\nTeacher salary\nAttract high income parents"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#section-3",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#section-3",
    "title": "Multiple Variable Regression Analysis",
    "section": "",
    "text": "Smaller class sizes (X) increases test scores (Y)"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#section-4",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#section-4",
    "title": "Multiple Variable Regression Analysis",
    "section": "",
    "text": "Smaller class sizes (X) increases test scores (Y) along with greater school funding (U)"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#section-5",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#section-5",
    "title": "Multiple Variable Regression Analysis",
    "section": "",
    "text": "Smaller class sizes (X) increases test scores (Y) along with greater school funding (U). And, school funding (U) is correlated with test scores (X).\n\n\nAny unobserved variable that connects a backdoor path between class size (X) and test scores (Y) will bias our point estimate of \\(\\beta_1\\)."
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#section-6",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#section-6",
    "title": "Multiple Variable Regression Analysis",
    "section": "",
    "text": "Any unobserved variable that connects a backdoor path between class size (X) and test scores (Y) will bias our point estimate of \\(\\beta_1\\). Why?\n\nA1. Linearity\nA2. Sample Variation\nA3. Exogeniety\nA4. Homoskedasticity\nA5. Non-autocorrelation\nA6. Normality"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#section-7",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#section-7",
    "title": "Multiple Variable Regression Analysis",
    "section": "",
    "text": "Any unobserved variable that connects a backdoor path between class size (X) and test scores (Y) will bias our point estimate of \\(\\beta_1\\). Why?\n\nA1. Linearity\nA2. Sample Variation\nA3. Exogeniety: The \\(X\\) variable is exogenous\nA4. Homoskedasticity\nA5. Non-autocorrelation\nA6. Normality"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#section-8",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#section-8",
    "title": "Multiple Variable Regression Analysis",
    "section": "",
    "text": "Any unobserved variable that connects a backdoor path between class size (X) and test scores (Y) will bias our point estimate of \\(\\beta_1\\). Why?\n\nA. Because is violates the exogeniety assumption\n\\[\n\\mathop{\\mathbb{E}}\\left( u|\\text{Class Size} \\right) \\neq 0\n\\]\nCorrelation between class size and school funding (\\(u_i\\)) is not zero."
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#section-9",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#section-9",
    "title": "Multiple Variable Regression Analysis",
    "section": "",
    "text": "Graphically… Valid exogeniety, i.e., \\(\\mathop{\\mathbb{E}}\\left( u \\mid X \\right) = 0\\)"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#section-10",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#section-10",
    "title": "Multiple Variable Regression Analysis",
    "section": "",
    "text": "Graphically… Invalid exogeniety, i.e., \\(\\mathop{\\mathbb{E}}\\left( u \\mid X \\right) \\neq 0\\)"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#section-11",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#section-11",
    "title": "Multiple Variable Regression Analysis",
    "section": "",
    "text": "What the actual data looks like:"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#omitted-variable-bias-1",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#omitted-variable-bias-1",
    "title": "Multiple Variable Regression Analysis",
    "section": "Omitted variable bias",
    "text": "Omitted variable bias\n\nBias that occurs in statistical models when a relevant variable is not included in the model.\n\n\n\nConsequence: Leads to the incorrect estimation of the relationships between variables, which may affect the reliability of the model’s predictions and inferences.\n\n\n\nSolution: “Control” for the omitted variable(s)."
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#section-12",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#section-12",
    "title": "Multiple Variable Regression Analysis",
    "section": "",
    "text": "Class funding (U) confounds our estimates of smaller class sizes (X) on test scores (Y).  \n\nAny unobserved variable that connects a backdoor path between class size (X) and test scores (Y) will bias our point estimate of \\(\\beta_1\\)."
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#section-13",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#section-13",
    "title": "Multiple Variable Regression Analysis",
    "section": "",
    "text": "Class funding (U) confounds our estimates of smaller class sizes (X) on test scores (Y). Including data on school funding (U) in a multiple linear regression allows us to close this backdoor path.\n\n\nWith all backdoor paths closed, point estimates of \\(\\beta_1\\) will no longer be biased and will return the population parameter of interest"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#omitted-variable-bias-2",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#omitted-variable-bias-2",
    "title": "Multiple Variable Regression Analysis",
    "section": "Omitted Variable Bias",
    "text": "Omitted Variable Bias\nImagine we have a population model of the form:\n\\[\nY_i = \\beta_0 + \\beta_1 X_i + \\beta_2 Z_i + u_i\n\\]\nwhere \\(Z_i\\) is a relevant variable that is omitted from the model.\nand suppose we estimate the following model:\n\\[\nY_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i + v_i\n\\]\nwhere \\(v_i\\) is the new error term that absorbs the effect of \\(Z_i\\)"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#omitted-variable-bias-3",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#omitted-variable-bias-3",
    "title": "Multiple Variable Regression Analysis",
    "section": "Omitted Variable Bias",
    "text": "Omitted Variable Bias\nTo derive the bias of \\(\\hat{\\beta}_1\\), we need to understand the relationship between \\(Z_i\\) and \\(X_i\\). Assume that:\n\\[\nZ_i = \\gamma_0 + \\gamma_1 X_i + \\varepsilon_i\n\\]\nwhere \\(\\varepsilon_i\\) is the part of \\(Z_i\\) that is uncorrelated with \\(X_i\\)\n\n\nIf we substitute \\(Z_i\\) into the population model, we get:\n\\[\n\\begin{align*}\nY_i &= \\beta_0 + \\beta_1 X_i + \\beta_2 \\left( \\gamma_0 + \\gamma_1 X_i + \\varepsilon_i \\right) + u_i \\\\\n    &= \\beta_0 + \\beta_2 \\gamma_0 + \\left( \\beta_1 + \\beta_2 \\gamma_1 \\right) X_i + \\beta_2 \\varepsilon_i + u_i\n\\end{align*}\n\\]"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#omitted-variable-bias-4",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#omitted-variable-bias-4",
    "title": "Multiple Variable Regression Analysis",
    "section": "Omitted Variable Bias",
    "text": "Omitted Variable Bias\nWe can rewrite this expression:\n\\[\n\\begin{align*}\nY_i &= \\beta_0 + \\beta_1 X_i + \\beta_2 \\left( \\gamma_0 + \\gamma_1 X_i + \\varepsilon_i \\right) + u_i \\\\\n    &= \\beta_0 + \\beta_2 \\gamma_0 + \\left( \\beta_1 + \\beta_2 \\gamma_1 \\right) X_i + \\beta_2 \\varepsilon_i + u_i\n\\end{align*}\n\\]\nas:\n\\[\nY_i = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 X_i + v_i\n\\]\nwhere:\n\n\\(\\widehat{\\beta}_0 = \\beta_0 + \\beta_2 \\gamma_0\\)\n\\(\\widehat{\\beta}_1 = \\beta_1 + \\beta_2 \\gamma_1\\)\n\\(v_i = \\beta_2 \\varepsilon_i + u_i\\)\n\nThus, we can see how \\(Z_i\\) will bias our estimate of \\(\\beta_1\\)"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#omitted-variable-bias-5",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#omitted-variable-bias-5",
    "title": "Multiple Variable Regression Analysis",
    "section": "Omitted Variable Bias",
    "text": "Omitted Variable Bias\nRecall that we define the bias of an estimator as:\n\\[\n\\mathop{\\text{Bias}}_\\theta \\left( W \\right) = \\mathop{\\boldsymbol{E}}\\left[ W \\right] - \\theta\n\\]\n\nThe bias of the estimator \\(\\hat{\\beta}_1\\) is given by:\n\\[\n\\begin{align*}\n\\mathop{\\text{Bias}}_{\\beta_1} \\left( \\hat{\\beta}_1 \\right) &= \\mathop{\\boldsymbol{E}}\\left[ \\hat{\\beta}_1 \\right] - \\beta_1 \\\\\n&= \\mathop{\\boldsymbol{E}}\\left[ \\beta_1 + \\beta_2 \\gamma_1 \\right] - \\beta_1 \\\\\n&= \\beta_2 \\gamma_1\n\\end{align*}\n\\]"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#omitted-variable-bias-6",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#omitted-variable-bias-6",
    "title": "Multiple Variable Regression Analysis",
    "section": "Omitted Variable Bias",
    "text": "Omitted Variable Bias\nFinally, we can write the bias of \\(\\hat{\\beta}_1\\) in terms of the correlation between \\(X_i\\) and \\(Z_i\\):\n\\[\n\\gamma_1 = \\frac{\\text{Cov}\\left( X_i, Z_i \\right)}{\\text{Var}\\left( X_i \\right)}\n\\]\n\nTherefore, we can write the bias of \\(\\hat{\\beta}_1\\) as:\n\\[\n\\mathop{\\text{Bias}}_{\\beta_1} \\left( \\hat{\\beta}_1 \\right) = \\beta_2 \\frac{\\text{Cov}\\left( X_i, Z_i \\right)}{\\text{Var}\\left( X_i \\right)}\n\\]"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#signing-the-bias",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#signing-the-bias",
    "title": "Multiple Variable Regression Analysis",
    "section": "Signing the Bias",
    "text": "Signing the Bias\nSometimes we’re stuck with omitted variable bias.\n\\[\n\\mathop{\\boldsymbol{E}} \\left[ \\hat{\\beta}_1 \\right] = \\beta_1 + \\beta_2 \\dfrac{ \\mathop{\\text{Cov}} \\left( X_i,\\, Z_i \\right)}{\\mathop{\\text{Var}} \\left( X_i \\right)}\n\\]\nWhen this happens, we can often at least know the direction of the bias."
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#signing-the-bias-1",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#signing-the-bias-1",
    "title": "Multiple Variable Regression Analysis",
    "section": "Signing the Bias",
    "text": "Signing the Bias\nBegin with\n\\[\n\\mathop{\\boldsymbol{E}} \\left[ \\hat{\\beta}_1 \\right] = \\beta_1 + \\beta_2 \\dfrac{ \\mathop{\\text{Cov}} \\left( X_i,\\, Z_i \\right)}{\\mathop{\\text{Var}} \\left( X_i \\right)}\n\\]\nWe know \\(\\color{#8FBCBB}{\\mathop{\\text{Var}} \\left( X_i \\right) &gt; 0}\\). Suppose \\(\\color{#81A1C1}{\\beta_2 &gt; 0}\\) and \\(\\color{#EBCB8B}{\\mathop{\\text{Cov}} \\left( X_i,\\,Z_i \\right) &gt; 0}\\). Then\n\n\\[\n\\begin{align}\n\\mathop{\\boldsymbol{E}} \\left[ \\hat{\\beta}_1 \\right] = \\beta_1 + \\color{#81A1C1}{(+)} \\dfrac{\\color{#EBCB8B}{(+)}}{\\color{#8FBCBB}{(+)}} \\implies \\mathop{\\boldsymbol{E}} \\left[ \\hat{\\beta}_1 \\right] &gt; \\beta_1\n\\end{align}\n\\] ∴ In this case, OLS is biased upward (estimates are too large).\n\n\n\\[\n\\begin{matrix}\n\\enspace & \\color{#EBCB8B}{\\text{Cov}(X_i,\\,Z_i)&gt; 0} & \\color{#EBCB8B}{\\text{Cov}(X_i,\\,Z_i)&lt; 0} \\\\\n\\color{#81A1C1}{\\beta_2 &gt; 0} & \\text{Upward} &  \\\\\n\\color{#81A1C1}{\\beta_2 &lt; 0} &  &\n\\end{matrix}\n\\]"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#signing-the-bias-2",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#signing-the-bias-2",
    "title": "Multiple Variable Regression Analysis",
    "section": "Signing the Bias",
    "text": "Signing the Bias\nBegin with\n\\[\n\\mathop{\\boldsymbol{E}} \\left[ \\hat{\\beta}_1 \\right] = \\beta_1 + \\beta_2 \\dfrac{ \\mathop{\\text{Cov}} \\left( X_i,\\, Z_i \\right)}{\\mathop{\\text{Var}} \\left( X_i \\right)}\n\\]\nWe know \\(\\color{#8FBCBB}{\\mathop{\\text{Var}} \\left( X_i \\right) &gt; 0}\\). Suppose \\(\\color{#81A1C1}{\\beta_2 &lt; 0}\\) and \\(\\color{#EBCB8B}{\\mathop{\\text{Cov}} \\left( X_i,\\,Z_i \\right) &gt; 0}\\). Then\n\n\\[\n\\begin{align}\n\\mathop{\\boldsymbol{E}} \\left[ \\hat{\\beta}_1 \\right] = \\beta_1 + \\color{#81A1C1}{(-)} \\dfrac{\\color{#EBCB8B}{(+)}}{\\color{#8FBCBB}{(+)}} \\implies \\mathop{\\boldsymbol{E}} \\left[ \\hat{\\beta}_1 \\right] &lt; \\beta_1\n\\end{align}\n\\] ∴ In this case, OLS is biased downward (estimates are too small).\n\\[\n\\begin{matrix}\n\\enspace & \\color{#EBCB8B}{\\text{Cov}(X_i,\\,Z_i)&gt; 0} & \\color{#EBCB8B}{\\text{Cov}(X_i,\\,Z_i)&lt; 0} \\\\\n\\color{#81A1C1}{\\beta_2 &gt; 0} & \\text{Upward} &  \\\\\n\\color{#81A1C1}{\\beta_2 &lt; 0} & \\text{Downward} &\n\\end{matrix}\n\\]"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#signing-the-bias-3",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#signing-the-bias-3",
    "title": "Multiple Variable Regression Analysis",
    "section": "Signing the Bias",
    "text": "Signing the Bias\nBegin with\n\\[\n\\mathop{\\boldsymbol{E}} \\left[ \\hat{\\beta}_1 \\right] = \\beta_1 + \\beta_2 \\dfrac{ \\mathop{\\text{Cov}} \\left( X_i,\\, Z_i \\right)}{\\mathop{\\text{Var}} \\left( X_i \\right)}\n\\]\nWe know \\(\\color{#8FBCBB}{\\mathop{\\text{Var}} \\left( X_i \\right) &gt; 0}\\). Suppose \\(\\color{#81A1C1}{\\beta_2 &gt; 0}\\) and \\(\\color{#EBCB8B}{\\mathop{\\text{Cov}} \\left( X_i,\\,Z_i \\right) &lt; 0}\\). Then\n\\[\n\\begin{align}\n\\mathop{\\boldsymbol{E}} \\left[ \\hat{\\beta}_1 \\right] = \\beta_1 + \\color{#81A1C1}{(+)} \\dfrac{\\color{#EBCB8B}{(-)}}{\\color{#8FBCBB}{(+)}} \\implies \\mathop{\\boldsymbol{E}} \\left[ \\hat{\\beta}_1 \\right] &lt; \\beta_1\n\\end{align}\n\\] ∴ In this case, OLS is biased downward (estimates are too small).\n\\[\n\\begin{matrix}\n\\enspace & \\color{#EBCB8B}{\\text{Cov}(X_i,\\,Z_i)&gt; 0} & \\color{#EBCB8B}{\\text{Cov}(X_i,\\,Z_i)&lt; 0} \\\\\n\\color{#81A1C1}{\\beta_2 &gt; 0} & \\text{Upward} & \\text{Downward} \\\\\n\\color{#81A1C1}{\\beta_2 &lt; 0} & \\text{Downward} &\n\\end{matrix}\n\\]"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#signing-the-bias-4",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#signing-the-bias-4",
    "title": "Multiple Variable Regression Analysis",
    "section": "Signing the Bias",
    "text": "Signing the Bias\nBegin with\n\\[\n\\mathop{\\boldsymbol{E}} \\left[ \\hat{\\beta}_1 \\right] = \\beta_1 + \\beta_2 \\dfrac{ \\mathop{\\text{Cov}} \\left( X_i,\\, Z_i \\right)}{\\mathop{\\text{Var}} \\left( X_i \\right)}\n\\]\nWe know \\(\\color{#8FBCBB}{\\mathop{\\text{Var}} \\left( X_i \\right) &gt; 0}\\). Suppose \\(\\color{#81A1C1}{\\beta_2 &lt; 0}\\) and \\(\\color{#EBCB8B}{\\mathop{\\text{Cov}} \\left( X_i,\\,Z_i \\right) &lt; 0}\\). Then\n\\[\n\\begin{align}\n\\mathop{\\boldsymbol{E}} \\left[ \\hat{\\beta}_1 \\right] = \\beta_1 + \\color{#81A1C1}{(-)} \\dfrac{\\color{#EBCB8B}{(-)}}{\\color{#8FBCBB}{(+)}} \\implies \\mathop{\\boldsymbol{E}} \\left[ \\hat{\\beta}_1 \\right] &gt; \\beta_1\n\\end{align}\n\\] ∴ In this case, OLS is biased upward (estimates are too large).\n\\[\n\\begin{matrix}\n\\enspace & \\color{#EBCB8B}{\\text{Cov}(X_i,\\,Z_i)&gt; 0} & \\color{#EBCB8B}{\\text{Cov}(X_i,\\,Z_i)&lt; 0} \\\\\n\\color{#81A1C1}{\\beta_2 &gt; 0} & \\text{Upward} & \\text{Downward} \\\\\n\\color{#81A1C1}{\\beta_2 &lt; 0} & \\text{Downward} & \\text{Upward}\n\\end{matrix}\n\\]"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#signing-the-bias-5",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#signing-the-bias-5",
    "title": "Multiple Variable Regression Analysis",
    "section": "Signing the Bias",
    "text": "Signing the Bias\nThus, in cases where we have a sense of\n\nthe sign of \\(\\mathop{\\text{Cov}} \\left( X_i,\\,Z_i \\right)\\)\nthe sign of \\(\\beta_2\\)\n\nwe know in which direction bias pushes our estimates.\nDirection of Bias\n\\[\n\\begin{matrix}\n\\enspace & \\color{#EBCB8B}{\\text{Cov}(X_i,\\,Z_i)&gt; 0} & \\color{#EBCB8B}{\\text{Cov}(X_i,\\,Z_i)&lt; 0} \\\\\n\\color{#81A1C1}{\\beta_2 &gt; 0} & \\text{Upward} & \\text{Downward} \\\\\n\\color{#81A1C1}{\\beta_2 &lt; 0} & \\text{Downward} & \\text{Upward}\n\\end{matrix}\n\\]"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#multiple-linear-regression-1",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#multiple-linear-regression-1",
    "title": "Multiple Variable Regression Analysis",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\nSimple linear regression features one dependent variable and one independent variable:\n\\[\n\\color{#434C5E}{Y_i} = \\beta_0 + \\beta_1 \\color{\"#81A1C1\"}{X_i} + u_i\n\\]\nMultiple linear regression features one dependent variable and multiple independent variables:\n\\[\n\\color{#434C5E}{Y_i} = \\beta_0 + \\beta_1 \\color{\"#81A1C1\"}{X_{1i}} + \\beta_2 \\color{\"#81A1C1\"}{X_{2i}} + \\cdots + \\beta_{k} \\color{\"#81A1C1\"}{X_{ki}} + u_i\n\\]\n\nThis serves more than one purpose. Multiple independent variables improves predictions, avoids OVB, and better explains variation in \\(Y\\)."
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#simple-linear-regression",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#simple-linear-regression",
    "title": "Multiple Variable Regression Analysis",
    "section": "Simple Linear Regression",
    "text": "Simple Linear Regression"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#add-dimension-rightarrow-per-student-expenditure",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#add-dimension-rightarrow-per-student-expenditure",
    "title": "Multiple Variable Regression Analysis",
    "section": "Add Dimension \\(\\rightarrow\\) Per Student Expenditure",
    "text": "Add Dimension \\(\\rightarrow\\) Per Student Expenditure"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#multiple-linear-regression-ex.",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#multiple-linear-regression-ex.",
    "title": "Multiple Variable Regression Analysis",
    "section": "Multiple Linear Regression Ex.",
    "text": "Multiple Linear Regression Ex.\nIf we ignore per student expenditure (aka our original simple regression)\n\\[\n\\text{Scores}_i = \\beta_0 + \\beta_1 \\text{Class Size}_i + u_i\n\\]"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#multiple-linear-regression-ex.-1",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#multiple-linear-regression-ex.-1",
    "title": "Multiple Variable Regression Analysis",
    "section": "Multiple Linear Regression Ex.",
    "text": "Multiple Linear Regression Ex.\nControlling for school funding\n\\[\n\\text{Scores}_i = \\beta_0 + \\beta_1 \\text{Class Size}_i + \\text{Expenditure}_i+ u_i\n\\]"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#ols-estimation",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#ols-estimation",
    "title": "Multiple Variable Regression Analysis",
    "section": "OLS Estimation",
    "text": "OLS Estimation\nResiduals are now defined as:\n\n\\[\n\\hat{u}_i = Y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 X_{1i} - \\hat{\\beta}_2 X_{2i} - \\cdots - \\hat{\\beta}_{k} X_{ki}\n\\]\n\n\nAs with SLR, OLS minimizes the sum of squared residuals (RSS).\n\n\n\\[\n\\begin{align*}\n  \\color{#D08770}{RSS} &= \\sum_{i = 1}^{n} (Y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 X_{1i} - \\hat{\\beta}_2 X_{2i} - \\cdots - \\hat{\\beta}_{k} X_{ki})^2 \\\\\n                        &= \\color{#D08770}{\\sum_{i=1}^n \\hat{u}_i^2}\n\\end{align*}\n\\]\nwhich is a familiar expression."
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#ols-estimation-1",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#ols-estimation-1",
    "title": "Multiple Variable Regression Analysis",
    "section": "OLS Estimation",
    "text": "OLS Estimation\nTo obtain point estimates:\n\\[\n\\min_{\\hat{\\beta}_0,\\, \\hat{\\beta}_1,\\, \\dots \\, \\hat{\\beta}_k} \\quad \\color{#D08770}{\\sum_{i=1}^n \\hat{u}_i^2}\n\\]\n\nTake partial derivatives of RSS with respect to each \\(\\hat{\\beta}\\)\nSet each derivative equal to zero\nSolve the system of \\(k+1\\) equations1.\n\n\nThe algebra is cumbersome. We let R do the heavy lifting.\n\n\\(k+1\\) due to the intercept."
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#coefficient-interpretation",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#coefficient-interpretation",
    "title": "Multiple Variable Regression Analysis",
    "section": "Coefficient Interpretation",
    "text": "Coefficient Interpretation\nModel\n\\[\n\\color{}{Y_i} = \\beta_0 + \\beta_1 \\color{}{X_{1i}} + \\beta_2 \\color{}{X_{2i}} + \\cdots + \\beta_{k} \\color{}{X_{ki}} + u_i\n\\]\nInterpretation\n\nThe intercept \\(\\hat{\\beta}_0\\) is the average value of \\(Y_i\\) when all of the independent variables are equal to zero.\nSlope parameters \\(\\hat{\\beta}_1, \\dots, \\hat{\\beta}_{k}\\) give us the change in \\(Y_i\\) from a one-unit change in \\(X_j\\), holding the other \\(X\\) variables constant."
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#algebraic-properties-of-ols",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#algebraic-properties-of-ols",
    "title": "Multiple Variable Regression Analysis",
    "section": "Algebraic properties of OLS",
    "text": "Algebraic properties of OLS\nThe OLS first-order conditions yield the same properties as before.\n\n1. Residuals sum to zero: \\(\\sum_{i=1}^n \\hat{u_i} = 0\\).\n2. The sample covariance \\(X_i\\) and the \\(\\hat{u_i}\\) is zero.\n3. The point \\((\\bar{X_1}, \\bar{X_2}, \\dots, \\bar{X_k}, \\bar{Y})\\) is on the fitted regression “line.”"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#goodness-of-fit",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#goodness-of-fit",
    "title": "Multiple Variable Regression Analysis",
    "section": "Goodness of fit",
    "text": "Goodness of fit\nFitted values are defined similarly:\n\\[\n\\hat{Y_i} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_{1i} + \\hat{\\beta}_2 X_{2i} + \\cdots + \\hat{\\beta}_{k} X_{ki}\n\\]\nThe formula for \\(R^2\\) is the same as before:\n\\[\nR^2 =\\frac{\\sum(\\hat{Y_i}-\\bar{Y})^2}{\\sum(Y_i-\\bar{Y})^2}\n\\]"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#goodness-of-fit-1",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#goodness-of-fit-1",
    "title": "Multiple Variable Regression Analysis",
    "section": "Goodness of fit",
    "text": "Goodness of fit\nWe can describe the variation explain in \\(Y\\) with venn diagrams"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#goodness-of-fit-2",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#goodness-of-fit-2",
    "title": "Multiple Variable Regression Analysis",
    "section": "Goodness of fit",
    "text": "Goodness of fit\nWe can describe the variation explain in \\(Y\\) with venn diagrams\n\n\n\n\n\n\n\n\n\nAs we add more variables, we are able to explain more “chunks” of the variation in \\(y\\)"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#section-14",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#section-14",
    "title": "Multiple Variable Regression Analysis",
    "section": "",
    "text": "Problem: As we add variables to our model, \\(R^2\\) mechanically increases.\n\nLet me show you this problem with a simulation"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#section-15",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#section-15",
    "title": "Multiple Variable Regression Analysis",
    "section": "",
    "text": "Simulate a dataset of 10,000 observations on \\(y\\) and 1,000 random \\(x_k\\) variables, where\n\\[\ny \\perp x_k \\quad \\forall x_k \\; \\text{s.t.} \\; k = 1, 2, \\dots, 1000\n\\]\n\nWe have 1,000 independent variables that are independent to the dependent variable. Each \\(x_k\\) has no relationship to \\(y\\) whatsoever."
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#section-16",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#section-16",
    "title": "Multiple Variable Regression Analysis",
    "section": "",
    "text": "Problem: As we add variables to our model, \\(R^2\\) mechanically increases."
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#section-17",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#section-17",
    "title": "Multiple Variable Regression Analysis",
    "section": "",
    "text": "Problem: As we add variables to our model, \\(R^2\\) mechanically increases.\nOne solution: Penalize for the number of variables, e.g., adjusted \\(R^2\\):"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#goodness-of-fit-3",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#goodness-of-fit-3",
    "title": "Multiple Variable Regression Analysis",
    "section": "Goodness of fit",
    "text": "Goodness of fit\nProblem: As we add variables to our model, \\(R^2\\) mechanically increases.\nOne solution: Penalize for the number of variables, e.g., adjusted \\(R^2\\):\n\\[\n\\bar{R}^2 = 1 - \\dfrac{\\sum_i \\left( Y_i - \\hat{Y}_i \\right)^2/(n-k-1)}{\\sum_i \\left( Y_i - \\bar{Y} \\right)^2/(n-1)}\n\\]\n\nNote: Adjusted \\(R^2\\) need not be between 0 and 1."
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#multiple-regression",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#multiple-regression",
    "title": "Multiple Variable Regression Analysis",
    "section": "Multiple regression",
    "text": "Multiple regression\nThere are tradeoffs to remember as we add/remove variables:\n\n\nFewer variables\n\nExplains less variation in \\(y\\)\nProvide simple interpretations and visualizations\nMore worried about omitted-variable bias\n\n\nMore variables\n\nMore likely to find spurious relationships1\nMore difficult interpretation\nThe variance of our point estimates will be bigger\nWe still might have omitted-variable bias\n\n\nSpurious meaning statistically significant by coincidence—not reflective of true, population-level relationship"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#multiple-regression-1",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#multiple-regression-1",
    "title": "Multiple Variable Regression Analysis",
    "section": "Multiple regression",
    "text": "Multiple regression\nThere are tradeoffs to remember as we add/remove variables:\n\n\nFewer variables\n\nExplains less variation in \\(y\\)\nProvide simple interpretations and visualizations\nMore worried about omitted-variable bias\n\n\nMore variables\n\nMore likely to find spurious relationships1\nMore difficult interpretation\nThe variance of our point estimates will be bigger\nWe still might have omitted-variable bias\n\n\nSpurious meaning statistically significant by coincidence—not reflective of true, population-level relationship"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#ols-variances",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#ols-variances",
    "title": "Multiple Variable Regression Analysis",
    "section": "OLS variances",
    "text": "OLS variances\nMultiple regression model:\n\\[\nY_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\cdots + \\beta_{k} X_{ki} + u_i\n\\]\nIt can be shown that the estimator \\(\\hat{\\beta}_j\\) on independent variable \\(X_j\\) is:\n\\[\n\\mathop{\\text{Var}} \\left( \\hat{\\beta_j} \\right) = \\dfrac{\\sigma^2}{\\left( 1 - R^2_j \\right)\\sum_{i=1}^n \\left( X_{ji} - \\bar{X}_j \\right)^2},\n\\]\nwhere \\(R^2_j\\) is the \\(R^2\\) from a regression of \\(X_j\\) on the other independent variables and the intercept"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#ols-variances-1",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#ols-variances-1",
    "title": "Multiple Variable Regression Analysis",
    "section": "OLS variances",
    "text": "OLS variances\n\\[\n\\mathop{\\text{Var}} \\left( \\hat{\\beta_j} \\right) = \\dfrac{{\\color{#81A1C1}\\sigma^2}}{\\left( 1 - \\color{#81A1C1}{R_j^2} \\right)\\color{#BF616A}{\\sum_{i=1}^n \\left( X_{ji} - \\bar{X}_j \\right)^2}},\n\\]\nMoving parts:\n\n1. Error variance: As \\(\\color{#81A1C1}{\\sigma^2}\\) increases, \\(Var(\\hat{\\beta}_j)\\) increases\n2. Total variation in \\(X_j\\): As \\(\\color{#BF616A}{\\sum_{i=1}^n \\left( X_{ji} - \\bar{X}_j \\right)^2}\\) increases, \\(Var(\\hat{\\beta}_j)\\) decreases\n3. Relationship across \\(X_i\\): As \\(\\color{#81A1C1}{R_j^2}\\) increases, \\(Var(\\hat{\\beta}_j)\\) increases\n\n\n\n3. is better known as Multicollinearity"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#multicollinearity-1",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#multicollinearity-1",
    "title": "Multiple Variable Regression Analysis",
    "section": "Multicollinearity",
    "text": "Multicollinearity\n\nCase in which two or more independent variables in a regression model are highly correlated.\n\n\n\nOne independent variable can predict most of the variation in another independent variable.\n\n\n\nMulticollinearity leads to imprecise estimates. Becomes difficult to distinguish between individual effects from of independent variables."
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#ols-assumptions",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#ols-assumptions",
    "title": "Multiple Variable Regression Analysis",
    "section": "OLS Assumptions",
    "text": "OLS Assumptions\nClassical assumptions for OLS change slightly for multiple OLS\nA1. Linearity: The population relationship is linear in parameters with an additive error term.\nA2. Sample Variation: No \\(X\\) variable is a perfect linear combination of the others\nA3. Exogeniety: The \\(X\\) variable is exogenous\nA4. Homoskedasticity: The error term has the same variance for each value of the independent variable\nA5. Non-autocorrelation: The values of error terms have independent distributions"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#perfect-collinearity",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#perfect-collinearity",
    "title": "Multiple Variable Regression Analysis",
    "section": "Perfect Collinearity",
    "text": "Perfect Collinearity\n\nCase in which two or more independent variables in a regression model are perfectly correlated.\n\nEx. 2016 Election\nOLS simultaneously cannot estimate parameters for white and nonwhite.\n\n\n\n\n\n\nR drops perfectly collinear variables for you."
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#multicollinearity-ex.",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#multicollinearity-ex.",
    "title": "Multiple Variable Regression Analysis",
    "section": "Multicollinearity Ex.",
    "text": "Multicollinearity Ex.\nSuppose that we want to understand the relationship between crime rates and poverty rates in US cities. We could estimate the model\n\\[\n\\text{Crime}_i = \\beta_0 + \\beta_1 \\text{Poverty}_i + \\beta_2 \\text{Income}_i + u_i\n\\]\n\nBefore obtaining standard errors, we need:\n\\[\n\\mathop{\\text{Var}} \\left( \\hat{\\beta}_1 \\right) = \\dfrac{\\sigma^2}{\\left( 1 - R^2_1 \\right)\\sum_{i=1}^n \\left( \\text{Poverty}_{i} - \\overline{\\text{Poverty}} \\right)^2}\n\\]\n\n\n\\(R^2_1\\) is the \\(R^2\\) from a regression of poverty on median income:\n\\[\n\\text{Poverty}_i = \\gamma_0 + \\gamma_1 \\text{Income}_i + v_i\n\\]"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#multicollinearity-2",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#multicollinearity-2",
    "title": "Multiple Variable Regression Analysis",
    "section": "Multicollinearity",
    "text": "Multicollinearity\nScenario 1: \\(\\text{Income}_i\\) explains most variation in \\(\\text{Poverty}_i\\), then \\(R^2_1 \\rightarrow 1\\)\n\nViolates the no perfect collinearity assumption\n\n\nScenario 2: If \\(\\text{Income}_i\\) explains no variation in \\(\\text{Poverty}_i\\), then \\(R^2_1 = 0\\)\n\n\nQ. In which scenario is the variance of the poverty coefficient smaller?\n\\[\n\\mathop{\\text{Var}} \\left( \\hat{\\beta}_1 \\right) = \\dfrac{\\sigma^2}{\\left( 1 - R^2_1 \\right)\\sum_{i=1}^n \\left( \\text{Poverty}_{i} - \\overline{\\text{Poverty}} \\right)^2}\n\\]\n\n\nA. Scenario 2."
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#multicollinearity-3",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#multicollinearity-3",
    "title": "Multiple Variable Regression Analysis",
    "section": "Multicollinearity",
    "text": "Multicollinearity\nAs the relationships between the variables increase, \\(R^2_j\\) increases.\nFor high \\(R^2_j\\), \\(\\mathop{\\text{Var}} \\left( \\hat{\\beta_j} \\right)\\) is large:\n\\[\n\\mathop{\\text{Var}} \\left( \\hat{\\beta_j} \\right) = \\dfrac{\\sigma^2}{\\left( 1 - R^2_j \\right)\\sum_{i=1}^n \\left( X_{ji} - \\bar{X}_j \\right)^2}\n\\]\n\n\nSome view multicollinearity as a “problem” to be solved.\nEither increase power (\\(n\\)) or drop correlated variables\nWarning: Dropping variables can generate omitted variable bias."
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#irrelevant-variables",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#irrelevant-variables",
    "title": "Multiple Variable Regression Analysis",
    "section": "Irrelevant Variables",
    "text": "Irrelevant Variables\nSuppose that the true relationship between birth weight and in utero exposure to toxic air pollution is\n\\[\n(\\text{Birth Weight})_i = \\beta_0 + \\beta_1 \\text{Pollution}_i + u_i\n\\]\n\nSuppose that an “analyst” estimates\n\\[\n(\\text{Birth Weight})_i = \\tilde{\\beta_0} + \\tilde{\\beta_1} \\text{Pollution}_i + \\tilde{\\beta_2}\\text{NBA}_i + u_i\n\\]\n\n\nOne can show that \\(\\mathop{\\mathbb{E}} \\left( \\hat{\\tilde{\\beta_1}} \\right) = \\beta_1\\) (i.e., \\(\\hat{\\tilde{\\beta_1}}\\) is unbiased).\nHowever, the variances of \\(\\hat{\\tilde{\\beta_1}}\\) and \\(\\hat{\\beta_1}\\) differ."
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#irrelevant-variables-1",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#irrelevant-variables-1",
    "title": "Multiple Variable Regression Analysis",
    "section": "Irrelevant Variables",
    "text": "Irrelevant Variables\n\nWe can reasonably say that the NBA has no direct impact on birth weight, so it is doing more damage to the model than helping"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#irrelevant-variables-2",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#irrelevant-variables-2",
    "title": "Multiple Variable Regression Analysis",
    "section": "Irrelevant Variables",
    "text": "Irrelevant Variables\nThe variance of \\(\\hat{\\beta}_1\\) from estimating the “true model” is\n\\[\n\\mathop{\\text{Var}} \\left( \\hat{\\beta_1} \\right) = \\dfrac{\\sigma^2}{\\sum_{i=1}^n \\left( \\text{Pollution}_{i} - \\overline{\\text{Pollution}} \\right)^2}\n\\]\nThe variance of \\(\\hat{\\tilde\\beta}_1\\) from estimating the model with the irrelevant variable is\n\\[\n\\mathop{\\text{Var}} \\left( \\hat{\\tilde{\\beta_1}} \\right) = \\dfrac{\\sigma^2}{\\left( 1 - R^2_1 \\right)\\sum_{i=1}^n \\left( \\text{Pollution}_{i} - \\overline{\\text{Pollution}} \\right)^2}\n\\]"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#irrelevant-variables-3",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#irrelevant-variables-3",
    "title": "Multiple Variable Regression Analysis",
    "section": "Irrelevant Variables",
    "text": "Irrelevant Variables\nNotice that \\(\\mathop{\\text{Var}} \\left( \\hat{\\beta_1} \\right) \\leq \\mathop{\\text{Var}} \\left( \\hat{\\tilde{\\beta_1}} \\right)\\) since,\n\\[\n\\sum_{i=1}^n \\left( \\text{Poll.}_{i} - \\overline{\\text{Poll.}} \\right)^2\n\\geq\n\\left( 1 - R^2_1 \\right)\\sum_{i=1}^n \\left( \\text{Poll.}_{i} - \\overline{\\text{Poll.}} \\right)^2\n\\]\n\n\nA tradeoff exists when including more control variables. Make sure you have good reason for your controls because including irrelevant control variables increase variances"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#estimating-error-variance",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#estimating-error-variance",
    "title": "Multiple Variable Regression Analysis",
    "section": "Estimating Error Variance",
    "text": "Estimating Error Variance\nWe cannot observe \\(\\sigma^2\\), so we must estimate it using the residuals from an estimated regression:\n\\[\ns_u^2 = \\dfrac{\\sum_{i=1}^n \\hat{u}_i^2}{n - k - 1}\n\\]\n\n\\(k+1\\) is the number of parameters (one “slope” for each \\(X\\) variable and an intercept).\n\\(n - k - 1\\) = degrees of freedom.\nUsing the first 5 OLS assumptions, one can prove that \\(s_u^2\\) is an unbiased estimator of \\(\\sigma^2\\)."
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#standard-errors",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#standard-errors",
    "title": "Multiple Variable Regression Analysis",
    "section": "Standard Errors",
    "text": "Standard Errors\nThe formula for the standard error is the square root of \\(\\mathop{\\text{Var}} \\left( \\hat{\\beta_j} \\right)\\):\n\\[\n\\mathop{\\text{SE}}(\\hat{\\beta_j}) = \\sqrt{ \\frac{s^2_u}{(  1 - R^2_j ) \\sum_{i=1}^n ( X_{ji} - \\bar{X}_j )^2} }\n\\]"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#f-tests-1",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#f-tests-1",
    "title": "Multiple Variable Regression Analysis",
    "section": "F Tests",
    "text": "F Tests\nt tests allow us to test simple hypotheses involving a single parameter.\n\ne.g., \\(\\beta_1 = 0\\) or \\(\\beta_2 = 1\\).\n\n\nF tests allow us to test hypotheses that involve multiple parameters.\n\ne.g., \\(\\beta_1 = \\beta_2\\) or \\(\\beta_3 + \\beta_4 = 1\\)."
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#f-tests-2",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#f-tests-2",
    "title": "Multiple Variable Regression Analysis",
    "section": "F Tests",
    "text": "F Tests\nEx. Is money “fungible”?\nEconomists often say that “money is fungible.”\nWe might want to test whether money received as income actually has the same effect on consumption as money received from tax credits.\n\\[\n\\text{Consumption}_i = \\beta_0 + \\beta_1 \\text{Income}_{i} + \\beta_2 \\text{Credit}_i + u_i\n\\]"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#f-tests-3",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#f-tests-3",
    "title": "Multiple Variable Regression Analysis",
    "section": "F Tests",
    "text": "F Tests\nEx. Is money “fungible”?\nWe can write our null hypothesis as\n\\[\nH_0:\\: \\beta_1 = \\beta_2 \\iff H_0 :\\: \\beta_1 - \\beta_2 = 0\n\\]\nImposing the null hypothesis gives us a restricted model\n\\[\n\\text{Consumption}_i = \\beta_0 + \\beta_1 \\text{Income}_{i} + \\beta_1 \\text{Credit}_i + u_i\n\\]\n\\[\n\\text{Consumption}_i = \\beta_0 + \\beta_1 \\left( \\text{Income}_{i} + \\text{Credit}_i \\right) + u_i\n\\]"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#f-tests-4",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#f-tests-4",
    "title": "Multiple Variable Regression Analysis",
    "section": "F Tests",
    "text": "F Tests\nEx. Is money “fungible”?\nTo test the null hypothesis \\(H_o :\\: \\beta_1 = \\beta_2\\) against \\(H_a :\\: \\beta_1 \\neq \\beta_2\\), we use the \\(F\\) statistic:\n\\[\n\\begin{align}\n  F_{q,\\,n-k-1} = \\dfrac{\\left(\\text{RSS}_r - \\text{RSS}_u\\right)/q}{\\text{RSS}_u/(n-k-1)}\n\\end{align}\n\\]\nwhich (as its name suggests) follows the \\(F\\) distribution with \\(q\\) numerator degrees of freedom and \\(n-k-1\\) denominator degrees of freedom.\nHere, \\(q\\) is the number of restrictions we impose via \\(H_0\\)."
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#f-tests-5",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#f-tests-5",
    "title": "Multiple Variable Regression Analysis",
    "section": "F Tests",
    "text": "F Tests\nEx. Is money “fungible”?\nThe term \\(\\text{RSS}_r\\) is the sum of squared residuals (RSS) from our restricted model\n\\[\n\\text{Consumption}_i = \\beta_0 + \\beta_1 \\left( \\text{Income}_{i} + \\text{Credit}_i \\right) + u_i\n\\]\nand \\(\\text{RSS}_u\\) is the sum of squared residuals (RSS) from our unrestricted model\n\\[\n\\text{Consumption}_i = \\beta_0 + \\beta_1 \\text{Income}_{i} + \\beta_2 \\text{Credit}_i + u_i\n\\]"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#f-tests-6",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#f-tests-6",
    "title": "Multiple Variable Regression Analysis",
    "section": "F Tests",
    "text": "F Tests\nFinally, we compare our \\(F\\)-statistic to a critical value of \\(F\\) to test the null hypothesis.\nIf \\(F\\) &gt; \\(F_\\text{crit}\\), then reject the null hypothesis at the \\(\\alpha \\times 100\\) percent level.\n\nFind \\(F_\\text{crit}\\) in a table using the desired significance level, numerator degrees of freedom, and denominator degrees of freedom.\n\n\nAside: Why are \\(F\\)-statistics always positive?"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/060-compile.html#f-tests-7",
    "href": "lectures/06-Multiple-Var-Reg/060-compile.html#f-tests-7",
    "title": "Multiple Variable Regression Analysis",
    "section": "F Tests",
    "text": "F Tests\nRSS is usually a large cumbersome number.\nAlternative: Calculate the \\(F\\)-statistic using \\(R^2\\).\n\\[\n\\begin{align}\n  F = \\dfrac{\\left(R^2_u - R^2_r\\right)/q}{ (1 - R^2_u)/(n-k-1)}\n\\end{align}\n\\]\n\nWhere does this come from?\n\n\n\\(\\text{TSS} = \\text{RSS} + \\text{ESS}\\)\n\\(R^2 = \\text{ESS}/\\text{TSS}\\)\n\n\\(\\text{RSS}_r = \\text{TSS}(1-R^2_r)\\)\n\\(\\text{RSS}_u = \\text{TSS}(1-R^2_u)\\)"
  },
  {
    "objectID": "lectures/05-Inference/053-hypothesis-test.html#hypothesis-tests",
    "href": "lectures/05-Inference/053-hypothesis-test.html#hypothesis-tests",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Hypothesis Tests",
    "text": "Hypothesis Tests\nSystematic procedure that gives us evidence to hang our hat on. Starting with a Null hypothesis (\\(H_0\\)) and an Alternative hypothesis (\\(H_1\\))\n\\[\n\\begin{align*}\nH_0:& \\beta_1 = 0 \\\\\nH_1:& \\beta_1 \\neq 0\n\\end{align*}\n\\]\n. . .\nIn the context of the wage regression:\n\\[\n\\text{Wage}_i = \\beta_0 + \\beta_1 \\cdot \\text{Education}_i + u_i\n\\]\n\n\\(H_0\\): Education has no effect on wage\n\n\n\\(H_1\\): Education has an effect on wage"
  },
  {
    "objectID": "lectures/05-Inference/053-hypothesis-test.html#possible-outcomes",
    "href": "lectures/05-Inference/053-hypothesis-test.html#possible-outcomes",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Possible outcomes",
    "text": "Possible outcomes\nWithin this structure, four possible outcomes exist:\n\n. . .\n1. We fail to reject the null hypothesis and the null is true.\n. . .\nEx. Education has no effect on wage and, correctly, we fail to reject \\(H_0\\)."
  },
  {
    "objectID": "lectures/05-Inference/053-hypothesis-test.html#possible-outcomes-1",
    "href": "lectures/05-Inference/053-hypothesis-test.html#possible-outcomes-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Possible outcomes",
    "text": "Possible outcomes\nWithin this structure, four possible outcomes exist:\n\n1. We fail to reject the null hypothesis and the null is true.\n2. We reject the null hypothesis and the null is false.\n. . .\nEx. Education has an effect on wage and, correctly, we reject \\(H_0\\)."
  },
  {
    "objectID": "lectures/05-Inference/053-hypothesis-test.html#possible-outcomes-2",
    "href": "lectures/05-Inference/053-hypothesis-test.html#possible-outcomes-2",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Possible outcomes",
    "text": "Possible outcomes\nWithin this structure, four possible outcomes exist:\n\n1. We fail to reject the null hypothesis and the null is true.\n2. We reject the null hypothesis and the null is false.\n3. We reject the null hypothesis, but the null is actually true.\n. . .\nEx. Education has no effect on wage, but we incorrectly reject \\(H_0\\).\nThis is an error. Defined as a Type I error."
  },
  {
    "objectID": "lectures/05-Inference/053-hypothesis-test.html#possible-outcomes-3",
    "href": "lectures/05-Inference/053-hypothesis-test.html#possible-outcomes-3",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Possible outcomes",
    "text": "Possible outcomes\nWithin this structure, four possible outcomes exist:\n\n1. We fail to reject the null hypothesis and the null is true.\n2. We reject the null hypothesis and the null is false.\n3. We reject the null hypothesis, but the null is actually true.\n4. We fail to reject the null hypothesis, but the null is actually false.\n. . .\nEx. Education has an effect on wage, but we incorrectly fail to reject \\(H_0\\).\nThis is an error. Defined as a Type II error."
  },
  {
    "objectID": "lectures/05-Inference/053-hypothesis-test.html#possible-outcomes-4",
    "href": "lectures/05-Inference/053-hypothesis-test.html#possible-outcomes-4",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Possible outcomes",
    "text": "Possible outcomes\nWithin this structure, four possible outcomes exist:\n\n1. We fail to reject the null hypothesis and the null is true.\n2. We reject the null hypothesis and the null is false.\n3. We reject the null hypothesis, but the null is actually true.1\n4. We fail to reject the null hypothesis, but the null is actually false.2\n\nOr… from the golden age of textbook illustrations\n. . .\n\n\n\nHow I think of it"
  },
  {
    "objectID": "lectures/05-Inference/053-hypothesis-test.html#hypothesis-tests-1",
    "href": "lectures/05-Inference/053-hypothesis-test.html#hypothesis-tests-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Hypothesis Tests",
    "text": "Hypothesis Tests\nGoal: Make a statement about \\(\\beta_1\\) using information on \\(\\hat{\\beta}_1\\).\n. . .\n\\(\\hat{\\beta}_1\\) is random—it could be anything, even if \\(\\beta_1 = 0\\) is true.\n\nBut if \\(\\beta_1 = 0\\) is true, then \\(\\hat{\\beta}_1\\) is unlikely to take values far from zero.\nAs the standard error shrinks, we are even less likely to observe “extreme” values of \\(\\hat{\\beta}_1\\) (assuming \\(\\beta_1 = 0\\)).\n\n. . .\nHypothesis testing takes extreme values of \\(\\hat{\\beta}_1\\) as evidence against the null hypothesis, but it will weight them by information about variance the estimated variance of \\(\\hat{\\beta}_1\\)."
  },
  {
    "objectID": "lectures/05-Inference/053-hypothesis-test.html#hypothesis-tests-2",
    "href": "lectures/05-Inference/053-hypothesis-test.html#hypothesis-tests-2",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Hypothesis Tests",
    "text": "Hypothesis Tests\n\n\\(H_0\\): \\(\\beta_1 = 0\\)\n\n\n\\(H_1\\): \\(\\beta \\neq 0\\)\n\nTo conduct the test, we calculate a \\(t\\)-statistic3:\n\\[\nt = \\frac{\\hat{\\beta}_1 - \\beta_1^0}{\\mathop{\\hat{\\text{SE}}} \\left( \\hat{\\beta}_1 \\right)}\n\\]\nDistributed by a \\(t\\)-distribution with \\(n-2\\) degrees of freedom4."
  },
  {
    "objectID": "lectures/05-Inference/053-hypothesis-test.html#hypothesis-testing",
    "href": "lectures/05-Inference/053-hypothesis-test.html#hypothesis-testing",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\nNormal distribution vs. \\(t\\) distribution\n\nA normal distribution has the same shape for any sample size.\nThe shape of the t distribution depends the degrees of freedom.\n\n\n\n\n\n\n\nDegrees of freedom = 5."
  },
  {
    "objectID": "lectures/05-Inference/053-hypothesis-test.html#hypothesis-testing-1",
    "href": "lectures/05-Inference/053-hypothesis-test.html#hypothesis-testing-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\nNormal distribution vs. \\(t\\) distribution\n\nA normal distribution has the same shape for any sample size.\nThe shape of the t distribution depends the degrees of freedom.\n\n\n\n\n\n\n\nDegrees of freedom = 50."
  },
  {
    "objectID": "lectures/05-Inference/053-hypothesis-test.html#hypothesis-testing-2",
    "href": "lectures/05-Inference/053-hypothesis-test.html#hypothesis-testing-2",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\nNormal distribution vs. \\(t\\) distribution\n\nA normal distribution has the same shape for any sample size.\nThe shape of the t distribution depends the degrees of freedom.\n\n\n\n\n\n\n\nDegrees of freedom = 500."
  },
  {
    "objectID": "lectures/05-Inference/053-hypothesis-test.html#hypothesis-testing-3",
    "href": "lectures/05-Inference/053-hypothesis-test.html#hypothesis-testing-3",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\nTwo sided t Tests\nTo conduct a t test, compare the \\(t\\) statistic to the appropriate critical value of the t distribution.\n\nTo find the critical value in a t table, we need the degrees of freedom and the significance level \\(\\alpha\\).\n\nReject (\\(\\text{H}_0\\)) at the \\(\\alpha \\cdot 100\\)-percent level if\n\\[\n\\left| t \\right| = \\left| \\dfrac{\\hat{\\mu} - \\mu_0}{\\mathop{\\text{SE}}(\\hat{\\mu})} \\right| &gt; t_\\text{crit}.\n\\]"
  },
  {
    "objectID": "lectures/05-Inference/053-hypothesis-test.html#hypothesis-tests-3",
    "href": "lectures/05-Inference/053-hypothesis-test.html#hypothesis-tests-3",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Hypothesis Tests",
    "text": "Hypothesis Tests\nNext, we use the \\(\\color{#434C5E}{t}\\)-statistic to calculate a \\(\\color{#B48EAD}{p}\\)-value.\n\n\n\n\n\nDescribes the probability of seeing a \\(\\color{#434C5E}{t}\\)-statistic as extreme as the one we observe if the null hypothesis is actually true.\n. . .\nBut…we still need some benchmark to compare our \\(\\color{#B48EAD}{p}\\)-value against."
  },
  {
    "objectID": "lectures/05-Inference/053-hypothesis-test.html#hypothesis-tests-4",
    "href": "lectures/05-Inference/053-hypothesis-test.html#hypothesis-tests-4",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Hypothesis Tests",
    "text": "Hypothesis Tests\nWe worry mostly about false positives, so we conduct hypothesis tests based on the probability of making a Type I error5.\n. . .\nHow? We select a significance level, \\(\\color{#434C5E}{\\alpha}\\), that specifies our tolerance for false positives (i.e., the probability of Type I error we choose to live with).\n. . .\n\n\n\n\n\n\n\nTo visualize Type I and Type II, we can plot the sampling distributions of \\(\\hat{\\beta}_1\\) under the null and alternative hypotheses"
  },
  {
    "objectID": "lectures/05-Inference/053-hypothesis-test.html#hypothesis-tests-5",
    "href": "lectures/05-Inference/053-hypothesis-test.html#hypothesis-tests-5",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Hypothesis Tests",
    "text": "Hypothesis Tests\nWe then compare \\(\\color{#434C5E}{\\alpha}\\) to the \\(\\color{#B48EAD}{p}\\)-value of our test.\n\nIf the \\(\\color{#B48EAD}{p}\\)-value is less than \\(\\color{#434C5E}{\\alpha}\\), then we reject the null hypothesis at the \\(\\color{#434C5E}{\\alpha}\\cdot100\\) percent level.\nIf the \\(\\color{#B48EAD}{p}\\)-value is greater than \\(\\color{#434C5E}{\\alpha}\\), then we fail to reject the null hypothesis at the \\(\\color{#434C5E}{\\alpha}\\cdot100\\) percent level.6\n\n\n\nEx. Are campus police associated with campus crime?\n\n\n\n\n\n\n\n\\(H_0\\): \\(\\beta_\\text{Police} = 0\\)\n\\(H_1\\): \\(\\beta_\\text{Police} \\neq 0\\)\n\nSignificance level: \\(\\color{#434C5E}{\\alpha} = 0.05\\) (i.e., 5 percent test)\nTest Condition: Reject \\(H_0\\) if \\(p &lt; \\alpha\\)\nWhat is the \\(\\color{#B48EAD}{p}\\)-value? \\(p = 0.18\\)\nDo we reject the null hypothesis? No."
  },
  {
    "objectID": "lectures/05-Inference/053-hypothesis-test.html#hypothesis-tests-6",
    "href": "lectures/05-Inference/053-hypothesis-test.html#hypothesis-tests-6",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Hypothesis Tests",
    "text": "Hypothesis Tests\n\\(\\color{#B48EAD}{p}\\)-values are difficult to calculate by hand.\nAlternative: Compare \\(\\color{#434C5E}{t}\\)-statistic to critical values from the \\({\\color{#434C5E} t}\\)-distribution."
  },
  {
    "objectID": "lectures/05-Inference/053-hypothesis-test.html#hypothesis-tests-7",
    "href": "lectures/05-Inference/053-hypothesis-test.html#hypothesis-tests-7",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Hypothesis Tests",
    "text": "Hypothesis Tests\nNotation: \\(t_{1-\\alpha/2, n-2}\\) or \\(t_\\text{crit}\\).\n\nFind in a \\(t\\)-table using \\(\\color{#434C5E}{\\alpha}\\) and \\(n-2\\) degrees of freedom.\n\nCompare the the critical value to your \\(t\\)-statistic:\n\nIf \\(|t| &gt; |t_{1-\\alpha/2, n-2}|\\), then reject the null.\nIf \\(|t| &lt; |t_{1-\\alpha/2, n-2}|\\), then fail to reject the null."
  },
  {
    "objectID": "lectures/05-Inference/053-hypothesis-test.html#two-sided-tests",
    "href": "lectures/05-Inference/053-hypothesis-test.html#two-sided-tests",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Two-sided tests",
    "text": "Two-sided tests\nBased on a critical value of \\(t_{1-\\alpha/2, n-2} = t_{0.975, 100} = 1.98\\) we can identify a rejection region on the \\(\\color{#434C5E}{t}\\)-distribution.\n\n. . .\nIf our \\(\\color{#434C5E}{t}\\)-statistic is in the rejection region, then we reject the null hypothesis at the 5 percent level.\n\nEx.7 \\(\\alpha = 0.05\\)\n\n\n\n\\(H_0\\): \\(\\beta_1 = 0\\)\n\\(H_1\\): \\(\\beta_1 \\neq 0\\)\n\nNotice that the \\(\\color{#434C5E}{t}\\)-statistic is 7.15. The critical value is \\(\\color{#434C5E}{t_{\\text{0.975, 28}}} = 2.05\\).\nWhich implies that \\(p &lt; 0.05\\). Therefore, we reject \\(H_0\\) at the 5% level.\n\nEx. Are campus police associated with campus crime? (\\(\\alpha = 0.1\\))\n\n\n\n\\(H_0\\): \\(\\beta_\\text{Police} = 0\\)\n\\(H_1\\): \\(\\beta_\\text{Police} \\neq 0\\)\n\nThe \\(\\color{#434C5E}{t \\text{-stat}} = 1.35\\). The critical value is \\(\\color{#434C5E}{t_{\\text{0.95, 94}}} = 1.66\\).\n|\\(\\color{#434C5E}{t \\text{-stat}}| &lt; |\\color{#434C5E}{t_{\\text{crit}}}|\\) implies that \\(p &gt; 0.05\\). Therefore, we fail to reject \\(H_0\\) at the 10% level."
  },
  {
    "objectID": "lectures/05-Inference/053-hypothesis-test.html#one-sided-tests",
    "href": "lectures/05-Inference/053-hypothesis-test.html#one-sided-tests",
    "title": "EC 320 - Intro. Econometrics",
    "section": "One-sided tests",
    "text": "One-sided tests\nWe might be confident in a parameter being non-negative/non-positive.\nOne-sided tests assume that the parameter of interest is either greater than/less than \\(H_0\\).\n\nOption 1 \\(H_0\\): \\(\\beta_1 = 0\\) vs. \\(H_1\\): \\(\\beta_1 &gt; 0\\)\nOption 2 \\(H_0\\): \\(\\beta_1 = 0\\) vs. \\(H_1\\): \\(\\beta_1 &lt; 0\\)\n\n. . .\nIf this assumption is reasonable, then our rejection region changes.\n\nSame \\(\\alpha\\)."
  },
  {
    "objectID": "lectures/05-Inference/053-hypothesis-test.html#one-sided-tests-1",
    "href": "lectures/05-Inference/053-hypothesis-test.html#one-sided-tests-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "One-sided tests",
    "text": "One-sided tests\nLeft-tailed: Based on a critical value of \\(t_{1-\\alpha, n-2} = t_{0.95, 100} = 1.66\\), we can identify a rejection region on the \\(t\\)-distribution.\n\n\n\n\n\n. . .\nIf our \\(t\\) statistic is in the rejection region, then we reject the null hypothesis at the 5 percent level."
  },
  {
    "objectID": "lectures/05-Inference/053-hypothesis-test.html#one-sided-tests-2",
    "href": "lectures/05-Inference/053-hypothesis-test.html#one-sided-tests-2",
    "title": "EC 320 - Intro. Econometrics",
    "section": "One-sided tests",
    "text": "One-sided tests\nRight-tailed: Based on a critical value of \\(t_{1-\\alpha, n-2} = t_{0.95, 100} = 1.66\\), we can identify a rejection region on the \\(t\\)-distribution.\n\n\n\n\n\n. . .\nIf our \\(t\\) statistic is in the rejection region, then we reject the null hypothesis at the 5 percent level.\n\nEx. Do campus police deter campus crime? (\\(\\alpha = 0.1\\))\nSuppose we rule out the possibility that police increase crime, but not that they have no effect.\n\n\n\n\n\n\n\n\\(H_0\\): \\(\\beta_\\text{Police} = 0\\)\n\\(H_1\\): \\(\\beta_\\text{Police} &lt; 0\\)\n\nNotice that the \\(\\color{#434C5E}{t \\text{-stat}} = 1.35\\). The critical value is \\(\\color{#434C5E}{t_{\\text{0.9, 94}}} = 1.29\\).\nWhich implies that \\(p &gt; 0.05\\). Therefore, we reject \\(H_0\\) at the 5% level."
  },
  {
    "objectID": "lectures/05-Inference/053-hypothesis-test.html#footnotes",
    "href": "lectures/05-Inference/053-hypothesis-test.html#footnotes",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nType I error↩︎\nType II error↩︎\n\\(\\beta_1^0\\) is the value of \\(\\beta_1\\) in our null hypothesis (e.g., \\(\\beta_1^0 = 0\\)).↩︎\nrepresents the number of independent values in a sample that are free to vary when estimating statistical parameters.↩︎\nWe reject the null hypothesis, but the null is actually true.↩︎\nNote: Fail to reject \\(\\neq\\) accept.↩︎\n{{&lt; fa brands r-project &gt;}} defaults to testing hypotheses against the null hypothesis of zero.↩︎"
  },
  {
    "objectID": "lectures/05-Inference/051-prologue.html",
    "href": "lectures/05-Inference/051-prologue.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Up to now, we have been focusing on OLS considering:\n\nHow we model regressions with this estimator\nHow the estimator is derived and what properties it demonstrates\nHow the classical assumptions make the estimator BLUE\n\n. . .\nWe have mostly ignored drawing conclusions about the true population parameters from the estimates of the sample data\n\nThis is inference"
  },
  {
    "objectID": "lectures/05-Inference/051-prologue.html#ols",
    "href": "lectures/05-Inference/051-prologue.html#ols",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Up to now, we have been focusing on OLS considering:\n\nHow we model regressions with this estimator\nHow the estimator is derived and what properties it demonstrates\nHow the classical assumptions make the estimator BLUE\n\n. . .\nWe have mostly ignored drawing conclusions about the true population parameters from the estimates of the sample data\n\nThis is inference"
  },
  {
    "objectID": "lectures/05-Inference/051-prologue.html#ols-1",
    "href": "lectures/05-Inference/051-prologue.html#ols-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "OLS",
    "text": "OLS\nThus far we have fit an OLS model to find an answer to the following questions:\n\nHow much does an additional year of schooling increase earnings?\nDoes the number of police officers affect campus crime rates?\n\n. . .\nUp to now, we have not discussed our confidence in our fitted relationship\nEven if all 6 Assumptions hold, sample selection might generate the incorrect conclusions in a completely unbiased, coincidental fashion.\n\nPreviously we used the first 3 assumptions to show that OLS is unbiased:\n\\[\n\\mathop{\\mathbb{E}}\\left[ \\hat{\\beta} \\right] = \\beta\n\\]\n\nWe used the first 5 assumptions to derive a formula for the variance of the OLS estimator:\n\\[\n\\mathop{\\text{Var}}(\\hat{\\beta}) = \\frac{\\sigma^2}{\\sum_{i=1}^n (X_i - \\bar{X})^2}\n\\]\n. . .\nBy using the variance of the OLS estimator, we can infer confidence from the sampling distribution"
  },
  {
    "objectID": "lectures/05-Inference/051-prologue.html#sampling-distribution",
    "href": "lectures/05-Inference/051-prologue.html#sampling-distribution",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Sampling distribution",
    "text": "Sampling distribution\n\nThe probability distribution of the OLS estimators obtained from repeatedly drawing random samples of the same size from a population and fitting point estimates each time.\n\nProvides information about their variability, accuracy, and precision across different samples.\n. . .\n\nPoint estimates\n\nThe fitted values of the OLS estimator (e.g., \\(\\hat{\\beta}_0, \\hat{\\beta}_1\\))"
  },
  {
    "objectID": "lectures/05-Inference/051-prologue.html#sampling-distribution-properties",
    "href": "lectures/05-Inference/051-prologue.html#sampling-distribution-properties",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Sampling distribution properties",
    "text": "Sampling distribution properties\n1. Unbiasedness: If the Gauss-Markov assumptions hold, the OLS estimators are unbiased (i.e., \\(E(\\hat{\\beta}_0) = \\beta_0\\) and \\(E(\\hat{\\beta}_1) = \\beta_1\\))\n. . .\n2. Variance: The variance of the OLS estimators describes their dispersion around the true population parameters.\n. . .\n3. Normality: If the errors are normally distributed or the sample size is large enough, by the Central Limit Theorem, the sampling distribution of the OLS estimators will be approximately normal.1"
  },
  {
    "objectID": "lectures/05-Inference/051-prologue.html#sampling-distribution-1",
    "href": "lectures/05-Inference/051-prologue.html#sampling-distribution-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Sampling distribution",
    "text": "Sampling distribution\nThe sampling distribution of \\(\\hat{\\beta}\\) to conduct hypothesis tests.\nUse all 6 classical assumptions to show that OLS is normally distributed:\n\\[\n\\hat{\\beta} \\sim \\mathop{N}\\left( \\beta, \\frac{\\sigma^2}{\\sum_{i=1}^n (X_i - \\bar{X})^2} \\right)\n\\]\n. . .\nLet’s look at a simulation"
  },
  {
    "objectID": "lectures/05-Inference/051-prologue.html#section",
    "href": "lectures/05-Inference/051-prologue.html#section",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Plotting the distributions of the point estimates in a histogram\n \n\n\n\n\n\nSimulating 1,000 draws\n\nPlotting the distributions of the point estimates in a histogram\n \n\n\n\n\n\nSimulating 10,000 draws"
  },
  {
    "objectID": "lectures/05-Inference/051-prologue.html#footnotes",
    "href": "lectures/05-Inference/051-prologue.html#footnotes",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nUseful for making inferences, constructing confidence intervals, and performing hypothesis tests using the t-distribution.↩︎"
  },
  {
    "objectID": "lectures/04-Estimators-02/044-gauss-markov.html",
    "href": "lectures/04-Estimators-02/044-gauss-markov.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "OLS is the Best Linear Unbiased Estimator (BLUE) when the following assumptions hold:\n\n. . .\nA1. Linearity: The population relationship is linear in parameters with an additive error term.\nA2. Sample Variation: There is variation in \\(X\\).\nA3. Exogeniety: The \\(X\\) variable is exogenous\nA4. Homoskedasticity: The error term has the same variance for each value of the independent variable\nA5. Non-autocorrelation: The values of error terms have independent distributions"
  },
  {
    "objectID": "lectures/04-Estimators-02/044-gauss-markov.html#gauss-markov-theorem",
    "href": "lectures/04-Estimators-02/044-gauss-markov.html#gauss-markov-theorem",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "OLS is the Best Linear Unbiased Estimator (BLUE) when the following assumptions hold:\n\n. . .\nA1. Linearity: The population relationship is linear in parameters with an additive error term.\nA2. Sample Variation: There is variation in \\(X\\).\nA3. Exogeniety: The \\(X\\) variable is exogenous\nA4. Homoskedasticity: The error term has the same variance for each value of the independent variable\nA5. Non-autocorrelation: The values of error terms have independent distributions"
  },
  {
    "objectID": "lectures/04-Estimators-02/044-gauss-markov.html#gauss-markov-theorem-1",
    "href": "lectures/04-Estimators-02/044-gauss-markov.html#gauss-markov-theorem-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Gauss-Markov Theorem",
    "text": "Gauss-Markov Theorem\n\nOLS is the Best Unbiased Estimator (BUE) when the following assumptions hold:\n\n. . .\nA1. Linearity: The population relationship is linear in parameters with an additive error term.\nA2. Sample Variation: There is variation in \\(X\\).\nA3. Exogeniety: The \\(X\\) variable is exogenous\nA4. Homoskedasticity: The error term has the same variance for each value of the independent variable\nA5. Non-autocorrelation: The values of error terms have independent distributions\nA6. Normality: The population error term in normally distributed with mean zero and variance \\(\\sigma^2\\)"
  },
  {
    "objectID": "lectures/04-Estimators-02/042-goodness-of-fit.html",
    "href": "lectures/04-Estimators-02/042-goodness-of-fit.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Say there are two regressions Regression 1 and Regression 2 with the:\n\nSame slope\nSame intercept\n\nThe question is: Which fitted regression line “explains/fits” the data better?"
  },
  {
    "objectID": "lectures/04-Estimators-02/042-goodness-of-fit.html#goodness-of-fit",
    "href": "lectures/04-Estimators-02/042-goodness-of-fit.html#goodness-of-fit",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Say there are two regressions Regression 1 and Regression 2 with the:\n\nSame slope\nSame intercept\n\nThe question is: Which fitted regression line “explains/fits” the data better?"
  },
  {
    "objectID": "lectures/04-Estimators-02/042-goodness-of-fit.html#goodness-of-fit-1",
    "href": "lectures/04-Estimators-02/042-goodness-of-fit.html#goodness-of-fit-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Goodness of Fit",
    "text": "Goodness of Fit\nRegression 1 vs Regression 2\nThe coefficient of determination, \\(R^{2}\\), is the fraction of the variation in \\(y_{i}\\) “explained” by \\(x_{i}\\).\n\n\\(R^{2} = 1 \\Rightarrow x_{i}\\) explains all of the variation in \\(y_{i}\\)\n\\(R^{2} = 0 \\Rightarrow x_{i}\\) explains none of the variation in \\(y_{i}\\)"
  },
  {
    "objectID": "lectures/04-Estimators-02/042-goodness-of-fit.html#explained-and-unexplained-variation",
    "href": "lectures/04-Estimators-02/042-goodness-of-fit.html#explained-and-unexplained-variation",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Explained and Unexplained Variation",
    "text": "Explained and Unexplained Variation\nResiduals remind us that there are parts of \\(y_{i}\\) we cannot explain:\n\\[\n    y_{i} = \\hat{y}_{i} + \\hat{u}_{i}\n\\]\n\nIf you sum the above, divide by \\(n\\), and use the fact that OLS residuals sum to zero, you get:\n\n\\[\n    \\bar{\\hat{u}} = 0 \\Rightarrow \\bar{y} = \\bar{\\hat{y}}\n\\]\n\nSo the fitted values average out to the actual values"
  },
  {
    "objectID": "lectures/04-Estimators-02/042-goodness-of-fit.html#explained-and-unexplained-variation-1",
    "href": "lectures/04-Estimators-02/042-goodness-of-fit.html#explained-and-unexplained-variation-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Explained and Unexplained Variation",
    "text": "Explained and Unexplained Variation\nTotal Sum of Squares (TSS) measures variation in \\(y_{i}\\):\n\\[\n    \\color{#BF616A}{TSS} \\equiv \\sum_{i = 1}^{n} (y_{i} - \\bar{y})^{2}\n\\]\n\nTSS can be decomposed into explained and unexplained variation\n\n\n\nExplained Sum of Squared (ESS) measures the variation in \\(\\hat{y}_{i}\\):\n\\[\n    \\color{#8FBCBB}{ESS} \\equiv \\sum_{i = 1}^{n} (\\hat{y}_{i} - \\bar{y})^{2}\n\\]\n\nResidual Sum of Squares (ESS) measures the variation in $ _{i}$:\n\\[\n    \\color{#D08770}{RSS} \\equiv \\sum_{i = 1}^{n} \\hat{u}_{i}^{2}\n\\]"
  },
  {
    "objectID": "lectures/04-Estimators-02/042-goodness-of-fit.html#section",
    "href": "lectures/04-Estimators-02/042-goodness-of-fit.html#section",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "This means that we can show \\(\\color{#BF616A}{TSS} = \\color{#8FBCBB}{ESS} + \\color{#D08770}{RSS}\\)\nStep 01: Plug \\(y_{i} = \\hat{y}_{i} + \\hat{u}_{i}\\) into TSS\n\\[\\begin{align*}\n    \\color{#BF616A}{TSS} &= \\sum_{i = 1}^{n} (\\hat{y}_{i} - \\bar{y})^{2} \\\\\n    &= \\sum_{i=1}^{n} ([\\hat{y}_{i} + \\hat{u}_{i}] - [\\bar{\\hat{y}} + \\bar{\\hat{u}}])^{2}\n\\end{align*}\\]"
  },
  {
    "objectID": "lectures/04-Estimators-02/042-goodness-of-fit.html#section-1",
    "href": "lectures/04-Estimators-02/042-goodness-of-fit.html#section-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "This means that we can show \\(\\color{#BF616A}{TSS} = \\color{#8FBCBB}{ESS} + \\color{#D08770}{RSS}\\)\nStep 02: Recall that \\(\\bar{\\hat{u}} = 0\\) & \\(\\bar{y} = \\bar{\\hat{y}}\\).\n\\[\\begin{align*}\n    \\color{#BF616A}{TSS} &= \\sum_{i=1}^{n} ([\\hat{y}_{i} + \\hat{u}_{i}] - [\\bar{\\hat{y}} + \\bar{\\hat{u}}])^{2} \\\\\n    &= \\sum_{i=1}^{n} ([\\hat{y}_{i} + \\hat{u}_{i}] - \\bar{\\hat{y}})^{2} \\\\\n    &= \\sum_{i=1}^{n} ([\\hat{y}_{i} - \\bar{y}] + \\hat{u}_{i}) ([\\hat{y}_{i} - \\bar{y}] + \\hat{u}_{i}) \\\\\n    &= \\sum_{i=1}^{n} (\\hat{y}_{i} - \\bar{y})^{2} +\n    \\sum_{i=1}^{n} \\hat{u}_{i}^{2} +\n    2\\sum_{i=1}^{n} \\left( (\\hat{y}_{i} - \\bar{y}) \\hat{u}_{i} \\right)\n\\end{align*}\\]"
  },
  {
    "objectID": "lectures/04-Estimators-02/042-goodness-of-fit.html#section-2",
    "href": "lectures/04-Estimators-02/042-goodness-of-fit.html#section-2",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Step 03: Notice ESS and RSS\n\\[\\begin{align*}\n    \\color{#BF616A}{TSS} &= \\color{#8FBCBB}{\\sum_{i=1}^{n} (\\hat{y}_{i} - \\bar{y})^{2}} +\n    \\color{#D08770}{\\sum_{i=1}^{n} \\hat{u}_{i}^{2}} +\n    2\\sum_{i=1}^{n} \\left( (\\hat{y}_{i} - \\bar{y}) \\hat{u}_{i} \\right) \\\\\n    &= \\color{#8FBCBB}{ESS} + \\color{#D08770}{RSS} + 2\\sum_{i=1}^{n} \\left( (\\hat{y}_{i} - \\bar{y}) \\hat{u}_{i} \\right) \\\\\n\\end{align*}\\]\nStep 04: Simplify\n\\[\\begin{align*}\n    \\color{#BF616A}{TSS} = \\color{#8FBCBB}{ESS} + \\color{#D08770}{RSS} +\n    2\\sum_{i=1}^{n}\\hat{y}_{i}\\hat{u}_{i} -\n    2\\bar{y} \\sum_{i=1}^{n} \\hat{u}_{i}\n\\end{align*}\\]"
  },
  {
    "objectID": "lectures/04-Estimators-02/042-goodness-of-fit.html#section-3",
    "href": "lectures/04-Estimators-02/042-goodness-of-fit.html#section-3",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Step 05: Shut down that last two terms by noticing that:\n\\[\\begin{align*}\n    2\\sum_{i=1}^{n}\\hat{y}_{i}\\hat{u}_{i} -\n    2\\bar{y} \\sum_{i=1}^{n} \\hat{u}_{i} =\n    0\n\\end{align*}\\]\nYou will prove this in an assignment\nThen we have:\n\\[\\begin{align*}\n     \\color{#BF616A}{TSS} = \\color{#8FBCBB}{ESS} + \\color{#D08770}{RSS}\n\\end{align*}\\]\n. . .\nSome visual intuition makes all the math seem a lot simpler\n\nPlot our data\n\n\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n\n\n\n\n\n\n\n\n\n\n\\[\n\\color{#148B25}{\\overline{\\text{MPG}}_{i}} = 20.09\n\\]\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead."
  },
  {
    "objectID": "lectures/04-Estimators-02/042-goodness-of-fit.html#section-4",
    "href": "lectures/04-Estimators-02/042-goodness-of-fit.html#section-4",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "\\[\n\\color{#BF616A}{\\text{TSS}} \\equiv \\sum_{i=1}^n (y_i - \\bar{y})^2\n\\]"
  },
  {
    "objectID": "lectures/04-Estimators-02/042-goodness-of-fit.html#section-5",
    "href": "lectures/04-Estimators-02/042-goodness-of-fit.html#section-5",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "\\[\n\\color{#148B25}{\\widehat{\\text{MPG}}_{i}} = 37.3 - 5.34 \\cdot \\text{weight}_i\n\\]"
  },
  {
    "objectID": "lectures/04-Estimators-02/042-goodness-of-fit.html#section-6",
    "href": "lectures/04-Estimators-02/042-goodness-of-fit.html#section-6",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "\\[\n\\color{#8FBCBB}{\\text{ESS}} \\equiv \\sum_{i=1}^n (\\hat{y}_{i} - \\bar{y})^2\n\\]"
  },
  {
    "objectID": "lectures/04-Estimators-02/042-goodness-of-fit.html#section-7",
    "href": "lectures/04-Estimators-02/042-goodness-of-fit.html#section-7",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "\\[\n\\color{#D08770}{\\text{RSS}} \\equiv \\sum_{i=1}^n \\hat{u}_i^2\n\\]"
  },
  {
    "objectID": "lectures/04-Estimators-02/042-goodness-of-fit.html#section-8",
    "href": "lectures/04-Estimators-02/042-goodness-of-fit.html#section-8",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "\\[\n\\color{#BF616A}{\\text{TSS}} \\equiv \\sum_{i=1}^n (Y_i - \\bar{Y})^2\n\\]\n\n\\[\n\\color{#8FBCBB}{\\text{ESS}} \\equiv \\sum_{i=1}^n (\\hat{Y_i} - \\bar{Y})^2\n\\]\n\n\\[\n\\color{#D08770}{\\text{RSS}} \\equiv \\sum_{i=1}^n \\hat{u}_i^2\n\\]\n\n\n\n\nWarning: `position_dodge()` requires non-overlapping x intervals.\n`position_dodge()` requires non-overlapping x intervals."
  },
  {
    "objectID": "lectures/04-Estimators-02/042-goodness-of-fit.html#goodness-of-fit-2",
    "href": "lectures/04-Estimators-02/042-goodness-of-fit.html#goodness-of-fit-2",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Goodness of Fit",
    "text": "Goodness of Fit\nWhat percentage of the variation in our \\(y_{i}\\) is apparently explained by our model? The \\(R^{2}\\) term represents this percentage.\nTotal variation is represented by TSS and our model is capturing the ‘explained’ sum of squares, ESS.\nTaking a simple ratio reveals how much variation our model explains:\n\n\\(R^{2} = \\dfrac{\\color{#8FBCBB}{ESS}}{\\color{#BF616A}{TSS}}\\) varies between 0 and 1\n\\(R^{2} = 1 - \\dfrac{\\color{#D08770}{RSS}}{\\color{#BF616A}{TSS}}\\), 100% minus the unexplained variation\n\n\\(R^{2}\\) is related to the correlation between the actual values of \\(y\\) and the fitted values of \\(y\\)."
  },
  {
    "objectID": "lectures/04-Estimators-02/042-goodness-of-fit.html#goodness-of-fit-3",
    "href": "lectures/04-Estimators-02/042-goodness-of-fit.html#goodness-of-fit-3",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Goodness of Fit",
    "text": "Goodness of Fit\nSo what? In the social sciences, low \\(R^{2}\\) values are common.\nLow \\(R^{2}\\) does not necessarily mean you have a “good” regression:\n\nWorries about selection bias and omitted variables still apply\nSome ‘powerfully high’ \\(R^{2}\\) values are the result of simple accounting exercises, and tell us nothing about causality"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#important-properties",
    "href": "lectures/04-Estimators-02/040-compile.html#important-properties",
    "title": "Estimators Part II",
    "section": "Important Properties",
    "text": "Important Properties\nThere are three important OLS properties\n\n\nThe point \\((\\bar{x}, \\bar{y})\\) is always on the regression line\n\n\n\nResiduals sum to zero: \\(\\sum_{i}^{n} \\hat{u}_{i} = 0\\)\n\n\n\nThe sample covariance between the independent variable and the residuals is zero: \\(\\sum_{i}^{n} x_{i}\\hat{u}_{i} = 0\\)"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#property-1---proof",
    "href": "lectures/04-Estimators-02/040-compile.html#property-1---proof",
    "title": "Estimators Part II",
    "section": "Property 1 - Proof",
    "text": "Property 1 - Proof\nThe point \\((\\bar{x}, \\bar{y})\\) is always on the regression line\n\nStart with the regression line: \\(\\hat{y}_{i} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1} x_{i}\\)\nRecall that \\(\\hat{\\beta}_{0} = \\bar{y} - \\hat{\\beta}_{1}\\bar{x}\\)\nPlug that in \\(\\hat{\\beta}_{0}\\) and substitute \\(\\bar{x}\\) for \\(x_{i}\\):\n\n\\[\\begin{align*}\n    \\hat{y}_{i} &= \\bar{y} - \\hat{\\beta}_{1}\\bar{x} + \\hat{\\beta}_{1} \\bar{x} \\\\\n    \\hat{y}_{i} &= \\bar{y}\n\\end{align*}\\]"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#property-2---proof",
    "href": "lectures/04-Estimators-02/040-compile.html#property-2---proof",
    "title": "Estimators Part II",
    "section": "Property 2 - Proof",
    "text": "Property 2 - Proof\nResiduals sum to zero: \\(\\sum_{i}^{n} \\hat{u}_{i} = 0\\)\n\nRecall a couple of things we have derived:\n\n\\[\n    \\hat{y}_{i} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1}x_{i} \\;\\; \\text{and} \\;\\; \\hat{u}_{i} = y_{i} - \\hat{y}_{i}\n\\]\n\nThe sum of residuals is:\n\n\\[\n    \\sum_{i} \\hat{u}_{i} = \\sum_{i} (y_{i} - \\hat{y}_{i}) = \\sum_{i} y_{i} - \\sum \\hat{y}_{i}\n\\]\n\nRecall the fact that \\(\\sum_{i} y_{i} = n\\bar{y}\\) and also:\n\n\\[\\begin{align*}\n    \\sum_{i} \\hat{y}_{i} &= \\sum_{i} (\\hat{\\beta}_{0} + \\hat{\\beta}_{1}x_{i})\n    = n \\hat{\\beta}_{0} + \\hat{\\beta}_{1} \\sum_{i} x_{i} \\\\\n    &= n (\\bar{y}_{i} - \\hat{\\beta}_{1}\\bar{x}) + \\hat{\\beta}_{1} n\\bar{x} = n\\bar{y}_{i}\n\\end{align*}\\]\n\nSo:\n\n\\[\n    \\sum_{i} \\hat{u}_{i} = n\\bar{y}_{i} - n\\bar{y}_{i} = 0\n\\]"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#property-3---proof",
    "href": "lectures/04-Estimators-02/040-compile.html#property-3---proof",
    "title": "Estimators Part II",
    "section": "Property 3 - Proof",
    "text": "Property 3 - Proof\nThe sample covariance between the independent variable and the residuals is zero: \\(\\sum_{i}^{n} x_{i}\\hat{u}_{i} = 0\\)\n\nStart with our residuals: \\(\\hat{u}_{i} = y_{i} - \\hat{\\beta}_{0} - \\hat{\\beta}_{1}x_{i}\\)\nMultiply both sides by \\(x_{i}\\) and sum them:\n\n\\[\n    \\sum_{i} x_{i}\\hat{u}_{i} = \\sum_{i} x_{i}y_{i} - \\hat{\\beta}_{0}\\sum_{i} x_{i} - \\hat{\\beta}_{1}\\sum_{i} x_{i}^{2}\n\\]\n\nRecall from our \\(\\hat{\\beta}_{1}\\) derivation that \\(\\sum_{i} x_{i}y_{i} = \\hat{\\beta}_{0}\\sum_{i} x_{i} + \\hat{\\beta}_{1}\\sum_{i} x_{i}^{2}\\)\n\nSo: \\(\\sum_{i}^{n} x_{i}\\hat{u}_{i} = \\hat{\\beta}_{0}\\sum_{i} x_{i} + \\hat{\\beta}_{1}\\sum_{i} x_{i}^{2} - \\hat{\\beta}_{0}\\sum_{i} x_{i} - \\hat{\\beta}_{1}\\sum_{i} x_{i}^{2} = 0\\)"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#goodness-of-fit-1",
    "href": "lectures/04-Estimators-02/040-compile.html#goodness-of-fit-1",
    "title": "Estimators Part II",
    "section": "Goodness of Fit",
    "text": "Goodness of Fit\nSay there are two regressions Regression 1 and Regression 2 with the:\n\nSame slope\nSame intercept\n\nThe question is: Which fitted regression line “explains/fits” the data better?"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#goodness-of-fit-2",
    "href": "lectures/04-Estimators-02/040-compile.html#goodness-of-fit-2",
    "title": "Estimators Part II",
    "section": "Goodness of Fit",
    "text": "Goodness of Fit\nRegression 1 vs Regression 2\nThe coefficient of determination, \\(R^{2}\\), is the fraction of the variation in \\(y_{i}\\) “explained” by \\(x_{i}\\).\n\n\\(R^{2} = 1 \\Rightarrow x_{i}\\) explains all of the variation in \\(y_{i}\\)\n\\(R^{2} = 0 \\Rightarrow x_{i}\\) explains none of the variation in \\(y_{i}\\)"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#explained-and-unexplained-variation",
    "href": "lectures/04-Estimators-02/040-compile.html#explained-and-unexplained-variation",
    "title": "Estimators Part II",
    "section": "Explained and Unexplained Variation",
    "text": "Explained and Unexplained Variation\nResiduals remind us that there are parts of \\(y_{i}\\) we cannot explain:\n\\[\n    y_{i} = \\hat{y}_{i} + \\hat{u}_{i}\n\\]\n\nIf you sum the above, divide by \\(n\\), and use the fact that OLS residuals sum to zero, you get:\n\n\\[\n    \\bar{\\hat{u}} = 0 \\Rightarrow \\bar{y} = \\bar{\\hat{y}}\n\\]\n\nSo the fitted values average out to the actual values"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#explained-and-unexplained-variation-1",
    "href": "lectures/04-Estimators-02/040-compile.html#explained-and-unexplained-variation-1",
    "title": "Estimators Part II",
    "section": "Explained and Unexplained Variation",
    "text": "Explained and Unexplained Variation\nTotal Sum of Squares (TSS) measures variation in \\(y_{i}\\):\n\\[\n    \\color{#BF616A}{TSS} \\equiv \\sum_{i = 1}^{n} (y_{i} - \\bar{y})^{2}\n\\]\n\nTSS can be decomposed into explained and unexplained variation\n\n\n\nExplained Sum of Squared (ESS) measures the variation in \\(\\hat{y}_{i}\\):\n\\[\n    \\color{#8FBCBB}{ESS} \\equiv \\sum_{i = 1}^{n} (\\hat{y}_{i} - \\bar{y})^{2}\n\\]\n\nResidual Sum of Squares (ESS) measures the variation in $ _{i}$:\n\\[\n    \\color{#D08770}{RSS} \\equiv \\sum_{i = 1}^{n} \\hat{u}_{i}^{2}\n\\]"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#section",
    "href": "lectures/04-Estimators-02/040-compile.html#section",
    "title": "Estimators Part II",
    "section": "",
    "text": "This means that we can show \\(\\color{#BF616A}{TSS} = \\color{#8FBCBB}{ESS} + \\color{#D08770}{RSS}\\)\nStep 01: Plug \\(y_{i} = \\hat{y}_{i} + \\hat{u}_{i}\\) into TSS\n\\[\\begin{align*}\n    \\color{#BF616A}{TSS} &= \\sum_{i = 1}^{n} (\\hat{y}_{i} - \\bar{y})^{2} \\\\\n    &= \\sum_{i=1}^{n} ([\\hat{y}_{i} + \\hat{u}_{i}] - [\\bar{\\hat{y}} + \\bar{\\hat{u}}])^{2}\n\\end{align*}\\]"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#section-1",
    "href": "lectures/04-Estimators-02/040-compile.html#section-1",
    "title": "Estimators Part II",
    "section": "",
    "text": "This means that we can show \\(\\color{#BF616A}{TSS} = \\color{#8FBCBB}{ESS} + \\color{#D08770}{RSS}\\)\nStep 02: Recall that \\(\\bar{\\hat{u}} = 0\\) & \\(\\bar{y} = \\bar{\\hat{y}}\\).\n\\[\\begin{align*}\n    \\color{#BF616A}{TSS} &= \\sum_{i=1}^{n} ([\\hat{y}_{i} + \\hat{u}_{i}] - [\\bar{\\hat{y}} + \\bar{\\hat{u}}])^{2} \\\\\n    &= \\sum_{i=1}^{n} ([\\hat{y}_{i} + \\hat{u}_{i}] - \\bar{\\hat{y}})^{2} \\\\\n    &= \\sum_{i=1}^{n} ([\\hat{y}_{i} - \\bar{y}] + \\hat{u}_{i}) ([\\hat{y}_{i} - \\bar{y}] + \\hat{u}_{i}) \\\\\n    &= \\sum_{i=1}^{n} (\\hat{y}_{i} - \\bar{y})^{2} +\n    \\sum_{i=1}^{n} \\hat{u}_{i}^{2} +\n    2\\sum_{i=1}^{n} \\left( (\\hat{y}_{i} - \\bar{y}) \\hat{u}_{i} \\right)\n\\end{align*}\\]"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#section-2",
    "href": "lectures/04-Estimators-02/040-compile.html#section-2",
    "title": "Estimators Part II",
    "section": "",
    "text": "Step 03: Notice ESS and RSS\n\\[\\begin{align*}\n    \\color{#BF616A}{TSS} &= \\color{#8FBCBB}{\\sum_{i=1}^{n} (\\hat{y}_{i} - \\bar{y})^{2}} +\n    \\color{#D08770}{\\sum_{i=1}^{n} \\hat{u}_{i}^{2}} +\n    2\\sum_{i=1}^{n} \\left( (\\hat{y}_{i} - \\bar{y}) \\hat{u}_{i} \\right) \\\\\n    &= \\color{#8FBCBB}{ESS} + \\color{#D08770}{RSS} + 2\\sum_{i=1}^{n} \\left( (\\hat{y}_{i} - \\bar{y}) \\hat{u}_{i} \\right) \\\\\n\\end{align*}\\]\nStep 04: Simplify\n\\[\\begin{align*}\n    \\color{#BF616A}{TSS} = \\color{#8FBCBB}{ESS} + \\color{#D08770}{RSS} +\n    2\\sum_{i=1}^{n}\\hat{y}_{i}\\hat{u}_{i} -\n    2\\bar{y} \\sum_{i=1}^{n} \\hat{u}_{i}\n\\end{align*}\\]"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#section-3",
    "href": "lectures/04-Estimators-02/040-compile.html#section-3",
    "title": "Estimators Part II",
    "section": "",
    "text": "Step 05: Shut down that last two terms by noticing that:\n\\[\\begin{align*}\n    2\\sum_{i=1}^{n}\\hat{y}_{i}\\hat{u}_{i} -\n    2\\bar{y} \\sum_{i=1}^{n} \\hat{u}_{i} =\n    0\n\\end{align*}\\]\nYou will prove this in an assignment\nThen we have:\n\\[\\begin{align*}\n     \\color{#BF616A}{TSS} = \\color{#8FBCBB}{ESS} + \\color{#D08770}{RSS}\n\\end{align*}\\]\n\nSome visual intuition makes all the math seem a lot simpler"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#section-4",
    "href": "lectures/04-Estimators-02/040-compile.html#section-4",
    "title": "Estimators Part II",
    "section": "",
    "text": "\\[\n\\color{#BF616A}{\\text{TSS}} \\equiv \\sum_{i=1}^n (y_i - \\bar{y})^2\n\\]"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#section-5",
    "href": "lectures/04-Estimators-02/040-compile.html#section-5",
    "title": "Estimators Part II",
    "section": "",
    "text": "\\[\n\\color{#148B25}{\\widehat{\\text{MPG}}_{i}} = 37.3 - 5.34 \\cdot \\text{weight}_i\n\\]"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#section-6",
    "href": "lectures/04-Estimators-02/040-compile.html#section-6",
    "title": "Estimators Part II",
    "section": "",
    "text": "\\[\n\\color{#8FBCBB}{\\text{ESS}} \\equiv \\sum_{i=1}^n (\\hat{y}_{i} - \\bar{y})^2\n\\]"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#section-7",
    "href": "lectures/04-Estimators-02/040-compile.html#section-7",
    "title": "Estimators Part II",
    "section": "",
    "text": "\\[\n\\color{#D08770}{\\text{RSS}} \\equiv \\sum_{i=1}^n \\hat{u}_i^2\n\\]"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#section-8",
    "href": "lectures/04-Estimators-02/040-compile.html#section-8",
    "title": "Estimators Part II",
    "section": "",
    "text": "\\[\n\\color{#BF616A}{\\text{TSS}} \\equiv \\sum_{i=1}^n (Y_i - \\bar{Y})^2\n\\]\n\n\\[\n\\color{#8FBCBB}{\\text{ESS}} \\equiv \\sum_{i=1}^n (\\hat{Y_i} - \\bar{Y})^2\n\\]\n\n\\[\n\\color{#D08770}{\\text{RSS}} \\equiv \\sum_{i=1}^n \\hat{u}_i^2\n\\]"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#goodness-of-fit-3",
    "href": "lectures/04-Estimators-02/040-compile.html#goodness-of-fit-3",
    "title": "Estimators Part II",
    "section": "Goodness of Fit",
    "text": "Goodness of Fit\nWhat percentage of the variation in our \\(y_{i}\\) is apparently explained by our model? The \\(R^{2}\\) term represents this percentage.\nTotal variation is represented by TSS and our model is capturing the ‘explained’ sum of squares, ESS.\nTaking a simple ratio reveals how much variation our model explains:\n\n\\(R^{2} = \\dfrac{\\color{#8FBCBB}{ESS}}{\\color{#BF616A}{TSS}}\\) varies between 0 and 1\n\\(R^{2} = 1 - \\dfrac{\\color{#D08770}{RSS}}{\\color{#BF616A}{TSS}}\\), 100% minus the unexplained variation\n\n\\(R^{2}\\) is related to the correlation between the actual values of \\(y\\) and the fitted values of \\(y\\)."
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#goodness-of-fit-4",
    "href": "lectures/04-Estimators-02/040-compile.html#goodness-of-fit-4",
    "title": "Estimators Part II",
    "section": "Goodness of Fit",
    "text": "Goodness of Fit\nSo what? In the social sciences, low \\(R^{2}\\) values are common.\nLow \\(R^{2}\\) does not necessarily mean you have a “good” regression:\n\nWorries about selection bias and omitted variables still apply\nSome ‘powerfully high’ \\(R^{2}\\) values are the result of simple accounting exercises, and tell us nothing about causality"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#residuals-vs-errors",
    "href": "lectures/04-Estimators-02/040-compile.html#residuals-vs-errors",
    "title": "Estimators Part II",
    "section": "Residuals vs Errors",
    "text": "Residuals vs Errors\n\nThe most important assumptions concern the error term \\(u_{i}\\).\nImportant: An error \\(u_{i}\\) and a residual \\(\\hat{u}_{i}\\) are related, but different.\nTake for example, a model of the effects of education on wages.\n\nError:\n\nDifference between the wage of a worker with 11 years of education and the expected wage with 11 years of education\n\nResidual:\n\nDifference between the wage of a worker with 11 years of education and the average wage of workers with 11 years of education\n\n\n\nPopulation vs. Sample"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#residuals-vs-errors-1",
    "href": "lectures/04-Estimators-02/040-compile.html#residuals-vs-errors-1",
    "title": "Estimators Part II",
    "section": "Residuals vs Errors",
    "text": "Residuals vs Errors\nA residual tells us how a worker’s wages comapre to the average wages of workers in the sample with the same level of education"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#residuals-vs-errors-2",
    "href": "lectures/04-Estimators-02/040-compile.html#residuals-vs-errors-2",
    "title": "Estimators Part II",
    "section": "Residuals vs Errors",
    "text": "Residuals vs Errors\nA residual tells us how a worker’s wages comapre to the average wages of workers in the sample with the same level of education"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#residuals-vs-errors-3",
    "href": "lectures/04-Estimators-02/040-compile.html#residuals-vs-errors-3",
    "title": "Estimators Part II",
    "section": "Residuals vs Errors",
    "text": "Residuals vs Errors\nAn error tells us how a worker’s wages compare to the expected wages of workers in the population with the same level of education"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#classical-assumptions-of-ols",
    "href": "lectures/04-Estimators-02/040-compile.html#classical-assumptions-of-ols",
    "title": "Estimators Part II",
    "section": "Classical Assumptions of OLS",
    "text": "Classical Assumptions of OLS\nA1. Linearity: The population relationship is linear in parameters with an additive error term\nA2. Sample Variation: There is variation in \\(X\\)\nA3. Exogeneity: The \\(X\\) variable is exogenous\nA4. Homosekdasticity: The error term has the same variance for each value of the independent variable\nA5. Non-Autocorrelation: The values of error terms have independent distributions\nA6. Normality: The population error term is normally distributed with mean zero and variance \\(\\sigma^{2}\\)"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#a1.-linearity",
    "href": "lectures/04-Estimators-02/040-compile.html#a1.-linearity",
    "title": "Estimators Part II",
    "section": "A1. Linearity",
    "text": "A1. Linearity\n\nThe population relationship is linear in parameters with an additive error term\n\nExamples\n\n\\(\\text{Wage}_i = \\beta_1 + \\beta_2 \\text{Experience}_i + u_i\\)\n\n\n\n\\(\\log(\\text{Happiness}_i) = \\beta_1 + \\beta_2 \\log(\\text{Money}_i) + u_i\\)\n\n\n\n\n\\(\\sqrt{\\text{Convictions}_i} = \\beta_1 + \\beta_2 (\\text{Early Childhood Lead Exposure})_i + u_i\\)\n\n\n\n\n\\(\\log(\\text{Earnings}_i) = \\beta_1 + \\beta_2 \\text{Education}_i + u_i\\)"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#a1.-linearity-1",
    "href": "lectures/04-Estimators-02/040-compile.html#a1.-linearity-1",
    "title": "Estimators Part II",
    "section": "A1. Linearity",
    "text": "A1. Linearity\n\nThe population relationship is linear in parameters with an additive error term.\n\nViolations\n\n\\(\\text{Wage}_i = (\\beta_1 + \\beta_2 \\text{Experience}_i)u_i\\)\n\n\n\n\\(\\text{Consumption}_i = \\frac{1}{\\beta_1 + \\beta_2 \\text{Income}_i} + u_i\\)\n\n\n\n\n\\(\\text{Population}_i = \\frac{\\beta_1}{1 + e^{\\beta_2 + \\beta_3 \\text{Food}_i}} + u_i\\)\n\n\n\n\n\\(\\text{Batting Average}_i = \\beta_1 (\\text{Wheaties Consumption})_i^{\\beta_2} + u_i\\)"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#a2.-sample-variation",
    "href": "lectures/04-Estimators-02/040-compile.html#a2.-sample-variation",
    "title": "Estimators Part II",
    "section": "A2. Sample Variation",
    "text": "A2. Sample Variation\n\nThere is variation in \\(X\\).\n\nExample"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#a2.-sample-variation-1",
    "href": "lectures/04-Estimators-02/040-compile.html#a2.-sample-variation-1",
    "title": "Estimators Part II",
    "section": "A2. Sample Variation",
    "text": "A2. Sample Variation\n\nThere is variation in \\(X\\).\n\nViolation\n\nWe will see later that variation matters for inference as well"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#a3.-exogeneity",
    "href": "lectures/04-Estimators-02/040-compile.html#a3.-exogeneity",
    "title": "Estimators Part II",
    "section": "A3. Exogeneity",
    "text": "A3. Exogeneity\n\nThe \\(X\\) variable is exogenous\n\nWe can write this as:\n\\[\n    \\mathbb{E}[(u|X)] = 0\n\\]\nWhich essentially says that the expected value of the errors term, conditional on the variable \\(X\\) is 0. The assignment of \\(X\\) is effectively random.\nA significant implication of this is no selection bias or omitted variable bias"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#a3.-exogeneity-1",
    "href": "lectures/04-Estimators-02/040-compile.html#a3.-exogeneity-1",
    "title": "Estimators Part II",
    "section": "A3. Exogeneity",
    "text": "A3. Exogeneity\n\nThe \\(X\\) variable is exogenous\n\n\\[\n    \\mathbb{E}[(u|X)] = 0\n\\]\nExample\nIn the labor market, an important component of \\(u\\) is unobserved ability\n\n\\(\\mathbb{E}(u|\\text{Education} = 12) = 0\\) and \\(\\mathbb{E}(u|\\text{Education} = 20) = 0\\)\n\\(\\mathbb{E}(u|\\text{Education} = 0) = 0\\) and \\(\\mathbb{E}(u|\\text{Education} = 40) = 0\\)\n\nnote: This is an assumption that does not necessarily hold true in real life, but with enough observations we can comfortably assume something like this"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#a3.-exogeneity-2",
    "href": "lectures/04-Estimators-02/040-compile.html#a3.-exogeneity-2",
    "title": "Estimators Part II",
    "section": "A3. Exogeneity",
    "text": "A3. Exogeneity\n\n\nValid Exogeneity\n\\[\n    \\mathbb{E}[(u|X)] = 0\n\\]\n\n\n\n\n\n\nInvalid Exogeneity\n\\[\n    \\mathbb{E}[(u|X)] \\neq 0\n\\]"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#interlude-unbiasedness-of-ols",
    "href": "lectures/04-Estimators-02/040-compile.html#interlude-unbiasedness-of-ols",
    "title": "Estimators Part II",
    "section": "Interlude: Unbiasedness of OLS",
    "text": "Interlude: Unbiasedness of OLS\nWhen can we trust OLS?\nIn estimators, the concept of bias means that the expected value of the estimate is different from the true population parameter.\nGraphically we have:\n\n\nUnbiased estimator: \\(\\mathop{\\mathbb{E}}\\left[ \\hat{\\beta} \\right] = \\beta\\)\n\n\n\n\n\n\n\n\n\n\nBiased estimator: \\(\\mathop{\\mathbb{E}}\\left[ \\hat{\\beta} \\right] \\neq \\beta\\)"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#is-ols-unbiased",
    "href": "lectures/04-Estimators-02/040-compile.html#is-ols-unbiased",
    "title": "Estimators Part II",
    "section": "Is OLS Unbiased?",
    "text": "Is OLS Unbiased?\nWe require our first 3 assumptions for unbaised OLS estimator\nA1. Linearity: The population relationship is linear in parameters with an additive error term\nA2. Sample Variation: There is variation in \\(X\\)\nA3. Exogeneity: The \\(X\\) variable is exogenous\nAnd we can mathematically prove it!"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#proving-unbiasedness-of-ols",
    "href": "lectures/04-Estimators-02/040-compile.html#proving-unbiasedness-of-ols",
    "title": "Estimators Part II",
    "section": "Proving Unbiasedness of OLS",
    "text": "Proving Unbiasedness of OLS\nSuppose we have the following model\n\\[\n    y_{i} = \\beta_{1} + \\beta_{2}x_{i} + u_{i}\n\\]\n\nThe slope parameter follows as:\n\\[\n\\hat{\\beta}_2 = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum(x_i - \\bar{x})^2}\n\\]\n\n\n(As shown in section 2.3 in ItE) that the estimator \\(\\hat{\\beta_2}\\), can be broken up into a nonrandom and a random component:"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#required-assumptions",
    "href": "lectures/04-Estimators-02/040-compile.html#required-assumptions",
    "title": "Estimators Part II",
    "section": "Required Assumptions",
    "text": "Required Assumptions\nA1. Linearity: The population relationship is linear in parameters with an additive error term.\nA2. Sample Variation: There is variation in \\(X\\).\nA3. Exogeniety: The \\(X\\) variable is exogenous\n\nA3 implies random sampling.\n\n\nResult: OLS is unbiased."
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#classical-assumptions-of-ols-1",
    "href": "lectures/04-Estimators-02/040-compile.html#classical-assumptions-of-ols-1",
    "title": "Estimators Part II",
    "section": "Classical Assumptions of OLS",
    "text": "Classical Assumptions of OLS\nA1. Linearity: The population relationship is linear in parameters with an additive error term.\nA2. Sample Variation: There is variation in \\(X\\).\nA3. Exogeniety: The \\(X\\) variable is exogenous\n\n \n\nThe following 2 assumptions are not required for unbiasedness…\n\n\nBut they are important for an efficient estimator\n\n\nLet’s talk about why variance matters"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#why-variance-matters",
    "href": "lectures/04-Estimators-02/040-compile.html#why-variance-matters",
    "title": "Estimators Part II",
    "section": "Why variance matters",
    "text": "Why variance matters\nUnbiasedness tells us that OLS gets it right, on average. But we can’t tell whether our sample is “typical.”\n\n\nVariance tells us how far OLS can deviate from the population mean.\n\nHow tight is OLS centered on its expected value?\nThis determines the efficiency of our estimator."
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#why-variance-matters-1",
    "href": "lectures/04-Estimators-02/040-compile.html#why-variance-matters-1",
    "title": "Estimators Part II",
    "section": "Why variance matters",
    "text": "Why variance matters\nUnbiasedness tells us that OLS gets it right, on average. But we can’t tell whether our sample is “typical.”\n\nThe smaller the variance, the closer OLS gets, on average, to the true population parameters on any sample.\n\nGiven two unbiased estimators, we want the one with smaller variance.\nIf two more assumptions are satisfied, we are using the most efficient linear estimator."
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#classical-assumptions-of-ols-2",
    "href": "lectures/04-Estimators-02/040-compile.html#classical-assumptions-of-ols-2",
    "title": "Estimators Part II",
    "section": "Classical Assumptions of OLS",
    "text": "Classical Assumptions of OLS\nA1. Linearity: The population relationship is linear in parameters with an additive error term.\nA2. Sample Variation: There is variation in \\(X\\).\nA3. Exogeniety: The \\(X\\) variable is exogenous\n\nA4. Homoskedasticity: The error term has the same variance for each value of the independent variable"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#a4.-homoskedasticity",
    "href": "lectures/04-Estimators-02/040-compile.html#a4.-homoskedasticity",
    "title": "Estimators Part II",
    "section": "A4. Homoskedasticity",
    "text": "A4. Homoskedasticity\n\nThe error term has the same variance for each value of the independent variable \\(x_{i}\\)\n\n\\[\n    Var(u|X) = \\sigma^{2}.\n\\]\nExample:"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#a4.-homoskedasticity-1",
    "href": "lectures/04-Estimators-02/040-compile.html#a4.-homoskedasticity-1",
    "title": "Estimators Part II",
    "section": "A4. Homoskedasticity",
    "text": "A4. Homoskedasticity\n\nThe error term has the same variance for each value of the independent variable \\(x_{i}\\)\n\n\\[\n    Var(u|X) = \\sigma^{2}.\n\\]\nViolation:"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#a4.-homoskedasticity-2",
    "href": "lectures/04-Estimators-02/040-compile.html#a4.-homoskedasticity-2",
    "title": "Estimators Part II",
    "section": "A4. Homoskedasticity",
    "text": "A4. Homoskedasticity\n\nThe error term has the same variance for each value of the independent variable \\(x_{i}\\)\n\n\\[\n    Var(u|X) = \\sigma^{2}.\n\\]\nViolation:"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#heteroskedasticity-example",
    "href": "lectures/04-Estimators-02/040-compile.html#heteroskedasticity-example",
    "title": "Estimators Part II",
    "section": "Heteroskedasticity Example",
    "text": "Heteroskedasticity Example\nSuppose we study the following relationship:\n\\[\n\\text{Luxury Expenditure}_i = \\beta_1 + \\beta_2 \\text{Income}_i + u_i\n\\]\n\nAs income increases, variation in luxury expenditures increase\n\nVariance of \\(u_i\\) is likely larger for higher-income households\nPlot of the residuals against the household income would likely reveal a funnel-shaped pattern"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#section-9",
    "href": "lectures/04-Estimators-02/040-compile.html#section-9",
    "title": "Estimators Part II",
    "section": "",
    "text": "Common test for heteroskedasticity… Plot the residuals across \\(X\\)"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#classical-assumptions-of-ols-3",
    "href": "lectures/04-Estimators-02/040-compile.html#classical-assumptions-of-ols-3",
    "title": "Estimators Part II",
    "section": "Classical Assumptions of OLS",
    "text": "Classical Assumptions of OLS\nA1. Linearity: The population relationship is linear in parameters with an additive error term.\nA2.Sample Variation: There is variation in \\(X\\).\nA3. Exogeniety: The \\(X\\) variable is exogenous\nA4. Homoskedasticity: The error term has the same variance for each value of the independent variable\n\nA5. Non-autocorrelation: The values of error terms have independent distributions"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#a5.-non-autocorrelation",
    "href": "lectures/04-Estimators-02/040-compile.html#a5.-non-autocorrelation",
    "title": "Estimators Part II",
    "section": "A5. Non-Autocorrelation",
    "text": "A5. Non-Autocorrelation\n\nThe values of error terms have independent distributions1\n\n\\[\nE[u_i u_j]=0, \\forall i \\text{ s.t. } i \\neq j\n\\]\n\nOr…\n\\[\n\\begin{align*}\n\\mathop{\\text{Cov}}(u_i, u_j) &= E[(u_i - \\mu_u)(u_j - \\mu_u)]\\\\\n                              &= E[u_i u_j] = E[u_i] E[u_j]  = 0, \\text{where } i \\neq j\n\\end{align*}\n\\]\n\nNotes: \\(\\forall i = \\text{for all} \\: i\\), \\(\\text{s.t.} = \\text{such that}\\), \\(i \\neq j \\: \\text{means} \\: i \\: \\text{is not equal to} \\: j\\)"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#a5.-non-autocorrelation-1",
    "href": "lectures/04-Estimators-02/040-compile.html#a5.-non-autocorrelation-1",
    "title": "Estimators Part II",
    "section": "A5. Non-Autocorrelation",
    "text": "A5. Non-Autocorrelation\n\nThe values of error terms have independent distributions\n\n\\[\nE[u_i u_j]=0, \\forall i \\text{ s.t. } i \\neq j\n\\]\n\nImplies no systematic association between pairs of individual \\(u_i\\)\nAlmost always some unobserved correlation across individuals1\nReferred to as clustering problem.\nAn easy solution exists where we can adjust our standard errors\n\n(e.g. common correlation in unobservables among individuals within a given US state)"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#section-10",
    "href": "lectures/04-Estimators-02/040-compile.html#section-10",
    "title": "Estimators Part II",
    "section": "",
    "text": "Let’s take a moment to talk about the variance of the OLS estimator\n\n\\[\n    Var(\\hat{\\beta}_{1}) = \\dfrac{\n        \\sigma^{2}\n        }{\n        \\sum (x_{i} - \\bar{x})^{2}\n        }\n\\]"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#classical-assumptions-of-ols-4",
    "href": "lectures/04-Estimators-02/040-compile.html#classical-assumptions-of-ols-4",
    "title": "Estimators Part II",
    "section": "Classical Assumptions of OLS",
    "text": "Classical Assumptions of OLS\nA1. Linearity: The population relationship is linear in parameters with an additive error term.\nA2. Sample Variation: There is variation in \\(X\\).\nA3. Exogeniety: The \\(X\\) variable is exogenous\nA4. Homoskedasticity: The error term has the same variance for each value of the independent variable\nA5. Non-autocorrelation: The values of error terms have independent distributions\n\nIf A4 and A5 are satisfied, along with A1, A2, and A3, then we are using the most efficient linear estimator"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#classical-assumptions-of-ols-5",
    "href": "lectures/04-Estimators-02/040-compile.html#classical-assumptions-of-ols-5",
    "title": "Estimators Part II",
    "section": "Classical Assumptions of OLS",
    "text": "Classical Assumptions of OLS\nA1. Linearity: The population relationship is linear in parameters with an additive error term.\nA2. Sample Variation: There is variation in \\(X\\).\nA3. Exogeniety: The \\(X\\) variable is exogenous\nA4. Homoskedasticity: The error term has the same variance for each value of the independent variable\nA5. Non-autocorrelation: The values of error terms have independent distributions\n\nA6. Normality The population error term in normally distributed with mean zero and variance \\(\\sigma^{2}\\)"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#a6.-normality",
    "href": "lectures/04-Estimators-02/040-compile.html#a6.-normality",
    "title": "Estimators Part II",
    "section": "A6. Normality",
    "text": "A6. Normality\n\nThe population error term in normally distributed with mean zero and variance \\(\\sigma^{2}\\)\n\nAlso known as:\n\\[\n    u \\sim N(0,\\sigma^{2})\n\\]\nWhere \\(\\sim\\) means distributed by and \\(N\\) stands for normal distribution\nHowever, A6 is not required for efficiency nor unbiasedness"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#gauss-markov-theorem-1",
    "href": "lectures/04-Estimators-02/040-compile.html#gauss-markov-theorem-1",
    "title": "Estimators Part II",
    "section": "Gauss-Markov Theorem",
    "text": "Gauss-Markov Theorem\n\nOLS is the Best Linear Unbiased Estimator (BLUE) when the following assumptions hold:\n\n\nA1. Linearity: The population relationship is linear in parameters with an additive error term.\nA2. Sample Variation: There is variation in \\(X\\).\nA3. Exogeniety: The \\(X\\) variable is exogenous\nA4. Homoskedasticity: The error term has the same variance for each value of the independent variable\nA5. Non-autocorrelation: The values of error terms have independent distributions"
  },
  {
    "objectID": "lectures/04-Estimators-02/040-compile.html#gauss-markov-theorem-2",
    "href": "lectures/04-Estimators-02/040-compile.html#gauss-markov-theorem-2",
    "title": "Estimators Part II",
    "section": "Gauss-Markov Theorem",
    "text": "Gauss-Markov Theorem\n\nOLS is the Best Unbiased Estimator (BUE) when the following assumptions hold:\n\n\nA1. Linearity: The population relationship is linear in parameters with an additive error term.\nA2. Sample Variation: There is variation in \\(X\\).\nA3. Exogeniety: The \\(X\\) variable is exogenous\nA4. Homoskedasticity: The error term has the same variance for each value of the independent variable\nA5. Non-autocorrelation: The values of error terms have independent distributions\nA6. Normality: The population error term in normally distributed with mean zero and variance \\(\\sigma^2\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/034-ols.html",
    "href": "lectures/03-Estimators-01/034-ols.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "The OLS Estimator chooses the parameters \\(\\hat{\\beta}_{0}\\) and \\(\\hat{\\beta}_{1}\\) that minimize the Residual Sum of Squares (RSS)\n\\[\n    \\min_{\\hat{\\beta}_{0},\\hat{\\beta}_{1}} \\sum_{i=1}^{n} \\hat{u}_{i}^{2}\n\\]\nThis is why we call the estimator ordinary least squares\nRecall that residuals are given by \\(y_{i} - \\hat{y}_{i}\\) and that:\n\\[\n    \\hat{y}_{i} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1} x_{i}\n\\]\nThen\n\\[\n    u_{i} = y_{i} - \\hat{\\beta}_{0} + \\hat{\\beta}_{1} x_{i}\n\\]"
  },
  {
    "objectID": "lectures/03-Estimators-01/034-ols.html#ols",
    "href": "lectures/03-Estimators-01/034-ols.html#ols",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "The OLS Estimator chooses the parameters \\(\\hat{\\beta}_{0}\\) and \\(\\hat{\\beta}_{1}\\) that minimize the Residual Sum of Squares (RSS)\n\\[\n    \\min_{\\hat{\\beta}_{0},\\hat{\\beta}_{1}} \\sum_{i=1}^{n} \\hat{u}_{i}^{2}\n\\]\nThis is why we call the estimator ordinary least squares\nRecall that residuals are given by \\(y_{i} - \\hat{y}_{i}\\) and that:\n\\[\n    \\hat{y}_{i} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1} x_{i}\n\\]\nThen\n\\[\n    u_{i} = y_{i} - \\hat{\\beta}_{0} + \\hat{\\beta}_{1} x_{i}\n\\]"
  },
  {
    "objectID": "lectures/03-Estimators-01/034-ols.html#ols-calculus",
    "href": "lectures/03-Estimators-01/034-ols.html#ols-calculus",
    "title": "EC 320 - Intro. Econometrics",
    "section": "OLS & Calculus",
    "text": "OLS & Calculus\nWe can find our choices \\(\\hat{\\beta}_{0}\\) and \\(\\hat{\\beta}_{1}\\) to minimize our residuals using calculus\nA minimization problem is essentially the same as an optimization problem where we find the point at which our choices have a slope of zero\nTo begin, let’s properly write out our minimization problem:\n\\[\n    \\min_{\\hat{\\beta}_{0}, \\hat{\\beta}_{1}} \\;\\; \\sum_{i} u_{i}^{2}\n\\]\n\\[\n    \\min_{\\hat{\\beta}_{0}, \\hat{\\beta}_{1}} \\; \\sum_{i} (y_{i} - \\hat{y}_{i})^{2}\n\\]\n\\[\n    \\min_{\\hat{\\beta}_{0}, \\hat{\\beta}_{1}} \\; \\sum_{i} (y_{i} - \\hat{\\beta}_{0} - \\hat{\\beta}_{1}x_{i}) (y_{i} - \\hat{\\beta}_{0} - \\hat{\\beta}_{1}x_{i})\n\\]"
  },
  {
    "objectID": "lectures/03-Estimators-01/034-ols.html#ols-calculus-1",
    "href": "lectures/03-Estimators-01/034-ols.html#ols-calculus-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "OLS & Calculus",
    "text": "OLS & Calculus\nThe calculus we’ll use is by finding the derivatives of the function with respect to \\(\\hat{\\beta}_{0}\\) and \\(\\hat{\\beta}_{1}\\).\nIt’s a lot of algebra but it is simple math, just a lot of it:\n\n\\[\\begin{align*}\n    \\min_{\\hat{\\beta}_{0}, \\hat{\\beta}_{1}} &\\;\n    \\sum_{i} y_{i}^{2} - \\hat{\\beta}_{0}y_{i} - \\hat{\\beta}_{1}x_{i}y_{i} - \\hat{\\beta}_{0}y_{i} + \\hat{\\beta}_{0}^{2} + \\hat{\\beta}_{0}\\hat{\\beta}_{1}x_{i} - \\hat{\\beta}_{1}x_{i}y_{i} + \\hat{\\beta}_{0}\\hat{\\beta}_{1}x_{i} + \\hat{\\beta}_{1}^{2}x_{i}^{2} \\\\\n    \\min_{\\hat{\\beta}_{0}, \\hat{\\beta}_{1}} &\\;\n    \\sum_{i} y_{i}^{2} - 2 \\hat{\\beta}_{0}y_{i} + \\hat{\\beta}_{0}^{2} - 2 \\hat{\\beta}_{1}x_{i}y_{i} + 2\\hat{\\beta}_{0}\\hat{\\beta}_{1}x_{i} + \\hat{\\beta}_{1}^{2}x_{i}^{2}\n\\end{align*}\\]\n\nThen, we take partial derivatives over our choices \\(\\hat{\\beta}_{0}\\) and \\(\\hat{\\beta}_{1}\\) to figure the best choices.\nThese are called First Order Conditions (FOCs)"
  },
  {
    "objectID": "lectures/03-Estimators-01/034-ols.html#ols-calculus-2",
    "href": "lectures/03-Estimators-01/034-ols.html#ols-calculus-2",
    "title": "EC 320 - Intro. Econometrics",
    "section": "OLS & Calculus",
    "text": "OLS & Calculus\nTo find our choices, we find the partial derivative and set it equal to 0\nFor our intercept \\(\\hat{\\beta}_{0}\\):\n\n\\[\\begin{align*}\n    &\\dfrac{\\partial u_{i}}{\\partial \\hat{\\beta}_{0}} = 0 \\\\\n    \\sum_{i} -2y_{i} + &2\\hat{\\beta}_{0} + 2\\hat{\\beta}_{1}x_{i} = 0\n\\end{align*}\\]\n\nFor our slope \\(\\hat{\\beta}_{1}\\):\n\n\\[\\begin{align*}\n    &\\dfrac{\\partial u_{i}}{\\partial \\hat{\\beta}_{1}} = 0 \\\\\n    \\sum_{i} -2x_{i}y_{i} + &2\\hat{\\beta}_{0}x_{i} + 2\\hat{\\beta}_{1}x_{i}^{2} = 0\n\\end{align*}\\]"
  },
  {
    "objectID": "lectures/03-Estimators-01/034-ols.html#hatbeta_0-derivation",
    "href": "lectures/03-Estimators-01/034-ols.html#hatbeta_0-derivation",
    "title": "EC 320 - Intro. Econometrics",
    "section": "\\(\\hat{\\beta}_{0}\\) Derivation",
    "text": "\\(\\hat{\\beta}_{0}\\) Derivation\n\\[\n    \\sum_{i} -2y_{i} + 2\\hat{\\beta}_{0} + 2\\hat{\\beta}_{1}x_{i} = 0\n\\]\nOur task is to find solve the above for \\(\\hat{\\beta}_{0}\\):"
  },
  {
    "objectID": "lectures/03-Estimators-01/034-ols.html#hatbeta_1-derivation",
    "href": "lectures/03-Estimators-01/034-ols.html#hatbeta_1-derivation",
    "title": "EC 320 - Intro. Econometrics",
    "section": "\\(\\hat{\\beta}_{1}\\) Derivation",
    "text": "\\(\\hat{\\beta}_{1}\\) Derivation\n\\[\n    \\sum_{i} -2x_{i}y_{i} + 2\\hat{\\beta}_{0}x_{i} + 2\\hat{\\beta}_{1}x_{i}^{2} = 0\n\\]\nOur task is to find solve the above for \\(\\hat{\\beta}_{1}\\):"
  },
  {
    "objectID": "lectures/03-Estimators-01/034-ols.html#ols-formulas",
    "href": "lectures/03-Estimators-01/034-ols.html#ols-formulas",
    "title": "EC 320 - Intro. Econometrics",
    "section": "OLS Formulas",
    "text": "OLS Formulas\n\nIntercept\n\\[\n    \\hat{\\beta}_{0} = \\bar{y} - \\hat{\\beta}_{1}\\bar{x}\n\\]\n\nSlope Coefficient\n\\[\n    \\hat{\\beta}_{1} =\n    \\dfrac{\n        \\sum_{i=1}^{n} (y_{i} - \\bar{y})(x_{i} - \\bar{x})\n    }{\n        \\sum_{i=1}^{n} (x_{i} - \\bar{x})^{2}\n    }\n\\]\nThese may look slightly different to my derivation. Part of your assignments is to bridge the gap."
  },
  {
    "objectID": "lectures/03-Estimators-01/032-linear-model.html",
    "href": "lectures/03-Estimators-01/032-linear-model.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Before we continue, let’s cover some important rules we will need to derive some OLS things in the near future:\nSummations \\((\\sum)\\) have certain rules that we cannot violate and are important to hold in mind:\n\n\n\\(\\sum_{i=1}^{n} x_{i} = x_{1} + x_{2} + \\cdots + x_{n}\\)\n\n\n\n\\(\\sum_{i} x_{i} + y_{i} = \\sum_{i} x_{i} + \\sum_{i} y_{i}\\)\n\n\n\n\\(\\sum_{i} x_{i} y_{i} \\neq \\sum_{i} x_{i} \\sum_{i} y_{i}\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/032-linear-model.html#sidebar-summation-rules",
    "href": "lectures/03-Estimators-01/032-linear-model.html#sidebar-summation-rules",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Before we continue, let’s cover some important rules we will need to derive some OLS things in the near future:\nSummations \\((\\sum)\\) have certain rules that we cannot violate and are important to hold in mind:\n\n\n\\(\\sum_{i=1}^{n} x_{i} = x_{1} + x_{2} + \\cdots + x_{n}\\)\n\n\n\n\\(\\sum_{i} x_{i} + y_{i} = \\sum_{i} x_{i} + \\sum_{i} y_{i}\\)\n\n\n\n\\(\\sum_{i} x_{i} y_{i} \\neq \\sum_{i} x_{i} \\sum_{i} y_{i}\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/032-linear-model.html#summation-rules",
    "href": "lectures/03-Estimators-01/032-linear-model.html#summation-rules",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Summation Rules",
    "text": "Summation Rules\n\\[\\sum_{i=1}^{n} x_{i} = x_{1} + x_{2} + \\cdots + x_{n}\\]\nLet \\(x\\) be the set of \\({1,5,2}\\) \\((x: \\{1,5,2\\})\\) then using our summation rule we have:\n\\[\n    \\sum_{i} x_{i} = 1 + 5 + 2 = 8.\n\\]"
  },
  {
    "objectID": "lectures/03-Estimators-01/032-linear-model.html#summation-rules-1",
    "href": "lectures/03-Estimators-01/032-linear-model.html#summation-rules-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Summation Rules",
    "text": "Summation Rules\n\\[\\sum_{i} x_{i} + y_{i} = \\sum_{i} x_{i} + \\sum_{i} y_{i}\\]\nLet \\(x: \\{1,5,2\\}\\) and \\(y: \\{1,2,1\\}\\), then using our summation rule we have:\n\\[\\begin{align*}\n    \\sum_{i} x_{i} + y_{i} &= x_{1} + y_{1} + x_{2} + y_{2} + x_{3} + y_{3} \\\\\n                           &= x_{1} + x_{2} + x_{3} + y_{1} + y_{2} + y_{3} \\\\\n                           &= 1 + 5 + 2 + 1 + 2 + 1 \\\\\n                           &= 12\n\\end{align*}\\]"
  },
  {
    "objectID": "lectures/03-Estimators-01/032-linear-model.html#summation-rules-2",
    "href": "lectures/03-Estimators-01/032-linear-model.html#summation-rules-2",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Summation Rules",
    "text": "Summation Rules\n\\[\\sum_{i} x_{i} y_{i} \\neq \\sum_{i} x_{i} \\sum_{i} y_{i}\\]\nIf we expand \\(\\sum_{i} x_{i} y_{i} \\neq \\sum_{i} x_{i} \\sum_{i} y_{i}\\), we get:\n\n\\[\\begin{align*}\n    x_{1}y_{1} + x_{2}y_{2} + x_{3}y_{3} \\neq (x_{1} + x_{2} + x_{3})(y_{1} + y_{2} + y_{3})\n\\end{align*}\\]\n\nI’ll leave it to you to use the above numbers to show this holds"
  },
  {
    "objectID": "lectures/03-Estimators-01/032-linear-model.html#linear-model-estimators",
    "href": "lectures/03-Estimators-01/032-linear-model.html#linear-model-estimators",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Linear Model Estimators",
    "text": "Linear Model Estimators\nWe will spend the rest of the course exploring how to use Ordinary Least Squares (OLS) to fit a linear model like:\n\\[\n    y_{i} = \\beta_{0} + \\beta_{1}x_{i} + u_{i},\n\\]\nThat is, if we wanted to hypothesize that some random variable \\(Y\\) depends on another random variable \\(X\\) and that there is a linear relationship between then, \\(\\beta_{0}\\) and \\(\\beta_{1}\\) are the parameters which describe the nature of that relationship.\nGiven a sample of \\(X\\) and \\(Y\\), we will derive unbiased estimators for the intercept \\(\\beta_{0}\\) and slope \\(\\beta_{1}\\). Those estimators help us combine observations of \\(X\\) and \\(Y\\) to estimate underlying relationships between them."
  },
  {
    "objectID": "lectures/03-Estimators-01/032-linear-model.html#the-linear-regression-model",
    "href": "lectures/03-Estimators-01/032-linear-model.html#the-linear-regression-model",
    "title": "EC 320 - Intro. Econometrics",
    "section": "The Linear Regression Model",
    "text": "The Linear Regression Model\nWe can estimate the effect of \\(X\\) on \\(Y\\) by estimating the model:\n\\[\n    y_{i} = \\beta_{0} + \\beta_{1}x_{i} + u_{i},\n\\]\n\n\\(y_i\\) is the dependent variable\n\\(x_i\\) is the independent variable (continuous)\n\\(\\beta_0\\) is the intercept parameter. \\(E\\left[ {y_i | x_i=0} \\right] = \\beta_0\\)\n\\(\\beta_1\\) is the slope parameter, which under the correct causal setting represents marginal change in \\(x_i\\)’s effect on \\(y_i\\). \\(\\frac{\\partial y_i}{\\partial x_i} = \\beta_1\\)\n\\(u_i\\) is an Error Term including all other (omitted) factors affecting \\(y_i\\)."
  },
  {
    "objectID": "lectures/03-Estimators-01/032-linear-model.html#the-error-term-u_i",
    "href": "lectures/03-Estimators-01/032-linear-model.html#the-error-term-u_i",
    "title": "EC 320 - Intro. Econometrics",
    "section": "The Error Term \\(u_{i}\\)",
    "text": "The Error Term \\(u_{i}\\)\n\\(u_{i}\\) is quite special\nConsider the data generating process of variable \\(y_{i}\\),\n\n\\(u_{i}\\) captures all unobserved variables that explain variation in \\(y_{i}\\)\n\n\nSome error will exist in all models, no model is perfect.\n\nOur aim is to minimize error under a set of constraints\n\n\nError is the price we are willing to accept for a simplified model"
  },
  {
    "objectID": "lectures/03-Estimators-01/032-linear-model.html#the-error-term",
    "href": "lectures/03-Estimators-01/032-linear-model.html#the-error-term",
    "title": "EC 320 - Intro. Econometrics",
    "section": "The Error Term",
    "text": "The Error Term\nFive items contribute to the existence of the disturbance term:\n. . .\n1. Omission of independent variables\n. . .\n\nOur description (model) of the relationship between \\(Y\\) and \\(X\\) is a simplification\nOther variables have been left out (omitted)"
  },
  {
    "objectID": "lectures/03-Estimators-01/032-linear-model.html#the-error-term-1",
    "href": "lectures/03-Estimators-01/032-linear-model.html#the-error-term-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "The Error Term",
    "text": "The Error Term\nFive items contribute to the existence of the disturbance term:\n1. Omission of independent variables\n2. Aggregation of Variables\n. . .\n\nMicroeconomic relationships are often summarized\nEx. Housing prices (\\(X\\)) are described by county-level median home value data"
  },
  {
    "objectID": "lectures/03-Estimators-01/032-linear-model.html#the-error-term-2",
    "href": "lectures/03-Estimators-01/032-linear-model.html#the-error-term-2",
    "title": "EC 320 - Intro. Econometrics",
    "section": "The Error Term",
    "text": "The Error Term\nFive items contribute to the existence of the disturbance term:\n1. Omission of independent variables\n2. Aggregation of Variables\n3. Model misspecificiation\n. . .\n\nModel structure is incorrectly specified\nEx. \\(Y\\) depends on the anticipated value of \\(X\\) in the previous period, not \\(X\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/032-linear-model.html#the-error-term-3",
    "href": "lectures/03-Estimators-01/032-linear-model.html#the-error-term-3",
    "title": "EC 320 - Intro. Econometrics",
    "section": "The Error Term",
    "text": "The Error Term\nFive items contribute to the existence of the disturbance term:\n1. Omission of independent variables\n2. Aggregation of Variables\n3. Model misspecificiation\n4. Functional misspecificiation\n. . .\n\nThe functional relationship is specified incorrectly\nTrue relationship is nonlinear, not linear"
  },
  {
    "objectID": "lectures/03-Estimators-01/032-linear-model.html#the-error-term-4",
    "href": "lectures/03-Estimators-01/032-linear-model.html#the-error-term-4",
    "title": "EC 320 - Intro. Econometrics",
    "section": "The Error Term",
    "text": "The Error Term\nFive items contribute to the existence of the disturbance term:\n1. Omission of independent variables\n2. Aggregation of Variables\n3. Model misspecificiation\n4. Functional misspecificiation\n5. Measurement error\n. . .\n\nMeasurement of the variables in the data is just wrong\n\\(Y\\) or \\(X\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/032-linear-model.html#the-error-term-5",
    "href": "lectures/03-Estimators-01/032-linear-model.html#the-error-term-5",
    "title": "EC 320 - Intro. Econometrics",
    "section": "The Error Term",
    "text": "The Error Term\nFive items contribute to the existence of the disturbance term:\n1. Omission of independent variables\n2. Aggregation of Variables\n3. Model misspecificiation\n4. Functional misspecificiation\n5. Measurement error"
  },
  {
    "objectID": "lectures/03-Estimators-01/032-linear-model.html#running-a-regression-model",
    "href": "lectures/03-Estimators-01/032-linear-model.html#running-a-regression-model",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Running a Regression Model",
    "text": "Running a Regression Model\nUsing an estimator with data on \\(x_{i}\\) and \\(y_{i}\\), we can estimate a fitted regression line:\n\\[\n    \\hat{y}_{i} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1}x_{i}\n\\]\n\n\\(\\hat{y}_{i}\\) is the fitted value of \\(y_{i}\\)\n\\(\\hat{\\beta}_{0}\\) is the estimated intercept\n\\(\\hat{\\beta}_{1}\\) is the estimated slope\n\nThis procedure produces misses, known as residuals \\(y_{i} - \\hat{y_{i}}\\)\nLet’s look at an example of how this works"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#why-estimate-things",
    "href": "lectures/03-Estimators-01/030-compile.html#why-estimate-things",
    "title": "Estimators",
    "section": "Why Estimate Things?",
    "text": "Why Estimate Things?\nWe estimate because we cannot measure everything\nSuppose we want to know the average height of the US population.\n\nWe only have a sample of 1 million Americans\n\nHow can we use these data to estimate the height of the population?\nWe will learn what we can do"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#estimators-1",
    "href": "lectures/03-Estimators-01/030-compile.html#estimators-1",
    "title": "Estimators",
    "section": "Estimators",
    "text": "Estimators\nLet’s define some concepts first:\nEstimand\n\nQuantity that is to be estimated in a statistical analysis\n\nEstimator\n\nA rule (or formula) for estimating an unknown population parameter given a sample of data\n\nEstimate\n\nA specific numerical value that we obtain from the smaple data by applying the estimator"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#estimators-example",
    "href": "lectures/03-Estimators-01/030-compile.html#estimators-example",
    "title": "Estimators",
    "section": "Estimators Example",
    "text": "Estimators Example\nSuppose we want to know the average height of the population in the US\n\nWe have a sample of 1 million Americans\n\nSo then we can identify our Estimand, Estimator, and Estimate\n\nEstimand: The population mean \\((\\mu)\\)\nEstimator: The sample mean \\((\\bar{X})\\)\n\n\\[\n    \\bar{X} = \\dfrac{1}{n} \\sum_{i=1}^{n} X_{i}\n\\]\n\nEstimate: The sample mean \\((\\hat{\\mu} = 5'6'')\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#properties-of-estimators",
    "href": "lectures/03-Estimators-01/030-compile.html#properties-of-estimators",
    "title": "Estimators",
    "section": "Properties of Estimators",
    "text": "Properties of Estimators\nThere are many ways to estimate things and they all have their benefits and costs.\nImagine we want to estimate an unknown parameter \\(\\mu\\), and we know the distributions of three competing estimators.\nWhich one should we use?"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#properties-of-estimators---unbiasedness",
    "href": "lectures/03-Estimators-01/030-compile.html#properties-of-estimators---unbiasedness",
    "title": "Estimators",
    "section": "Properties of Estimators - Unbiasedness",
    "text": "Properties of Estimators - Unbiasedness\nWe ask: What properties make an estimator reliable?\nAnswer (1): Unbiasedness\nOn average, does the estimator tend toward the correct value?\n\nFormally: Does the mean of the estimator’s distribution equal the parameter it estimates?\n\\[\n    \\text{Bias}_{\\mu} (\\hat{\\mu}) = E[\\hat{\\mu}] - \\mu\n\\]"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#properties-of-estimators-1",
    "href": "lectures/03-Estimators-01/030-compile.html#properties-of-estimators-1",
    "title": "Estimators",
    "section": "Properties of estimators",
    "text": "Properties of estimators\nQuestion What properties make an estimator reliable?\nA01: Unbiasedness\n\n\nUnbiased estimator: \\(E\\left[ \\hat{\\mu} \\right] = \\mu\\)\n\n\n\n\n\n\n\n\n\n\nBiased estimator \\(E\\left[ \\hat{\\mu} \\right] \\neq \\mu\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#properties-of-estimators---efficiency",
    "href": "lectures/03-Estimators-01/030-compile.html#properties-of-estimators---efficiency",
    "title": "Estimators",
    "section": "Properties of Estimators - Efficiency",
    "text": "Properties of Estimators - Efficiency\nWe ask: What properties make an estimator reliable?\nAnswer (1): Efficiency (Low Variance)\nThe central tendencies (means) of competing distribution are not the only things that matter. We also care about the variance of an estimator.\n\\[\n    Var(\\hat{\\mu}) = E \\left[ (\\hat{\\mu} - E[\\hat{\\mu}])^{2} \\right]\n\\]\nLower variance estimators estimate closer to the mean in each sample"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#properties-of-estimators---efficiency-1",
    "href": "lectures/03-Estimators-01/030-compile.html#properties-of-estimators---efficiency-1",
    "title": "Estimators",
    "section": "Properties of Estimators - Efficiency",
    "text": "Properties of Estimators - Efficiency\nImagine low variance to be similar to accuracy \\(\\rightarrow\\) tighter estimates"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#the-bias-variance-tradeoff",
    "href": "lectures/03-Estimators-01/030-compile.html#the-bias-variance-tradeoff",
    "title": "Estimators",
    "section": "The Bias-Variance Tradeoff",
    "text": "The Bias-Variance Tradeoff\nMuch like everything, there are tradeoffs from gaining one thing over another.\nShould we be willing to take a bit of bias to reduce the variance?\nIn economics/causal inference, we emphasize unbiasedness"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#unbiased-estimators",
    "href": "lectures/03-Estimators-01/030-compile.html#unbiased-estimators",
    "title": "Estimators",
    "section": "Unbiased estimators",
    "text": "Unbiased estimators\nIn addition to the sample mean, there are other unbiased estimators we will often use\n\n\nSample variance estimates the variance \\(\\sigma^{2}\\)\n\n\n\nSample covariance setimates the covariance \\(\\sigma_{XY}\\)\n\n\n\nSample correlation estimates the pop. correlation coefficient \\(\\rho_{XY}\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#sample-variance",
    "href": "lectures/03-Estimators-01/030-compile.html#sample-variance",
    "title": "Estimators",
    "section": "Sample Variance",
    "text": "Sample Variance\nThe sample variance, \\(S_{X}^{2}\\), is an unbiased estimator of the population variance\n\n\\[\n    S_{X}^{2} = \\dfrac{1}{n - 1} \\sum_{i=1}^{n} (X_{i} - \\bar{X})^{2}.\n\\]"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#sample-covariance",
    "href": "lectures/03-Estimators-01/030-compile.html#sample-covariance",
    "title": "Estimators",
    "section": "Sample Covariance",
    "text": "Sample Covariance\nThe sample covariance, \\(S_{XY}\\), is an unbiaed estimator of the population covariance\n\n\\[\n    S_{XY} = \\dfrac{1}{n-1} \\sum_{i=1}^{n} (X_{i} - \\bar{X})(Y_{i} - \\bar{Y}).\n\\]"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#sample-correlation",
    "href": "lectures/03-Estimators-01/030-compile.html#sample-correlation",
    "title": "Estimators",
    "section": "Sample Correlation",
    "text": "Sample Correlation\nSample correlation, \\(r_{XY}\\), is an unbiased estimator of the population correlation coefficient\n\n\\[\n    r_{XY} = \\dfrac{S_{XY}}{\\sqrt{S_{X}^{2}}\\sqrt{S_{Y}^{2}}}.\n\\]"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#sidebar-summation-rules",
    "href": "lectures/03-Estimators-01/030-compile.html#sidebar-summation-rules",
    "title": "Estimators",
    "section": "Sidebar: Summation Rules",
    "text": "Sidebar: Summation Rules\nBefore we continue, let’s cover some important rules we will need to derive some OLS things in the near future:\nSummations \\((\\sum)\\) have certain rules that we cannot violate and are important to hold in mind:\n\n\n\\(\\sum_{i=1}^{n} x_{i} = x_{1} + x_{2} + \\cdots + x_{n}\\)\n\n\n\n\\(\\sum_{i} x_{i} + y_{i} = \\sum_{i} x_{i} + \\sum_{i} y_{i}\\)\n\n\n\n\\(\\sum_{i} x_{i} y_{i} \\neq \\sum_{i} x_{i} \\sum_{i} y_{i}\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#summation-rules",
    "href": "lectures/03-Estimators-01/030-compile.html#summation-rules",
    "title": "Estimators",
    "section": "Summation Rules",
    "text": "Summation Rules\n\\[\\sum_{i=1}^{n} x_{i} = x_{1} + x_{2} + \\cdots + x_{n}\\]\nLet \\(x\\) be the set of \\({1,5,2}\\) \\((x: \\{1,5,2\\})\\) then using our summation rule we have:\n\\[\n    \\sum_{i} x_{i} = 1 + 5 + 2 = 8.\n\\]"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#summation-rules-1",
    "href": "lectures/03-Estimators-01/030-compile.html#summation-rules-1",
    "title": "Estimators",
    "section": "Summation Rules",
    "text": "Summation Rules\n\\[\\sum_{i} x_{i} + y_{i} = \\sum_{i} x_{i} + \\sum_{i} y_{i}\\]\nLet \\(x: \\{1,5,2\\}\\) and \\(y: \\{1,2,1\\}\\), then using our summation rule we have:\n\\[\\begin{align*}\n    \\sum_{i} x_{i} + y_{i} &= x_{1} + y_{1} + x_{2} + y_{2} + x_{3} + y_{3} \\\\\n                           &= x_{1} + x_{2} + x_{3} + y_{1} + y_{2} + y_{3} \\\\\n                           &= 1 + 5 + 2 + 1 + 2 + 1 \\\\\n                           &= 12\n\\end{align*}\\]"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#summation-rules-2",
    "href": "lectures/03-Estimators-01/030-compile.html#summation-rules-2",
    "title": "Estimators",
    "section": "Summation Rules",
    "text": "Summation Rules\n\\[\\sum_{i} x_{i} y_{i} \\neq \\sum_{i} x_{i} \\sum_{i} y_{i}\\]\nIf we expand \\(\\sum_{i} x_{i} y_{i} \\neq \\sum_{i} x_{i} \\sum_{i} y_{i}\\), we get:\n\n\\[\\begin{align*}\n    x_{1}y_{1} + x_{2}y_{2} + x_{3}y_{3} \\neq (x_{1} + x_{2} + x_{3})(y_{1} + y_{2} + y_{3})\n\\end{align*}\\]\n\nI’ll leave it to you to use the above numbers to show this holds"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#linear-model-estimators",
    "href": "lectures/03-Estimators-01/030-compile.html#linear-model-estimators",
    "title": "Estimators",
    "section": "Linear Model Estimators",
    "text": "Linear Model Estimators\nWe will spend the rest of the course exploring how to use Ordinary Least Squares (OLS) to fit a linear model like:\n\\[\n    y_{i} = \\beta_{0} + \\beta_{1}x_{i} + u_{i},\n\\]\nThat is, if we wanted to hypothesize that some random variable \\(Y\\) depends on another random variable \\(X\\) and that there is a linear relationship between then, \\(\\beta_{0}\\) and \\(\\beta_{1}\\) are the parameters which describe the nature of that relationship.\nGiven a sample of \\(X\\) and \\(Y\\), we will derive unbiased estimators for the intercept \\(\\beta_{0}\\) and slope \\(\\beta_{1}\\). Those estimators help us combine observations of \\(X\\) and \\(Y\\) to estimate underlying relationships between them."
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#the-linear-regression-model",
    "href": "lectures/03-Estimators-01/030-compile.html#the-linear-regression-model",
    "title": "Estimators",
    "section": "The Linear Regression Model",
    "text": "The Linear Regression Model\nWe can estimate the effect of \\(X\\) on \\(Y\\) by estimating the model:\n\\[\n    y_{i} = \\beta_{0} + \\beta_{1}x_{i} + u_{i},\n\\]\n\n\\(y_i\\) is the dependent variable\n\\(x_i\\) is the independent variable (continuous)\n\\(\\beta_0\\) is the intercept parameter. \\(E\\left[ {y_i | x_i=0} \\right] = \\beta_0\\)\n\\(\\beta_1\\) is the slope parameter, which under the correct causal setting represents marginal change in \\(x_i\\)’s effect on \\(y_i\\). \\(\\frac{\\partial y_i}{\\partial x_i} = \\beta_1\\)\n\\(u_i\\) is an Error Term including all other (omitted) factors affecting \\(y_i\\)."
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#the-error-term-u_i",
    "href": "lectures/03-Estimators-01/030-compile.html#the-error-term-u_i",
    "title": "Estimators",
    "section": "The Error Term \\(u_{i}\\)",
    "text": "The Error Term \\(u_{i}\\)\n\\(u_{i}\\) is quite special\nConsider the data generating process of variable \\(y_{i}\\),\n\n\\(u_{i}\\) captures all unobserved variables that explain variation in \\(y_{i}\\)\n\n\nSome error will exist in all models, no model is perfect.\n\nOur aim is to minimize error under a set of constraints\n\n\nError is the price we are willing to accept for a simplified model"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#the-error-term",
    "href": "lectures/03-Estimators-01/030-compile.html#the-error-term",
    "title": "Estimators",
    "section": "The Error Term",
    "text": "The Error Term\nFive items contribute to the existence of the disturbance term:\n\n1. Omission of independent variables\n\n\n\nOur description (model) of the relationship between \\(Y\\) and \\(X\\) is a simplification\nOther variables have been left out (omitted)"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#the-error-term-1",
    "href": "lectures/03-Estimators-01/030-compile.html#the-error-term-1",
    "title": "Estimators",
    "section": "The Error Term",
    "text": "The Error Term\nFive items contribute to the existence of the disturbance term:\n1. Omission of independent variables\n2. Aggregation of Variables\n\n\nMicroeconomic relationships are often summarized\nEx. Housing prices (\\(X\\)) are described by county-level median home value data"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#the-error-term-2",
    "href": "lectures/03-Estimators-01/030-compile.html#the-error-term-2",
    "title": "Estimators",
    "section": "The Error Term",
    "text": "The Error Term\nFive items contribute to the existence of the disturbance term:\n1. Omission of independent variables\n2. Aggregation of Variables\n3. Model misspecificiation\n\n\nModel structure is incorrectly specified\nEx. \\(Y\\) depends on the anticipated value of \\(X\\) in the previous period, not \\(X\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#the-error-term-3",
    "href": "lectures/03-Estimators-01/030-compile.html#the-error-term-3",
    "title": "Estimators",
    "section": "The Error Term",
    "text": "The Error Term\nFive items contribute to the existence of the disturbance term:\n1. Omission of independent variables\n2. Aggregation of Variables\n3. Model misspecificiation\n4. Functional misspecificiation\n\n\nThe functional relationship is specified incorrectly\nTrue relationship is nonlinear, not linear"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#the-error-term-4",
    "href": "lectures/03-Estimators-01/030-compile.html#the-error-term-4",
    "title": "Estimators",
    "section": "The Error Term",
    "text": "The Error Term\nFive items contribute to the existence of the disturbance term:\n1. Omission of independent variables\n2. Aggregation of Variables\n3. Model misspecificiation\n4. Functional misspecificiation\n5. Measurement error\n\n\nMeasurement of the variables in the data is just wrong\n\\(Y\\) or \\(X\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#the-error-term-5",
    "href": "lectures/03-Estimators-01/030-compile.html#the-error-term-5",
    "title": "Estimators",
    "section": "The Error Term",
    "text": "The Error Term\nFive items contribute to the existence of the disturbance term:\n1. Omission of independent variables\n2. Aggregation of Variables\n3. Model misspecificiation\n4. Functional misspecificiation\n5. Measurement error"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#running-a-regression-model",
    "href": "lectures/03-Estimators-01/030-compile.html#running-a-regression-model",
    "title": "Estimators",
    "section": "Running a Regression Model",
    "text": "Running a Regression Model\nUsing an estimator with data on \\(x_{i}\\) and \\(y_{i}\\), we can estimate a fitted regression line:\n\\[\n    \\hat{y}_{i} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1}x_{i}\n\\]\n\n\\(\\hat{y}_{i}\\) is the fitted value of \\(y_{i}\\)\n\\(\\hat{\\beta}_{0}\\) is the estimated intercept\n\\(\\hat{\\beta}_{1}\\) is the estimated slope\n\nThis procedure produces misses, known as residuals \\(y_{i} - \\hat{y_{i}}\\)\nLet’s look at an example of how this works"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#ex.-effect-of-police-on-crime",
    "href": "lectures/03-Estimators-01/030-compile.html#ex.-effect-of-police-on-crime",
    "title": "Estimators",
    "section": "Ex. Effect of police on crime",
    "text": "Ex. Effect of police on crime\n\nEmpirical question:\n\nDoes the number of on-campus police officers affect campus crime rates? If so, by how much?\n\n\n\n\nAlways plot your data first"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#ex.-effect-of-police-on-crime-1",
    "href": "lectures/03-Estimators-01/030-compile.html#ex.-effect-of-police-on-crime-1",
    "title": "Estimators",
    "section": "Ex. Effect of police on crime",
    "text": "Ex. Effect of police on crime\nThe scatter plot suggest that a weak positive relationship exists\n\nA sample correlation of 0.14 confirms this\n\n\n\nBut correlation does not imply causation\n\n\n\nLets estimate a statistical model"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#ex.-effect-of-police-on-crime-2",
    "href": "lectures/03-Estimators-01/030-compile.html#ex.-effect-of-police-on-crime-2",
    "title": "Estimators",
    "section": "Ex. Effect of police on crime",
    "text": "Ex. Effect of police on crime\nWe express the relationship between a dependent variable and an independent variable as linear:\n\\[\n{\\text{Crime}_i} = \\beta_0 + \\beta_1 \\text{Police}_i + u_i.\n\\]\n\n\\(\\beta_0\\) is the intercept or constant.\n\\(\\beta_1\\) is the slope coefficient.\n\\(u_i\\) is an error term or disturbance term."
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#ex.-effect-of-police-on-crime-3",
    "href": "lectures/03-Estimators-01/030-compile.html#ex.-effect-of-police-on-crime-3",
    "title": "Estimators",
    "section": "Ex. Effect of police on crime",
    "text": "Ex. Effect of police on crime\nThe intercept tells us the expected value of \\(\\text{Crime}_i\\) when \\(\\text{Police}_i = 0\\).\n\\[\n\\text{Crime}_i = {\\color{#BF616A} \\beta_{0}} + \\beta_1\\text{Police}_i + u_i\n\\]\nUsually not the focus of an analysis."
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#ex.-effect-of-police-on-crime-4",
    "href": "lectures/03-Estimators-01/030-compile.html#ex.-effect-of-police-on-crime-4",
    "title": "Estimators",
    "section": "Ex. Effect of police on crime",
    "text": "Ex. Effect of police on crime\nThe slope coefficient tells us the expected change in \\(\\text{Crime}_i\\) when \\(\\text{Police}_i\\) increases by one.\n\\[\n\\text{Crime}_i = \\beta_0 + {\\color{#BF616A} \\beta_1} \\text{Police}_i + u_i\n\\]\n“A one-unit increase in \\(\\text{Police}_i\\) is associated with a \\(\\color{#BF616A}{\\beta_1}\\)-unit increase in \\(\\text{Crime}_i\\).”\n\nInterpretation of this parameter is crucial\n\n\nUnder certain (strong) assumptions1, \\(\\color{#BF616A}{\\beta_1}\\) is the effect of \\(X_i\\) on \\(Y_i\\).\n\nOtherwise, it’s the association of \\(X_i\\) with \\(Y_i\\).\n\n\nAssumptions regarding the error term"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#ex.-effect-of-police-on-crime-5",
    "href": "lectures/03-Estimators-01/030-compile.html#ex.-effect-of-police-on-crime-5",
    "title": "Estimators",
    "section": "Ex. Effect of police on crime",
    "text": "Ex. Effect of police on crime\nThe error term reminds us that \\(\\text{Police}_i\\) does not perfectly explain \\(Y_i\\).\n\\[\n\\text{Crime}_i = \\beta_0 + \\beta_1\\text{Police}_i + {\\color{#BF616A} u_i}\n\\]\nRepresents all other factors that explain \\(\\text{Crime}_i\\).\n\nUseful mnemonic: pretend that \\(u\\) stands for “unobserved” or “unexplained.”"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#ex.-effect-of-police-on-crime-6",
    "href": "lectures/03-Estimators-01/030-compile.html#ex.-effect-of-police-on-crime-6",
    "title": "Estimators",
    "section": "Ex. Effect of police on crime",
    "text": "Ex. Effect of police on crime\nHow might we apply the simple linear regression model to our question about the effect of on-campus police on campus crime?\n\\[\n\\text{Crime}_i = \\beta_0 + \\beta_1\\text{Police}_i + u_i.\n\\]\n\n\\(\\beta_0\\) is the crime rate for colleges without police.\n\\(\\beta_1\\) is the increase in the crime rate for an additional police officer per 1000 students."
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#ex.-effect-of-police-on-crime-7",
    "href": "lectures/03-Estimators-01/030-compile.html#ex.-effect-of-police-on-crime-7",
    "title": "Estimators",
    "section": "Ex. Effect of police on crime",
    "text": "Ex. Effect of police on crime\nHow might we apply the simple linear regression model to our question?\n\\[\n\\text{Crime}_i = \\beta_0 + \\beta_1\\text{Police}_i + u_i\n\\]\n\\(\\beta_0\\) and \\(\\beta_1\\) are the unobserved population parameters we want\n\n\nWe estimate\n\n\\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) generate predictions of \\(\\text{Crime}_i\\) called \\(\\widehat{\\text{Crime}_i}\\).\nWe call the predictions of the dependent variable fitted values.\n\n\n\n\nTogether, these trace a line: \\(\\widehat{\\text{Crime}_i} = \\hat{\\beta_0} + \\hat{\\beta_1}\\text{Police}_i\\)."
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#section-1",
    "href": "lectures/03-Estimators-01/030-compile.html#section-1",
    "title": "Estimators",
    "section": "",
    "text": "So, the question becomes, how do I pick \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#section-2",
    "href": "lectures/03-Estimators-01/030-compile.html#section-2",
    "title": "Estimators",
    "section": "",
    "text": "Let’s take some guesses: \\(\\hat{\\beta_0} = 60\\) and \\(\\hat{\\beta}_{1} = -7\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#section-3",
    "href": "lectures/03-Estimators-01/030-compile.html#section-3",
    "title": "Estimators",
    "section": "",
    "text": "Let’s take some guesses: \\(\\hat{\\beta_0} = 30\\) and \\(\\hat{\\beta}_{1} = 0\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#section-4",
    "href": "lectures/03-Estimators-01/030-compile.html#section-4",
    "title": "Estimators",
    "section": "",
    "text": "Let’s take some guesses: \\(\\hat{\\beta_0} = 15.6\\) and \\(\\hat{\\beta}_{1} = 7.94\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#residuals",
    "href": "lectures/03-Estimators-01/030-compile.html#residuals",
    "title": "Estimators",
    "section": "Residuals",
    "text": "Residuals\nUsing \\(\\hat{\\beta}_{0}\\) and \\(\\hat{\\beta}_{1}\\) to make \\(\\hat{y}_{i}\\) generates misses.\n\n\n\n \\(\\hat{\\beta_0} = 60 \\;\\) Guess\n\n \\(\\hat{\\beta_0} = 30 \\;\\) Guess\n\n \\(\\hat{\\beta_0} = 15 \\;\\) Guess"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#residuals-sum-of-squares-rss",
    "href": "lectures/03-Estimators-01/030-compile.html#residuals-sum-of-squares-rss",
    "title": "Estimators",
    "section": "Residuals Sum of Squares (RSS)",
    "text": "Residuals Sum of Squares (RSS)\nWhat if we picked an estimator that minimizes the residuals?\nWhy do we not minimize:\n\\[\n    \\sum_{i=1}^{n} \\hat{u}_{i}^{2}\n\\]\nso that the estimator makes fewer big misses?\nThis estimator, the residual sum of squares (RSS), is convenient because squared numbers are never negative so we can minimze an absolut sum of the residuals\nRSS will give bigger penalties to bigger residuals"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#minimizing-rss",
    "href": "lectures/03-Estimators-01/030-compile.html#minimizing-rss",
    "title": "Estimators",
    "section": "Minimizing RSS",
    "text": "Minimizing RSS\nWe could test thousands of guesses of \\(\\beta_0\\) and \\(\\beta_1\\) an pick the pair the has the smallest RSS\nWe could painstakingly do that, and eventually figure out which one fits best.\nOr… We could just do a little math"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#ols",
    "href": "lectures/03-Estimators-01/030-compile.html#ols",
    "title": "Estimators",
    "section": "OLS",
    "text": "OLS\nThe OLS Estimator chooses the parameters \\(\\hat{\\beta}_{0}\\) and \\(\\hat{\\beta}_{1}\\) that minimize the Residual Sum of Squares (RSS)\n\\[\n    \\min_{\\hat{\\beta}_{0},\\hat{\\beta}_{1}} \\sum_{i=1}^{n} \\hat{u}_{i}^{2}\n\\]\nThis is why we call the estimator ordinary least squares\nRecall that residuals are given by \\(y_{i} - \\hat{y}_{i}\\) and that:\n\\[\n    \\hat{y}_{i} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1} x_{i}\n\\]\nThen\n\\[\n    u_{i} = y_{i} - \\hat{\\beta}_{0} + \\hat{\\beta}_{1} x_{i}\n\\]"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#ols-calculus",
    "href": "lectures/03-Estimators-01/030-compile.html#ols-calculus",
    "title": "Estimators",
    "section": "OLS & Calculus",
    "text": "OLS & Calculus\nWe can find our choices \\(\\hat{\\beta}_{0}\\) and \\(\\hat{\\beta}_{1}\\) to minimize our residuals using calculus\nA minimization problem is essentially the same as an optimization problem where we find the point at which our choices have a slope of zero\nTo begin, let’s properly write out our minimization problem:\n\\[\n    \\min_{\\hat{\\beta}_{0}, \\hat{\\beta}_{1}} \\;\\; \\sum_{i} u_{i}^{2}\n\\]\n\\[\n    \\min_{\\hat{\\beta}_{0}, \\hat{\\beta}_{1}} \\; \\sum_{i} (y_{i} - \\hat{y}_{i})^{2}\n\\]\n\\[\n    \\min_{\\hat{\\beta}_{0}, \\hat{\\beta}_{1}} \\; \\sum_{i} (y_{i} - \\hat{\\beta}_{0} - \\hat{\\beta}_{1}x_{i}) (y_{i} - \\hat{\\beta}_{0} - \\hat{\\beta}_{1}x_{i})\n\\]"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#ols-calculus-1",
    "href": "lectures/03-Estimators-01/030-compile.html#ols-calculus-1",
    "title": "Estimators",
    "section": "OLS & Calculus",
    "text": "OLS & Calculus\nThe calculus we’ll use is by finding the derivatives of the function with respect to \\(\\hat{\\beta}_{0}\\) and \\(\\hat{\\beta}_{1}\\).\nIt’s a lot of algebra but it is simple math, just a lot of it:\n\n\\[\\begin{align*}\n    \\min_{\\hat{\\beta}_{0}, \\hat{\\beta}_{1}} &\\;\n    \\sum_{i} y_{i}^{2} - \\hat{\\beta}_{0}y_{i} - \\hat{\\beta}_{1}x_{i}y_{i} - \\hat{\\beta}_{0}y_{i} + \\hat{\\beta}_{0}^{2} + \\hat{\\beta}_{0}\\hat{\\beta}_{1}x_{i} - \\hat{\\beta}_{1}x_{i}y_{i} + \\hat{\\beta}_{0}\\hat{\\beta}_{1}x_{i} + \\hat{\\beta}_{1}^{2}x_{i}^{2} \\\\\n    \\min_{\\hat{\\beta}_{0}, \\hat{\\beta}_{1}} &\\;\n    \\sum_{i} y_{i}^{2} - 2 \\hat{\\beta}_{0}y_{i} + \\hat{\\beta}_{0}^{2} - 2 \\hat{\\beta}_{1}x_{i}y_{i} + 2\\hat{\\beta}_{0}\\hat{\\beta}_{1}x_{i} + \\hat{\\beta}_{1}^{2}x_{i}^{2}\n\\end{align*}\\]\n\nThen, we take partial derivatives over our choices \\(\\hat{\\beta}_{0}\\) and \\(\\hat{\\beta}_{1}\\) to figure the best choices.\nThese are called First Order Conditions (FOCs)"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#ols-calculus-2",
    "href": "lectures/03-Estimators-01/030-compile.html#ols-calculus-2",
    "title": "Estimators",
    "section": "OLS & Calculus",
    "text": "OLS & Calculus\nTo find our choices, we find the partial derivative and set it equal to 0\nFor our intercept \\(\\hat{\\beta}_{0}\\):\n\n\\[\\begin{align*}\n    &\\dfrac{\\partial u_{i}}{\\partial \\hat{\\beta}_{0}} = 0 \\\\\n    \\sum_{i} -2y_{i} + &2\\hat{\\beta}_{0} + 2\\hat{\\beta}_{1}x_{i} = 0\n\\end{align*}\\]\n\nFor our slope \\(\\hat{\\beta}_{1}\\):\n\n\\[\\begin{align*}\n    &\\dfrac{\\partial u_{i}}{\\partial \\hat{\\beta}_{1}} = 0 \\\\\n    \\sum_{i} -2x_{i}y_{i} + &2\\hat{\\beta}_{0}x_{i} + 2\\hat{\\beta}_{1}x_{i}^{2} = 0\n\\end{align*}\\]"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#hatbeta_0-derivation",
    "href": "lectures/03-Estimators-01/030-compile.html#hatbeta_0-derivation",
    "title": "Estimators",
    "section": "\\(\\hat{\\beta}_{0}\\) Derivation",
    "text": "\\(\\hat{\\beta}_{0}\\) Derivation\n\\[\n    \\sum_{i} -2y_{i} + 2\\hat{\\beta}_{0} + 2\\hat{\\beta}_{1}x_{i} = 0\n\\]\nOur task is to find solve the above for \\(\\hat{\\beta}_{0}\\):"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#hatbeta_1-derivation",
    "href": "lectures/03-Estimators-01/030-compile.html#hatbeta_1-derivation",
    "title": "Estimators",
    "section": "\\(\\hat{\\beta}_{1}\\) Derivation",
    "text": "\\(\\hat{\\beta}_{1}\\) Derivation\n\\[\n    \\sum_{i} -2x_{i}y_{i} + 2\\hat{\\beta}_{0}x_{i} + 2\\hat{\\beta}_{1}x_{i}^{2} = 0\n\\]\nOur task is to find solve the above for \\(\\hat{\\beta}_{1}\\):"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#ols-formulas",
    "href": "lectures/03-Estimators-01/030-compile.html#ols-formulas",
    "title": "Estimators",
    "section": "OLS Formulas",
    "text": "OLS Formulas\n\nIntercept\n\\[\n    \\hat{\\beta}_{0} = \\bar{y} - \\hat{\\beta}_{1}\\bar{x}\n\\]\n\nSlope Coefficient\n\\[\n    \\hat{\\beta}_{1} =\n    \\dfrac{\n        \\sum_{i=1}^{n} (y_{i} - \\bar{y})(x_{i} - \\bar{x})\n    }{\n        \\sum_{i=1}^{n} (x_{i} - \\bar{x})^{2}\n    }\n\\]\nThese may look slightly different to my derivation. Part of your assignments is to bridge the gap."
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#interpretation",
    "href": "lectures/03-Estimators-01/030-compile.html#interpretation",
    "title": "Estimators",
    "section": "Interpretation",
    "text": "Interpretation\nThere are two stages of interpretation of a regression equation\n\nInterpret regression estimates into words\nDeciding whether this interpretation should be taken at face value\n\n\nBoth stages are important, but for now, we will focus on the first\nLet’s revisit our crime example"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#ex-effect-of-police-on-crime",
    "href": "lectures/03-Estimators-01/030-compile.html#ex-effect-of-police-on-crime",
    "title": "Estimators",
    "section": "Ex: Effect of Police on Crime",
    "text": "Ex: Effect of Police on Crime\nUsing the OLS formulas, we get \\(\\hat{\\beta}_{0} = 18.41\\) and \\(\\hat{\\beta}_{1} = 1.76\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#coefficient-interpretation-1",
    "href": "lectures/03-Estimators-01/030-compile.html#coefficient-interpretation-1",
    "title": "Estimators",
    "section": "Coefficient Interpretation",
    "text": "Coefficient Interpretation\nHow do I interpret \\(\\hat{\\beta}_{0} = 18.41\\) and \\(\\hat{\\beta}_{1} = 1.76\\)?\nThe general interpration of the intercept is the estimated value of \\(y_{i}\\) when \\(x_{i} = 0\\)\nAnd the general interpretation of the slope parameter is the estimated change \\(y_{i}\\) for the marginal increase \\(x_{i}\\)\n\nFirst, it is important to understand the units:\n\n\\(\\widehat{\\text{Crime}}_{i}\\) is measured as a crime rate, the number of crimes per 1,000 students on campus\n\\(\\text{Police}_{i}\\) is also measured as a rate, the number of police officers per 1,000 students on campus"
  },
  {
    "objectID": "lectures/03-Estimators-01/030-compile.html#coefficient-interpretation-2",
    "href": "lectures/03-Estimators-01/030-compile.html#coefficient-interpretation-2",
    "title": "Estimators",
    "section": "Coefficient Interpretation",
    "text": "Coefficient Interpretation\nUsing OLS gives us the fitted line\n\\[\n\\widehat{\\text{Crime}_i} = \\hat{\\beta}_1 + \\hat{\\beta}_2\\text{Police}_i.\n\\]\nWhat does \\(\\hat{\\beta_0}\\) = \\(18.41\\) tell us? Without any police on campus, the crime rate is \\(18.41\\) per 1,000 people on campus\n\nWhat does \\(\\hat{\\beta_1}\\) = \\(1.76\\) tell us? For each additional police officer per 1,000, there is an associated increase in the crime rate by \\(1.76\\) crimes per 1,000 people on campus.\n\n\nDoes this mean that police cause crime? Probably not.\nThis is where deciding if the interpretation should be taken at face value. It now becomes your job to bring reason to the values."
  },
  {
    "objectID": "lectures/02-Vectors-and-Pipes-R/022-tidy.html",
    "href": "lectures/02-Vectors-and-Pipes-R/022-tidy.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Vectors\nPipes"
  },
  {
    "objectID": "lectures/02-Vectors-and-Pipes-R/022-tidy.html#topics",
    "href": "lectures/02-Vectors-and-Pipes-R/022-tidy.html#topics",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Vectors\nPipes"
  },
  {
    "objectID": "lectures/02-Vectors-and-Pipes-R/022-tidy.html#vectors",
    "href": "lectures/02-Vectors-and-Pipes-R/022-tidy.html#vectors",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Vectors",
    "text": "Vectors\nIn R, data is held in vectors. You can construct a vector using the function c(). c is short for “combine”.\nYou can combine elements to form a vector, for example:\n\n\nCombine numbers to form a numeric vector:\n\n\nc(5,3,9)\n\n[1] 5 3 9\n\n\n\n\n\nCombine character strings to form a character vector:\n\n\nc(\"apple\", \"banana\", \"strawberry\")\n\n[1] \"apple\"      \"banana\"     \"strawberry\"\n\n\n\n\nNote: character strings need quotes around them. numbers should not have quotes around them"
  },
  {
    "objectID": "lectures/02-Vectors-and-Pipes-R/020-compile.html#preview",
    "href": "lectures/02-Vectors-and-Pipes-R/020-compile.html#preview",
    "title": "Learning R: Vectors and Pipes",
    "section": "Preview",
    "text": "Preview\nIn this lecture, you will:\n\nDownload R and RStudio\nInstall some packages\nLearn how to create vectors and use pipes to apply functions\nComplete some exercises"
  },
  {
    "objectID": "lectures/02-Vectors-and-Pipes-R/020-compile.html#step-1-grab-a-computer",
    "href": "lectures/02-Vectors-and-Pipes-R/020-compile.html#step-1-grab-a-computer",
    "title": "Learning R: Vectors and Pipes",
    "section": "Step 1: Grab a Computer",
    "text": "Step 1: Grab a Computer\nGood news! If you are reading this, you are most likely on a computer already. Great work so far.\nThe good news about R is that it works on any operating system. I personally use it on both Mac and Windows."
  },
  {
    "objectID": "lectures/02-Vectors-and-Pipes-R/020-compile.html#step-2-download-and-install-r",
    "href": "lectures/02-Vectors-and-Pipes-R/020-compile.html#step-2-download-and-install-r",
    "title": "Learning R: Vectors and Pipes",
    "section": "Step 2: Download and Install R",
    "text": "Step 2: Download and Install R\nGo to https://cran.r-project.org/ and follow the instructions to download R for your device."
  },
  {
    "objectID": "lectures/02-Vectors-and-Pipes-R/020-compile.html#step-3-install-rstudio",
    "href": "lectures/02-Vectors-and-Pipes-R/020-compile.html#step-3-install-rstudio",
    "title": "Learning R: Vectors and Pipes",
    "section": "Step 3: Install RStudio",
    "text": "Step 3: Install RStudio\nGo to https://posit.co/download/rstudio-desktop/ and click the blue button that says step 2: install Rstudio Desktop. Follow the instructions to complete the installation."
  },
  {
    "objectID": "lectures/02-Vectors-and-Pipes-R/020-compile.html#step-4-open-rstudio-and-install-some-packages",
    "href": "lectures/02-Vectors-and-Pipes-R/020-compile.html#step-4-open-rstudio-and-install-some-packages",
    "title": "Learning R: Vectors and Pipes",
    "section": "Step 4: Open RStudio and Install Some Packages",
    "text": "Step 4: Open RStudio and Install Some Packages\nRun these lines of code in your console to make sure you have the tidyverse installed and attached to your current session.\n\ninstall.packages(\"tidyverse\", dependencies = TRUE)\nlibrary(tidyverse)\n\n\nAn important shortcut to installing and using packages is a package called pacman. It has a function p_load() which functions as both install.packages() and library() above.\n\ninstall.packages(\"pacman\")\n\nUse p_load to install the next set of packages"
  },
  {
    "objectID": "lectures/02-Vectors-and-Pipes-R/020-compile.html#step-4a-install-more-packages",
    "href": "lectures/02-Vectors-and-Pipes-R/020-compile.html#step-4a-install-more-packages",
    "title": "Learning R: Vectors and Pipes",
    "section": "Step 4a: Install More Packages",
    "text": "Step 4a: Install More Packages\nA former PhD Student (Colleen O’Briant) in the Econ Department created a lovely package that serves as an alternative set of beginner friendly help docs.\nRun this code to install it:\n\nlibrary(pacman)\np_load(Rcpp, devtools) # Note you can do several packages at once\n\ninstall_github(\"cobriant/qelp\")\n\nTo test everything worked, now run:\n\n?qelp::install.packages\n\nIf everything went right, the help docs she wrote on the function install.packages should pop up.\n\n\nNote: It is not a complete library of every function, but many of the basic ones we will use are included."
  },
  {
    "objectID": "lectures/02-Vectors-and-Pipes-R/020-compile.html#topics",
    "href": "lectures/02-Vectors-and-Pipes-R/020-compile.html#topics",
    "title": "Learning R: Vectors and Pipes",
    "section": "Topics",
    "text": "Topics\n\nVectors\nPipes"
  },
  {
    "objectID": "lectures/02-Vectors-and-Pipes-R/020-compile.html#vectors",
    "href": "lectures/02-Vectors-and-Pipes-R/020-compile.html#vectors",
    "title": "Learning R: Vectors and Pipes",
    "section": "Vectors",
    "text": "Vectors\nIn R, data is held in vectors. You can construct a vector using the function c(). c is short for “combine”.\nYou can combine elements to form a vector, for example:\n\n\nCombine numbers to form a numeric vector:\n\n\nc(5,3,9)\n\n[1] 5 3 9\n\n\n\n\n\nCombine character strings to form a character vector:\n\n\nc(\"apple\", \"banana\", \"strawberry\")\n\n[1] \"apple\"      \"banana\"     \"strawberry\"\n\n\n\n\nNote: character strings need quotes around them. numbers should not have quotes around them"
  },
  {
    "objectID": "lectures/02-Vectors-and-Pipes-R/020-compile.html#pipes",
    "href": "lectures/02-Vectors-and-Pipes-R/020-compile.html#pipes",
    "title": "Learning R: Vectors and Pipes",
    "section": "Pipes",
    "text": "Pipes\nThe pipe %&gt;% is the most frequently used function in the tidyverse.\nWhat It Does:\nSuppose you have some data x and you would like to apply some function f() on it. So you run f(x).\n\nFor example, take the vector 1:3 and find its minimum by applying min():\n\n\nmin(1:3)\n\n[1] 1\n\n\n\nAnother way to do the same things is to use a pipe:\n\n1:3 %&gt;% min()\n\n[1] 1"
  },
  {
    "objectID": "lectures/02-Vectors-and-Pipes-R/020-compile.html#pipes-1",
    "href": "lectures/02-Vectors-and-Pipes-R/020-compile.html#pipes-1",
    "title": "Learning R: Vectors and Pipes",
    "section": "Pipes",
    "text": "Pipes\nThe pipe simply takes the data that comes before it and inserts it into the function that comes after\nThe way you should read the pipe is with the word “then”, as in: “take x, then apply f()”\nWe are not limited to only one pipe. What if we wanted to take x then apply f(), then apply g(), then apply h().\nUsing pipes:\n\nx %&gt;% f() %&gt;% g() %&gt;% h()\n\nOr using multiple lines (this is easier to read once things get hectic)\n\nx %&gt;%\n    f() %&gt;% # Benefit: Allows you to comment on each function to keep track of what you are doing\n    g() %&gt;% # Hello\n    h() # Goodbye\n\nIf we did not use pipes, we’d have to read it inside-out h(g(f(x))), which is nasty."
  },
  {
    "objectID": "lectures/02-Vectors-and-Pipes-R/020-compile.html#practice-download-worksheet-01-from-the-site",
    "href": "lectures/02-Vectors-and-Pipes-R/020-compile.html#practice-download-worksheet-01-from-the-site",
    "title": "Learning R: Vectors and Pipes",
    "section": "Practice: Download “Worksheet 01” From the Site",
    "text": "Practice: Download “Worksheet 01” From the Site\nThis worksheet will help you learn coding by doing. You will learn:\n\nMath operators work on vectors\nmin() and sum() work on vectors\nHow to use length() to find the number of elements in a vector\nRepeat elements using rep()\nHow to do Random Sampling from a vector\nHow to pipe data through multiple functions"
  },
  {
    "objectID": "lectures/01-Random-Variables/015-correlation.html",
    "href": "lectures/01-Random-Variables/015-correlation.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "An issue with covariance is that the covariance between two random variables depends on the units those variables are measured in. That’s where correlation comes in:\nCorrelation is another measure of linear association that has the benefit of being dimensionless because the units in the numerator cancel with the units in the denominator.\nIt is also the case that the correlation between two variables is always between -1 and 1. Where correlation = 1, the two variables have a perfect positive linear relationsihp, and when correlation = -1, the two variables have a perfect negative linear relationship.\nWe will use the greek letter \\(\\rho\\) (“rho”) to refer to the correlation between two RVs. The formula is:\n\\[\\begin{align*}\n    \\rho_{XY} =\n    \\dfrac{\n        \\sigma_{XY}\n    }{\n        \\sqrt{\\sigma_{X}^{2}\\sigma_{Y}^{2}}\n    }\n\\end{align*}\\]"
  },
  {
    "objectID": "lectures/01-Random-Variables/015-correlation.html#definition",
    "href": "lectures/01-Random-Variables/015-correlation.html#definition",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "An issue with covariance is that the covariance between two random variables depends on the units those variables are measured in. That’s where correlation comes in:\nCorrelation is another measure of linear association that has the benefit of being dimensionless because the units in the numerator cancel with the units in the denominator.\nIt is also the case that the correlation between two variables is always between -1 and 1. Where correlation = 1, the two variables have a perfect positive linear relationsihp, and when correlation = -1, the two variables have a perfect negative linear relationship.\nWe will use the greek letter \\(\\rho\\) (“rho”) to refer to the correlation between two RVs. The formula is:\n\\[\\begin{align*}\n    \\rho_{XY} =\n    \\dfrac{\n        \\sigma_{XY}\n    }{\n        \\sqrt{\\sigma_{X}^{2}\\sigma_{Y}^{2}}\n    }\n\\end{align*}\\]"
  },
  {
    "objectID": "lectures/01-Random-Variables/013-variance.html",
    "href": "lectures/01-Random-Variables/013-variance.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "The variance of a random variable measures its dispersion. It asks “on average, how far is the variable from its average”? Differences are squared to get rid of the negative sign and punish large deviances a little more. We will use the greek letter \\(\\sigma\\) (“sigma”) for variance \\((\\sigma^{2})\\) and standard deviation \\((\\sigma)\\)\nThe formula is:\n\\[\\begin{align}\n    Var(X) = \\sigma_{X}^{2}\n    &= E[(X - \\mu_{X})^{2}] \\\\\n    &= (x_{1} - \\mu_{X})^{2}p_{1} + (x_{2} - \\mu_{X})^{2}p_{2} + \\cdots + (x_{n} - \\mu_{X})^{2}p_{n} \\\\\n    &= \\sum_{i = 1}^{n} (x_{i} - \\mu_{X})^{2}p_{i}\n\\end{align}\\]\nNote that because of the square and the fact that probabilities \\(p_{i}\\) are never negative, the variance of a RV can never be a negative number"
  },
  {
    "objectID": "lectures/01-Random-Variables/013-variance.html#definition",
    "href": "lectures/01-Random-Variables/013-variance.html#definition",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "The variance of a random variable measures its dispersion. It asks “on average, how far is the variable from its average”? Differences are squared to get rid of the negative sign and punish large deviances a little more. We will use the greek letter \\(\\sigma\\) (“sigma”) for variance \\((\\sigma^{2})\\) and standard deviation \\((\\sigma)\\)\nThe formula is:\n\\[\\begin{align}\n    Var(X) = \\sigma_{X}^{2}\n    &= E[(X - \\mu_{X})^{2}] \\\\\n    &= (x_{1} - \\mu_{X})^{2}p_{1} + (x_{2} - \\mu_{X})^{2}p_{2} + \\cdots + (x_{n} - \\mu_{X})^{2}p_{n} \\\\\n    &= \\sum_{i = 1}^{n} (x_{i} - \\mu_{X})^{2}p_{i}\n\\end{align}\\]\nNote that because of the square and the fact that probabilities \\(p_{i}\\) are never negative, the variance of a RV can never be a negative number"
  },
  {
    "objectID": "lectures/01-Random-Variables/013-variance.html#rules",
    "href": "lectures/01-Random-Variables/013-variance.html#rules",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Rules",
    "text": "Rules\nSome important rules about the way variance works. Let \\(X\\) and \\(Y\\) be random variables and let \\(b\\) be a constant.\n\nThe variance of the sum of two RVs is the sum of their variances plus two times their covariance: \\[\nVar(X + Y) = Var(X) + Var(Y) + 2Cov(X,Y)\n\\]\nConstants can pass outside of a variance if you square them: \\[\nVar(bX) = b^{2}Var(X)\n\\]\nThe variance of a constant is 0: \\[\nVar(b) = 0\n\\]\nThe variance of a RV plus a constant is the variance of that random variable: \\[\nVar(X + b) = Var(X)\n\\]"
  },
  {
    "objectID": "lectures/01-Random-Variables/011-probs.html",
    "href": "lectures/01-Random-Variables/011-probs.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "A Random Variable is any variable whose value cannot be predicted exactly. For example:\n\nThe message you get in a fortune cookie\nThe amount of time spent searching for your keys\nThe number of likes you get on a social media post\nThe number of customers that enter a store in a day\n\nAll of these are random variables.\nSome random variables are discrete and some are continuous"
  },
  {
    "objectID": "lectures/01-Random-Variables/011-probs.html#random-variables",
    "href": "lectures/01-Random-Variables/011-probs.html#random-variables",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "A Random Variable is any variable whose value cannot be predicted exactly. For example:\n\nThe message you get in a fortune cookie\nThe amount of time spent searching for your keys\nThe number of likes you get on a social media post\nThe number of customers that enter a store in a day\n\nAll of these are random variables.\nSome random variables are discrete and some are continuous"
  },
  {
    "objectID": "lectures/01-Random-Variables/011-probs.html#discrete-and-continuous-rv",
    "href": "lectures/01-Random-Variables/011-probs.html#discrete-and-continuous-rv",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Discrete and Continuous RV",
    "text": "Discrete and Continuous RV\nWhat’s the difference?\n\n\nDiscrete\n\nCounted\nTake on a small number of possible values\nEx: Number of M&Ms in your bag\n\n\nContinuous\n\nMeasured\nCan take on an infinite number of possible values\nEx: How heavy your bag is\n\n\n\nVariables can also be categorical instead of numeric. They may represent qualitative data that can be divided into categories or groups. For now, we will lump them in with discrete variables"
  },
  {
    "objectID": "lectures/01-Random-Variables/011-probs.html#discrete-probability-distributions",
    "href": "lectures/01-Random-Variables/011-probs.html#discrete-probability-distributions",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Discrete Probability Distributions",
    "text": "Discrete Probability Distributions\nConsider the event of a dice roll. This action produces a discrete random variable.\nIt could take on values 1 to 6 and, if it is a fair die, it takes on each of those values with equali probability \\(1/6\\).\nOur notation will be:\n\n\\(X\\) is the random variable, \\(x_{i}\\) is a potential outcome for \\(X\\), and each potential outcome \\(x_{i}\\) happens with probability \\(p_{i}\\)\n\n\n\n\n\\(x_{i}\\)\n1\n2\n3\n4\n5\n6\n\n\n\n\n\n\\(p_{i}\\)\n1/6\n1/6\n1/6\n1/6\n1/6\n1/6"
  },
  {
    "objectID": "lectures/01-Random-Variables/011-probs.html#discrete-probability-distributions-1",
    "href": "lectures/01-Random-Variables/011-probs.html#discrete-probability-distributions-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Discrete Probability Distributions",
    "text": "Discrete Probability Distributions\nConsider another random variable \\(X\\) to be the sum of two dice rolls. In the table below, the first row represents the potential outcomes for the first roll and the first column represents the potential outcomes for the second roll. The values inside the table represent the potential outcomes for \\(X\\) (the sum)\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n2\n3\n4\n5\n6\n7\n8\n\n\n3\n4\n5\n6\n7\n8\n9\n\n\n4\n5\n6\n7\n8\n9\n10\n\n\n5\n6\n7\n8\n9\n10\n11\n\n\n6\n7\n8\n9\n10\n11\n12\n\n\n\n\nEach of the cells occur with equal probability. So that X = 2 has probability 1/36. X = 3 has probability 2/36, as it can occur in two ways."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "EC320 - Introduction to Econometrics",
    "section": "",
    "text": "Hello! Welcome to EC 320. I am Jose Rojas-Fallas, a 4th Year PhD student in the Econ department. And welcome to the course website! I have begun to use a website as a main method of course delivery as it is more intuitive (and easier to manage) than Canvas. For this course, Canvas is a glorified assignment submission site and gradebook. You will find everything you need on this site by navigating the tabs above."
  },
  {
    "objectID": "index.html#hello",
    "href": "index.html#hello",
    "title": "EC320 - Introduction to Econometrics",
    "section": "",
    "text": "Hello! Welcome to EC 320. I am Jose Rojas-Fallas, a 4th Year PhD student in the Econ department. And welcome to the course website! I have begun to use a website as a main method of course delivery as it is more intuitive (and easier to manage) than Canvas. For this course, Canvas is a glorified assignment submission site and gradebook. You will find everything you need on this site by navigating the tabs above."
  },
  {
    "objectID": "index.html#syllabus",
    "href": "index.html#syllabus",
    "title": "EC320 - Introduction to Econometrics",
    "section": "Syllabus",
    "text": "Syllabus\n\n  Summer 2025 Syllabus PDF\n  \n     Download"
  },
  {
    "objectID": "index.html#summer-2025-class-details",
    "href": "index.html#summer-2025-class-details",
    "title": "EC320 - Introduction to Econometrics",
    "section": "Summer 2025 Class Details",
    "text": "Summer 2025 Class Details\n\n\nInstructor: Jose Rojas-Fallas\nOffice Hours: Tues/Thurs 02:00 to 04:00 pm\n\nEmail: jrojas2@uoregon.edu\nZoom Room\n\n\n\nCourse Grade Breakdown\nFor more details, read the Syllabus above\n\n\n\nAssignment\nGrade Weight\n\n\n\n\nProblem Sets (x4)\n20%\n\n\nQuizzes (x4)\n20%\n\n\nMidterm Exam\n30%\n\n\nFinal Exam\n30%\n\n\n\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "documents/problem-sets/index.html#final-exam-available-at-1200pm-on-july-18-due-july-18-at-1159pm-on-canvas",
    "href": "documents/problem-sets/index.html#final-exam-available-at-1200pm-on-july-18-due-july-18-at-1159pm-on-canvas",
    "title": "Assignments",
    "section": "Final Exam Available at 12:00pm on July 18 (Due July 18 at 11:59pm) on Canvas",
    "text": "Final Exam Available at 12:00pm on July 18 (Due July 18 at 11:59pm) on Canvas"
  },
  {
    "objectID": "documents/problem-sets/index.html#problem-sets",
    "href": "documents/problem-sets/index.html#problem-sets",
    "title": "Assignments",
    "section": "Problem Sets",
    "text": "Problem Sets\n\nPS01 (Due June 29 at 11:59pm) on Canvas\n\n  View PDF \n  \n     Download \n  \n  \n    \n  \n\n\n  Solutions \n  \n     Download \n  \n  \n    \n  \n\n\n\nPS02 (Due July 05 at 11:59pm) on Canvas\n\n  View PDF \n  \n     Download \n  \n  \n    \n  \n\n\n  Solutions \n  \n     Download \n  \n  \n    \n  \n\n\n\nPS03 (Due July 11 at 11:59pm) on Canvas\n\n  View PDF \n  \n     Download \n  \n  \n    \n  \n\n\n  Solutions \n  \n     Download \n  \n  \n    \n  \n\n\n\nPS04 (Due July 17 at 11:59pm) on Canvas\nDownload this Assignment and submit your completed R script on Canvas\nAlso submit your answer to who did it on Canvas"
  },
  {
    "objectID": "documents/problem-sets/index.html#quiz",
    "href": "documents/problem-sets/index.html#quiz",
    "title": "Assignments",
    "section": "Quiz",
    "text": "Quiz\n\nQuiz 01 on Canvas\n\n\nQuiz 02 on Canvas\n\n\nQuiz 03 on Canvas\n\n\nQuiz 04 on Canvas"
  },
  {
    "objectID": "documents/problem-sets/index.html#midterm-exam",
    "href": "documents/problem-sets/index.html#midterm-exam",
    "title": "Assignments",
    "section": "Midterm Exam",
    "text": "Midterm Exam\n\nMidterm Exam Available at 12:00pm on July 03 (Due July 03 at 11:59pm) on Canvas\n\n  Solutions \n  \n     Download"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "documents/R-practice/index.html",
    "href": "documents/R-practice/index.html",
    "title": "R Worksheets",
    "section": "",
    "text": "Solutions \n  \n    \n    \n  \n\n\n\n\n\n  Solutions \n  \n    \n    \n  \n\n\n\n\n\n  Solutions \n  \n    \n    \n  \n\n\n\n\n\n  Solutions"
  },
  {
    "objectID": "documents/R-practice/index.html#worksheet-01---vectors-and-pipes",
    "href": "documents/R-practice/index.html#worksheet-01---vectors-and-pipes",
    "title": "R Worksheets",
    "section": "",
    "text": "Solutions"
  },
  {
    "objectID": "documents/R-practice/index.html#worksheet-02---tibbles-and-lm",
    "href": "documents/R-practice/index.html#worksheet-02---tibbles-and-lm",
    "title": "R Worksheets",
    "section": "",
    "text": "Solutions"
  },
  {
    "objectID": "documents/R-practice/index.html#worksheet-03---dplyr",
    "href": "documents/R-practice/index.html#worksheet-03---dplyr",
    "title": "R Worksheets",
    "section": "",
    "text": "Solutions"
  },
  {
    "objectID": "documents/R-practice/index.html#worksheet-04---ggplot",
    "href": "documents/R-practice/index.html#worksheet-04---ggplot",
    "title": "R Worksheets",
    "section": "",
    "text": "Solutions"
  },
  {
    "objectID": "documents/readings/index.html",
    "href": "documents/readings/index.html",
    "title": "Readings",
    "section": "",
    "text": "View PDF \n  \n     Download"
  },
  {
    "objectID": "documents/readings/index.html#math-rules",
    "href": "documents/readings/index.html#math-rules",
    "title": "Readings",
    "section": "",
    "text": "View PDF \n  \n     Download"
  },
  {
    "objectID": "documents/readings/index.html#review",
    "href": "documents/readings/index.html#review",
    "title": "Readings",
    "section": "Review",
    "text": "Review\n\n  View PDF \n  \n     Download"
  },
  {
    "objectID": "documents/readings/index.html#ch.01---simple-regression-analysis",
    "href": "documents/readings/index.html#ch.01---simple-regression-analysis",
    "title": "Readings",
    "section": "Ch.01 - Simple Regression Analysis",
    "text": "Ch.01 - Simple Regression Analysis\n\n  View PDF \n  \n     Download"
  },
  {
    "objectID": "documents/readings/index.html#ch.02---coefficient-hypothesis-testing",
    "href": "documents/readings/index.html#ch.02---coefficient-hypothesis-testing",
    "title": "Readings",
    "section": "Ch.02 - Coefficient & Hypothesis Testing",
    "text": "Ch.02 - Coefficient & Hypothesis Testing\n\n  View PDF \n  \n     Download"
  },
  {
    "objectID": "documents/readings/index.html#ch.03---multiple-regression-analysis",
    "href": "documents/readings/index.html#ch.03---multiple-regression-analysis",
    "title": "Readings",
    "section": "Ch.03 - Multiple Regression Analysis",
    "text": "Ch.03 - Multiple Regression Analysis\n\n  View PDF \n  \n     Download"
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#preview",
    "href": "lectures/01-Random-Variables/010-compile.html#preview",
    "title": "Random Variables",
    "section": "Preview",
    "text": "Preview\nIn this chapter we will:\n\nLearn what discrete and continuous random variables are\nHow to use the probability distribution of a discrete random variable to obtain the expected value and variance of the random variable\nHow to use the probability density function (PDF) of a continuous random variable to obtain the expected value and variance of the random variable\nHow to obtain the covariance and correlation between two random variables"
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#notation",
    "href": "lectures/01-Random-Variables/010-compile.html#notation",
    "title": "Random Variables",
    "section": "Notation",
    "text": "Notation\nSome important notation we need to introduce:\n\n\n\n\nSymbol\nMeaning\n\n\n\n\n\\(X\\)\nRandom Variable (RV)\n\n\n\\(x_i\\)\nA potential outcome for the RV \\(X\\)\n\n\n\\(p_i\\)\nThe probability a certain outcome will occur (discrete RVs)\n\n\n\\(\\mu_X\\)\nThe expected value of \\(X\\), also known as \\(E[X]\\)\n\n\n\\(\\sigma_X^2\\)\nThe variance of \\(X\\)\n\n\n\\(\\sigma_X\\)\nThe standard deviation of \\(X\\)\n\n\n\\(\\sigma_{XY}\\)\nThe covariance of \\(X\\) and \\(Y\\)\n\n\n\\(\\rho_{XY}\\)\nThe correlation between \\(X\\) and \\(Y\\)"
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#random-variables-1",
    "href": "lectures/01-Random-Variables/010-compile.html#random-variables-1",
    "title": "Random Variables",
    "section": "Random Variables",
    "text": "Random Variables\nA Random Variable is any variable whose value cannot be predicted exactly. For example:\n\nThe message you get in a fortune cookie\nThe amount of time spent searching for your keys\nThe number of likes you get on a social media post\nThe number of customers that enter a store in a day\n\nAll of these are random variables.\nSome random variables are discrete and some are continuous"
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#discrete-and-continuous-rv",
    "href": "lectures/01-Random-Variables/010-compile.html#discrete-and-continuous-rv",
    "title": "Random Variables",
    "section": "Discrete and Continuous RV",
    "text": "Discrete and Continuous RV\nWhat’s the difference?\n\n\nDiscrete\n\nCounted\nTake on a small number of possible values\nEx: Number of M&Ms in your bag\n\n\nContinuous\n\nMeasured\nCan take on an infinite number of possible values\nEx: How heavy your bag is\n\n\nVariables can also be categorical instead of numeric. They may represent qualitative data that can be divided into categories or groups. For now, we will lump them in with discrete variables"
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#discrete-probability-distributions",
    "href": "lectures/01-Random-Variables/010-compile.html#discrete-probability-distributions",
    "title": "Random Variables",
    "section": "Discrete Probability Distributions",
    "text": "Discrete Probability Distributions\nConsider the event of a dice roll. This action produces a discrete random variable.\nIt could take on values 1 to 6 and, if it is a fair die, it takes on each of those values with equali probability \\(1/6\\).\nOur notation will be:\n\n\\(X\\) is the random variable, \\(x_{i}\\) is a potential outcome for \\(X\\), and each potential outcome \\(x_{i}\\) happens with probability \\(p_{i}\\)\n\n\n\n\n\\(x_{i}\\)\n1\n2\n3\n4\n5\n6\n\n\n\n\n\n\\(p_{i}\\)\n1/6\n1/6\n1/6\n1/6\n1/6\n1/6"
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#discrete-probability-distributions-1",
    "href": "lectures/01-Random-Variables/010-compile.html#discrete-probability-distributions-1",
    "title": "Random Variables",
    "section": "Discrete Probability Distributions",
    "text": "Discrete Probability Distributions\nConsider another random variable \\(X\\) to be the sum of two dice rolls. In the table below, the first row represents the potential outcomes for the first roll and the first column represents the potential outcomes for the second roll. The values inside the table represent the potential outcomes for \\(X\\) (the sum)\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n2\n3\n4\n5\n6\n7\n8\n\n\n3\n4\n5\n6\n7\n8\n9\n\n\n4\n5\n6\n7\n8\n9\n10\n\n\n5\n6\n7\n8\n9\n10\n11\n\n\n6\n7\n8\n9\n10\n11\n12\n\n\n\n\nEach of the cells occur with equal probability. So that X = 2 has probability 1/36. X = 3 has probability 2/36, as it can occur in two ways."
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#expected-values-of-discrete-random-variables",
    "href": "lectures/01-Random-Variables/010-compile.html#expected-values-of-discrete-random-variables",
    "title": "Random Variables",
    "section": "Expected Values of Discrete Random Variables",
    "text": "Expected Values of Discrete Random Variables\nThe expected value of a random variable is its long-term average.\nWe will use the greek letter \\(\\mu\\) (“mew”) to refer to expected values. That is, we will say that the expected value of \\(X\\) is \\(\\mu_{X}\\), or equivalently, \\(E[X] = \\mu_{X}\\).\nIf the variable is discrete, you can calculate its expectation by taking the sum of all possible values of the random variable, each multiplied by their corresponding probabilities.\nWe write this as:\n\\[\n    E[X] = \\sum_{i} x_{i}p_{i}\n\\]\nWhere \\(x_{i}\\) is a potential outcome for \\(X\\) and \\(p_{i}\\) is the probability that outcome occurs"
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#expected-value-rules",
    "href": "lectures/01-Random-Variables/010-compile.html#expected-value-rules",
    "title": "Random Variables",
    "section": "Expected Value Rules",
    "text": "Expected Value Rules\n\nHere are some very important math rules to know about the way expected values work. Let \\(X\\),\\(Y\\), and \\(Z\\) be random variables and let \\(b\\) be a constant.\n\n\nThe expectation of the sum of several RVs is the sum of their expectation: \\[\nE[X + Y + Z] = E[X] + E[Y] + E[Z]\n\\]\nConstants can pass outside of an expectation: \\[\nE[bX] = bE[X]\n\\]\nThe expected value of a constant is that constant: \\[\nE[b] = b\n\\]"
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#definition",
    "href": "lectures/01-Random-Variables/010-compile.html#definition",
    "title": "Random Variables",
    "section": "Definition",
    "text": "Definition\nThe variance of a random variable measures its dispersion. It asks “on average, how far is the variable from its average”? Differences are squared to get rid of the negative sign and punish large deviances a little more. We will use the greek letter \\(\\sigma\\) (“sigma”) for variance \\((\\sigma^{2})\\) and standard deviation \\((\\sigma)\\)\nThe formula is:\n\\[\\begin{align}\n    Var(X) = \\sigma_{X}^{2}\n    &= E[(X - \\mu_{X})^{2}] \\\\\n    &= (x_{1} - \\mu_{X})^{2}p_{1} + (x_{2} - \\mu_{X})^{2}p_{2} + \\cdots + (x_{n} - \\mu_{X})^{2}p_{n} \\\\\n    &= \\sum_{i = 1}^{n} (x_{i} - \\mu_{X})^{2}p_{i}\n\\end{align}\\]\nNote that because of the square and the fact that probabilities \\(p_{i}\\) are never negative, the variance of a RV can never be a negative number"
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#rules",
    "href": "lectures/01-Random-Variables/010-compile.html#rules",
    "title": "Random Variables",
    "section": "Rules",
    "text": "Rules\nSome important rules about the way variance works. Let \\(X\\) and \\(Y\\) be random variables and let \\(b\\) be a constant.\n\nThe variance of the sum of two RVs is the sum of their variances plus two times their covariance: \\[\nVar(X + Y) = Var(X) + Var(Y) + 2Cov(X,Y)\n\\]\nConstants can pass outside of a variance if you square them: \\[\nVar(bX) = b^{2}Var(X)\n\\]\nThe variance of a constant is 0: \\[\nVar(b) = 0\n\\]\nThe variance of a RV plus a constant is the variance of that random variable: \\[\nVar(X + b) = Var(X)\n\\]"
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#definition-1",
    "href": "lectures/01-Random-Variables/010-compile.html#definition-1",
    "title": "Random Variables",
    "section": "Definition",
    "text": "Definition\nThe covariance of two random variables \\((\\sigma_{XY})\\) is a measure of the linear association between those variables. For example, since people who are taller are generally heavier, we would say that the random variables height and weight have a positive covariance. On the other hand, if large values for one random variable tend to correspond to small values in the other, we would say the two variables have a negative covariance. Two variables are independent have a covariance of 0.\nThe formula is:\n\\[\n    Cov(X,Y) = \\sigma_{XY} = E[(X - \\mu_{X})(Y - \\mu_{Y})]\n\\]\nNotice that the covariance of a random variable \\(X\\) with itself is the variance of \\(X\\)"
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#rules-1",
    "href": "lectures/01-Random-Variables/010-compile.html#rules-1",
    "title": "Random Variables",
    "section": "Rules",
    "text": "Rules\nSome important rules about the way variance works. Let \\(X\\),\\(Y\\), and \\(Z\\) be random variables and let \\(b\\) be a constant.\n\nThe covariance of a random variable with a constant is 0 \\[\nCov(X,b) = 0\n\\]\nThe covariance of a random variable with itself is its variance: \\[\nCov(X,X) = Var(X)\n\\]\nConstants can come outside of the covariance: \\[\nCov(X,bY) = bCov(X,Y)\n\\]\nIf \\(Z\\) is a third random variable, we write: \\[\nCov(X,Y + Z) = Cov(X,Y) + Cov(X,Z)\n\\]"
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#definition-2",
    "href": "lectures/01-Random-Variables/010-compile.html#definition-2",
    "title": "Random Variables",
    "section": "Definition",
    "text": "Definition\nAn issue with covariance is that the covariance between two random variables depends on the units those variables are measured in. That’s where correlation comes in:\nCorrelation is another measure of linear association that has the benefit of being dimensionless because the units in the numerator cancel with the units in the denominator.\nIt is also the case that the correlation between two variables is always between -1 and 1. Where correlation = 1, the two variables have a perfect positive linear relationsihp, and when correlation = -1, the two variables have a perfect negative linear relationship.\nWe will use the greek letter \\(\\rho\\) (“rho”) to refer to the correlation between two RVs. The formula is:\n\\[\\begin{align*}\n    \\rho_{XY} =\n    \\dfrac{\n        \\sigma_{XY}\n    }{\n        \\sqrt{\\sigma_{X}^{2}\\sigma_{Y}^{2}}\n    }\n\\end{align*}\\]"
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#probabilities-of-continuous-rvs",
    "href": "lectures/01-Random-Variables/010-compile.html#probabilities-of-continuous-rvs",
    "title": "Random Variables",
    "section": "Probabilities of Continuous RVs",
    "text": "Probabilities of Continuous RVs\nWhen the variable can take on an infinite number of possible values, the probability it takes on any given value must be zero.\nThe variable takes so many values that we cannot count all possibilities, so the probability of any one particular value is zero.\nWe can use probability density functions (PDFs) to help describe continuous RVs of which there are many but we will give emphasis to two:\n\nUniform Distribution\nNormal Distribution"
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#distributions",
    "href": "lectures/01-Random-Variables/010-compile.html#distributions",
    "title": "Random Variables",
    "section": "Distributions",
    "text": "Distributions\nA distribution is a function that represents all outcomes of a random variable and the corresponding probabilities. It is:\n\nA summary that describes the spread of data points in a set\nEssential for making inferences and assumptions from data\n\nKey Takeaway: The shape of a distribution provides valuable information of the data"
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#uniform-distribution",
    "href": "lectures/01-Random-Variables/010-compile.html#uniform-distribution",
    "title": "Random Variables",
    "section": "Uniform Distribution",
    "text": "Uniform Distribution\nThe probability density function of a variable uniformly distributed between 0 and 2 is\n\\[\\begin{align*}\n    f(x) =\n        \\begin{cases}\n        \\dfrac{1}{2} & \\text{if } 0 \\leq x \\leq 2 \\\\\n        0   & \\text{otherwise }\n        \\end{cases}\n\\end{align*}\\]"
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#uniform-distribution-1",
    "href": "lectures/01-Random-Variables/010-compile.html#uniform-distribution-1",
    "title": "Random Variables",
    "section": "Uniform Distribution",
    "text": "Uniform Distribution\nBy definition, the area under \\(f(x)\\) is equal to 1.\nThe shaded area illustrates the probability of the event \\(1 \\leq X \\leq 1.5\\).\n\\[\n    P(1 \\leq X \\leq 1.5) = (1.5 - 1) \\times 0.5 = 0.25\n\\]"
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#normal-distribution",
    "href": "lectures/01-Random-Variables/010-compile.html#normal-distribution",
    "title": "Random Variables",
    "section": "Normal Distribution",
    "text": "Normal Distribution\nThis is commonly called a “bell curve”. It is:\n\nSymmetric: Mean and median occur at the same point (i.e. no skew)\nLow-probability events are in the tails\nHigh-probability events are near the center"
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#normal-distribution-1",
    "href": "lectures/01-Random-Variables/010-compile.html#normal-distribution-1",
    "title": "Random Variables",
    "section": "Normal Distribution",
    "text": "Normal Distribution\nThe shaded area illustrates the probability of the event \\(-2 \\leq X \\leq 2\\) occurring\n\nTo “find the area under the curve” we use integral calculus (or, in practice ).\n\n\\[\n    P(-2 \\leq X \\leq 2) \\approx 0.95\n\\]"
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#normal-distribution-2",
    "href": "lectures/01-Random-Variables/010-compile.html#normal-distribution-2",
    "title": "Random Variables",
    "section": "Normal Distribution",
    "text": "Normal Distribution\nContinuous distribution where \\(x_{i}\\) takes the value of any real number \\((\\mathbb{R})\\)\n\nThe domain spans the entire real line\nCentered on the distribution mean \\(\\mu\\)\n\nA couple of important rules to recall:\n\nThe probability that the random variable takes a value \\(x_{i}\\) is 0 for any \\(x_{i} \\in \\mathbb{R}\\)\nThe probability that the random variable falls between \\([x_{i},x_{j}]\\) range, where \\(x_{i} \\neq x_{j}\\), is the area under \\(p(x)\\) between those two values.\n\nThe area highlighted in the previous graph represents \\(p(x) = 0.95\\). The values \\({-1.96,1.95}\\) represent the 95% confidence interval for \\(\\mu\\)"
  },
  {
    "objectID": "lectures/01-Random-Variables/010-compile.html#primary-differences-in-expected-values-by-rv-type",
    "href": "lectures/01-Random-Variables/010-compile.html#primary-differences-in-expected-values-by-rv-type",
    "title": "Random Variables",
    "section": "Primary Differences in Expected Values by RV Type",
    "text": "Primary Differences in Expected Values by RV Type\nTo find the expected value or variance of a continuous random variable instead of a discrete random variable, we just swap integrals for sums and the PDF \\(f(X)\\) for \\(p_{i}\\):\n\n\n\n\n\n\n\n\n\n\\(E[X]\\)\n\\(Var(X) = E[(X - \\mu_{X})^{2}]\\)\n\n\n\n\nDiscrete\n\\(\\sum_{i=1}^{n} x_{i}p_{i}\\)\n\\(\\sum_{i=1}^{n} (x_{i} - \\mu_{X})^{2} p_{i}\\)\n\n\nContinuous\n\\(\\int X f(X) dX\\)\n\\(\\int (X - \\mu_{x})^{2} f(X) dX\\)"
  },
  {
    "objectID": "lectures/01-Random-Variables/012-expected-values.html",
    "href": "lectures/01-Random-Variables/012-expected-values.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "The expected value of a random variable is its long-term average.\nWe will use the greek letter \\(\\mu\\) (“mew”) to refer to expected values. That is, we will say that the expected value of \\(X\\) is \\(\\mu_{X}\\), or equivalently, \\(E[X] = \\mu_{X}\\).\nIf the variable is discrete, you can calculate its expectation by taking the sum of all possible values of the random variable, each multiplied by their corresponding probabilities.\nWe write this as:\n\\[\n    E[X] = \\sum_{i} x_{i}p_{i}\n\\]\nWhere \\(x_{i}\\) is a potential outcome for \\(X\\) and \\(p_{i}\\) is the probability that outcome occurs"
  },
  {
    "objectID": "lectures/01-Random-Variables/012-expected-values.html#expected-values-of-discrete-random-variables",
    "href": "lectures/01-Random-Variables/012-expected-values.html#expected-values-of-discrete-random-variables",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "The expected value of a random variable is its long-term average.\nWe will use the greek letter \\(\\mu\\) (“mew”) to refer to expected values. That is, we will say that the expected value of \\(X\\) is \\(\\mu_{X}\\), or equivalently, \\(E[X] = \\mu_{X}\\).\nIf the variable is discrete, you can calculate its expectation by taking the sum of all possible values of the random variable, each multiplied by their corresponding probabilities.\nWe write this as:\n\\[\n    E[X] = \\sum_{i} x_{i}p_{i}\n\\]\nWhere \\(x_{i}\\) is a potential outcome for \\(X\\) and \\(p_{i}\\) is the probability that outcome occurs"
  },
  {
    "objectID": "lectures/01-Random-Variables/012-expected-values.html#expected-value-rules",
    "href": "lectures/01-Random-Variables/012-expected-values.html#expected-value-rules",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Expected Value Rules",
    "text": "Expected Value Rules\n\nHere are some very important math rules to know about the way expected values work. Let \\(X\\),\\(Y\\), and \\(Z\\) be random variables and let \\(b\\) be a constant.\n\n\nThe expectation of the sum of several RVs is the sum of their expectation: \\[\nE[X + Y + Z] = E[X] + E[Y] + E[Z]\n\\]\nConstants can pass outside of an expectation: \\[\nE[bX] = bE[X]\n\\]\nThe expected value of a constant is that constant: \\[\nE[b] = b\n\\]"
  },
  {
    "objectID": "lectures/01-Random-Variables/014-covariance.html",
    "href": "lectures/01-Random-Variables/014-covariance.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "The covariance of two random variables \\((\\sigma_{XY})\\) is a measure of the linear association between those variables. For example, since people who are taller are generally heavier, we would say that the random variables height and weight have a positive covariance. On the other hand, if large values for one random variable tend to correspond to small values in the other, we would say the two variables have a negative covariance. Two variables are independent have a covariance of 0.\nThe formula is:\n\\[\n    Cov(X,Y) = \\sigma_{XY} = E[(X - \\mu_{X})(Y - \\mu_{Y})]\n\\]\nNotice that the covariance of a random variable \\(X\\) with itself is the variance of \\(X\\)"
  },
  {
    "objectID": "lectures/01-Random-Variables/014-covariance.html#definition",
    "href": "lectures/01-Random-Variables/014-covariance.html#definition",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "The covariance of two random variables \\((\\sigma_{XY})\\) is a measure of the linear association between those variables. For example, since people who are taller are generally heavier, we would say that the random variables height and weight have a positive covariance. On the other hand, if large values for one random variable tend to correspond to small values in the other, we would say the two variables have a negative covariance. Two variables are independent have a covariance of 0.\nThe formula is:\n\\[\n    Cov(X,Y) = \\sigma_{XY} = E[(X - \\mu_{X})(Y - \\mu_{Y})]\n\\]\nNotice that the covariance of a random variable \\(X\\) with itself is the variance of \\(X\\)"
  },
  {
    "objectID": "lectures/01-Random-Variables/014-covariance.html#rules",
    "href": "lectures/01-Random-Variables/014-covariance.html#rules",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Rules",
    "text": "Rules\nSome important rules about the way variance works. Let \\(X\\),\\(Y\\), and \\(Z\\) be random variables and let \\(b\\) be a constant.\n\nThe covariance of a random variable with a constant is 0 \\[\nCov(X,b) = 0\n\\]\nThe covariance of a random variable with itself is its variance: \\[\nCov(X,X) = Var(X)\n\\]\nConstants can come outside of the covariance: \\[\nCov(X,bY) = bCov(X,Y)\n\\]\nIf \\(Z\\) is a third random variable, we write: \\[\nCov(X,Y + Z) = Cov(X,Y) + Cov(X,Z)\n\\]"
  },
  {
    "objectID": "lectures/01-Random-Variables/016-continuous-rvs.html",
    "href": "lectures/01-Random-Variables/016-continuous-rvs.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "When the variable can take on an infinite number of possible values, the probability it takes on any given value must be zero.\nThe variable takes so many values that we cannot count all possibilities, so the probability of any one particular value is zero.\nWe can use probability density functions (PDFs) to help describe continuous RVs of which there are many but we will give emphasis to two:\n\nUniform Distribution\nNormal Distribution"
  },
  {
    "objectID": "lectures/01-Random-Variables/016-continuous-rvs.html#probabilities-of-continuous-rvs",
    "href": "lectures/01-Random-Variables/016-continuous-rvs.html#probabilities-of-continuous-rvs",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "When the variable can take on an infinite number of possible values, the probability it takes on any given value must be zero.\nThe variable takes so many values that we cannot count all possibilities, so the probability of any one particular value is zero.\nWe can use probability density functions (PDFs) to help describe continuous RVs of which there are many but we will give emphasis to two:\n\nUniform Distribution\nNormal Distribution"
  },
  {
    "objectID": "lectures/01-Random-Variables/016-continuous-rvs.html#distributions",
    "href": "lectures/01-Random-Variables/016-continuous-rvs.html#distributions",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Distributions",
    "text": "Distributions\nA distribution is a function that represents all outcomes of a random variable and the corresponding probabilities. It is:\n\nA summary that describes the spread of data points in a set\nEssential for making inferences and assumptions from data\n\nKey Takeaway: The shape of a distribution provides valuable information of the data"
  },
  {
    "objectID": "lectures/01-Random-Variables/016-continuous-rvs.html#uniform-distribution",
    "href": "lectures/01-Random-Variables/016-continuous-rvs.html#uniform-distribution",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Uniform Distribution",
    "text": "Uniform Distribution\nThe probability density function of a variable uniformly distributed between 0 and 2 is\n\\[\\begin{align*}\n    f(x) =\n        \\begin{cases}\n        \\dfrac{1}{2} & \\text{if } 0 \\leq x \\leq 2 \\\\\n        0   & \\text{otherwise }\n        \\end{cases}\n\\end{align*}\\]"
  },
  {
    "objectID": "lectures/01-Random-Variables/016-continuous-rvs.html#uniform-distribution-1",
    "href": "lectures/01-Random-Variables/016-continuous-rvs.html#uniform-distribution-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Uniform Distribution",
    "text": "Uniform Distribution\nBy definition, the area under \\(f(x)\\) is equal to 1.\nThe shaded area illustrates the probability of the event \\(1 \\leq X \\leq 1.5\\).\n\\[\n    P(1 \\leq X \\leq 1.5) = (1.5 - 1) \\times 0.5 = 0.25\n\\]\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead."
  },
  {
    "objectID": "lectures/01-Random-Variables/016-continuous-rvs.html#normal-distribution",
    "href": "lectures/01-Random-Variables/016-continuous-rvs.html#normal-distribution",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Normal Distribution",
    "text": "Normal Distribution\nThis is commonly called a “bell curve”. It is:\n\nSymmetric: Mean and median occur at the same point (i.e. no skew)\nLow-probability events are in the tails\nHigh-probability events are near the center\n\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union"
  },
  {
    "objectID": "lectures/01-Random-Variables/016-continuous-rvs.html#normal-distribution-1",
    "href": "lectures/01-Random-Variables/016-continuous-rvs.html#normal-distribution-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Normal Distribution",
    "text": "Normal Distribution\nThe shaded area illustrates the probability of the event \\(-2 \\leq X \\leq 2\\) occurring\n\nTo “find the area under the curve” we use integral calculus (or, in practice ).\n\n\\[\n    P(-2 \\leq X \\leq 2) \\approx 0.95\n\\]"
  },
  {
    "objectID": "lectures/01-Random-Variables/016-continuous-rvs.html#normal-distribution-2",
    "href": "lectures/01-Random-Variables/016-continuous-rvs.html#normal-distribution-2",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Normal Distribution",
    "text": "Normal Distribution\nContinuous distribution where \\(x_{i}\\) takes the value of any real number \\((\\mathbb{R})\\)\n\nThe domain spans the entire real line\nCentered on the distribution mean \\(\\mu\\)\n\nA couple of important rules to recall:\n\nThe probability that the random variable takes a value \\(x_{i}\\) is 0 for any \\(x_{i} \\in \\mathbb{R}\\)\nThe probability that the random variable falls between \\([x_{i},x_{j}]\\) range, where \\(x_{i} \\neq x_{j}\\), is the area under \\(p(x)\\) between those two values.\n\nThe area highlighted in the previous graph represents \\(p(x) = 0.95\\). The values \\({-1.96,1.95}\\) represent the 95% confidence interval for \\(\\mu\\)"
  },
  {
    "objectID": "lectures/01-Random-Variables/016-continuous-rvs.html#primary-differences-in-expected-values-by-rv-type",
    "href": "lectures/01-Random-Variables/016-continuous-rvs.html#primary-differences-in-expected-values-by-rv-type",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Primary Differences in Expected Values by RV Type",
    "text": "Primary Differences in Expected Values by RV Type\nTo find the expected value or variance of a continuous random variable instead of a discrete random variable, we just swap integrals for sums and the PDF \\(f(X)\\) for \\(p_{i}\\):\n\n\n\n\n\n\n\n\n\n\\(E[X]\\)\n\\(Var(X) = E[(X - \\mu_{X})^{2}]\\)\n\n\n\n\nDiscrete\n\\(\\sum_{i=1}^{n} x_{i}p_{i}\\)\n\\(\\sum_{i=1}^{n} (x_{i} - \\mu_{X})^{2} p_{i}\\)\n\n\nContinuous\n\\(\\int X f(X) dX\\)\n\\(\\int (X - \\mu_{x})^{2} f(X) dX\\)"
  },
  {
    "objectID": "lectures/02-Vectors-and-Pipes-R/021-install-r.html",
    "href": "lectures/02-Vectors-and-Pipes-R/021-install-r.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Good news! If you are reading this, you are most likely on a computer already. Great work so far.\nThe good news about R is that it works on any operating system. I personally use it on both Mac and Windows."
  },
  {
    "objectID": "lectures/02-Vectors-and-Pipes-R/021-install-r.html#step-1-grab-a-computer",
    "href": "lectures/02-Vectors-and-Pipes-R/021-install-r.html#step-1-grab-a-computer",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Good news! If you are reading this, you are most likely on a computer already. Great work so far.\nThe good news about R is that it works on any operating system. I personally use it on both Mac and Windows."
  },
  {
    "objectID": "lectures/02-Vectors-and-Pipes-R/021-install-r.html#step-2-download-and-install-r",
    "href": "lectures/02-Vectors-and-Pipes-R/021-install-r.html#step-2-download-and-install-r",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Step 2: Download and Install R",
    "text": "Step 2: Download and Install R\nGo to https://cran.r-project.org/ and follow the instructions to download R for your device."
  },
  {
    "objectID": "lectures/02-Vectors-and-Pipes-R/021-install-r.html#step-3-install-rstudio",
    "href": "lectures/02-Vectors-and-Pipes-R/021-install-r.html#step-3-install-rstudio",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Step 3: Install RStudio",
    "text": "Step 3: Install RStudio\nGo to https://posit.co/download/rstudio-desktop/ and click the blue button that says step 2: install Rstudio Desktop. Follow the instructions to complete the installation."
  },
  {
    "objectID": "lectures/02-Vectors-and-Pipes-R/021-install-r.html#step-4-open-rstudio-and-install-some-packages",
    "href": "lectures/02-Vectors-and-Pipes-R/021-install-r.html#step-4-open-rstudio-and-install-some-packages",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Step 4: Open RStudio and Install Some Packages",
    "text": "Step 4: Open RStudio and Install Some Packages\nRun these lines of code in your console to make sure you have the tidyverse installed and attached to your current session.\n\ninstall.packages(\"tidyverse\", dependencies = TRUE)\nlibrary(tidyverse)\n\n\nAn important shortcut to installing and using packages is a package called pacman. It has a function p_load() which functions as both install.packages() and library() above.\n\ninstall.packages(\"pacman\")\n\nUse p_load to install the next set of packages"
  },
  {
    "objectID": "lectures/02-Vectors-and-Pipes-R/021-install-r.html#step-4a-install-more-packages",
    "href": "lectures/02-Vectors-and-Pipes-R/021-install-r.html#step-4a-install-more-packages",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Step 4a: Install More Packages",
    "text": "Step 4a: Install More Packages\nA former PhD Student (Colleen O’Briant) in the Econ Department created a lovely package that serves as an alternative set of beginner friendly help docs.\nRun this code to install it:\n\nlibrary(pacman)\np_load(Rcpp, devtools) # Note you can do several packages at once\n\ninstall_github(\"cobriant/qelp\")\n\nTo test everything worked, now run:\n\n?qelp::install.packages\n\nIf everything went right, the help docs she wrote on the function install.packages should pop up.\n\n\nNote: It is not a complete library of every function, but many of the basic ones we will use are included."
  },
  {
    "objectID": "lectures/02-Vectors-and-Pipes-R/023-pipes.html",
    "href": "lectures/02-Vectors-and-Pipes-R/023-pipes.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "The pipe %&gt;% is the most frequently used function in the tidyverse.\nWhat It Does:\nSuppose you have some data x and you would like to apply some function f() on it. So you run f(x).\n\nFor example, take the vector 1:3 and find its minimum by applying min():\n\n\nmin(1:3)\n\n[1] 1\n\n\n\nAnother way to do the same things is to use a pipe:\n\n1:3 %&gt;% min()\n\n[1] 1"
  },
  {
    "objectID": "lectures/02-Vectors-and-Pipes-R/023-pipes.html#pipes",
    "href": "lectures/02-Vectors-and-Pipes-R/023-pipes.html#pipes",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "The pipe %&gt;% is the most frequently used function in the tidyverse.\nWhat It Does:\nSuppose you have some data x and you would like to apply some function f() on it. So you run f(x).\n\nFor example, take the vector 1:3 and find its minimum by applying min():\n\n\nmin(1:3)\n\n[1] 1\n\n\n\nAnother way to do the same things is to use a pipe:\n\n1:3 %&gt;% min()\n\n[1] 1"
  },
  {
    "objectID": "lectures/02-Vectors-and-Pipes-R/023-pipes.html#pipes-1",
    "href": "lectures/02-Vectors-and-Pipes-R/023-pipes.html#pipes-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Pipes",
    "text": "Pipes\nThe pipe simply takes the data that comes before it and inserts it into the function that comes after\nThe way you should read the pipe is with the word “then”, as in: “take x, then apply f()”\nWe are not limited to only one pipe. What if we wanted to take x then apply f(), then apply g(), then apply h().\nUsing pipes:\n\nx %&gt;% f() %&gt;% g() %&gt;% h()\n\nOr using multiple lines (this is easier to read once things get hectic)\n\nx %&gt;%\n    f() %&gt;% # Benefit: Allows you to comment on each function to keep track of what you are doing\n    g() %&gt;% # Hello\n    h() # Goodbye\n\nIf we did not use pipes, we’d have to read it inside-out h(g(f(x))), which is nasty."
  },
  {
    "objectID": "lectures/03-Estimators-01/031-estimators.html",
    "href": "lectures/03-Estimators-01/031-estimators.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Let’s define some concepts first:\nEstimand\n\nQuantity that is to be estimated in a statistical analysis\n\nEstimator\n\nA rule (or formula) for estimating an unknown population parameter given a sample of data\n\nEstimate\n\nA specific numerical value that we obtain from the smaple data by applying the estimator"
  },
  {
    "objectID": "lectures/03-Estimators-01/031-estimators.html#estimators",
    "href": "lectures/03-Estimators-01/031-estimators.html#estimators",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Let’s define some concepts first:\nEstimand\n\nQuantity that is to be estimated in a statistical analysis\n\nEstimator\n\nA rule (or formula) for estimating an unknown population parameter given a sample of data\n\nEstimate\n\nA specific numerical value that we obtain from the smaple data by applying the estimator"
  },
  {
    "objectID": "lectures/03-Estimators-01/031-estimators.html#estimators-example",
    "href": "lectures/03-Estimators-01/031-estimators.html#estimators-example",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Estimators Example",
    "text": "Estimators Example\nSuppose we want to know the average height of the population in the US\n\nWe have a sample of 1 million Americans\n\nSo then we can identify our Estimand, Estimator, and Estimate\n\nEstimand: The population mean \\((\\mu)\\)\nEstimator: The sample mean \\((\\bar{X})\\)\n\n\\[\n    \\bar{X} = \\dfrac{1}{n} \\sum_{i=1}^{n} X_{i}\n\\]\n\nEstimate: The sample mean \\((\\hat{\\mu} = 5'6'')\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/031-estimators.html#properties-of-estimators",
    "href": "lectures/03-Estimators-01/031-estimators.html#properties-of-estimators",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Properties of Estimators",
    "text": "Properties of Estimators\nThere are many ways to estimate things and they all have their benefits and costs.\nImagine we want to estimate an unknown parameter \\(\\mu\\), and we know the distributions of three competing estimators.\nWhich one should we use?"
  },
  {
    "objectID": "lectures/03-Estimators-01/031-estimators.html#properties-of-estimators---unbiasedness",
    "href": "lectures/03-Estimators-01/031-estimators.html#properties-of-estimators---unbiasedness",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Properties of Estimators - Unbiasedness",
    "text": "Properties of Estimators - Unbiasedness\nWe ask: What properties make an estimator reliable?\nAnswer (1): Unbiasedness\nOn average, does the estimator tend toward the correct value?\n\nFormally: Does the mean of the estimator’s distribution equal the parameter it estimates?\n\\[\n    \\text{Bias}_{\\mu} (\\hat{\\mu}) = E[\\hat{\\mu}] - \\mu\n\\]"
  },
  {
    "objectID": "lectures/03-Estimators-01/031-estimators.html#properties-of-estimators-1",
    "href": "lectures/03-Estimators-01/031-estimators.html#properties-of-estimators-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Properties of estimators",
    "text": "Properties of estimators\nQuestion What properties make an estimator reliable?\nA01: Unbiasedness\n\n\nUnbiased estimator: \\(E\\left[ \\hat{\\mu} \\right] = \\mu\\)\n\n\n\n\n\n\n\n\n\n\nBiased estimator \\(E\\left[ \\hat{\\mu} \\right] \\neq \\mu\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/031-estimators.html#properties-of-estimators---efficiency",
    "href": "lectures/03-Estimators-01/031-estimators.html#properties-of-estimators---efficiency",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Properties of Estimators - Efficiency",
    "text": "Properties of Estimators - Efficiency\nWe ask: What properties make an estimator reliable?\nAnswer (1): Efficiency (Low Variance)\nThe central tendencies (means) of competing distribution are not the only things that matter. We also care about the variance of an estimator.\n\\[\n    Var(\\hat{\\mu}) = E \\left[ (\\hat{\\mu} - E[\\hat{\\mu}])^{2} \\right]\n\\]\nLower variance estimators estimate closer to the mean in each sample"
  },
  {
    "objectID": "lectures/03-Estimators-01/031-estimators.html#properties-of-estimators---efficiency-1",
    "href": "lectures/03-Estimators-01/031-estimators.html#properties-of-estimators---efficiency-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Properties of Estimators - Efficiency",
    "text": "Properties of Estimators - Efficiency\nImagine low variance to be similar to accuracy \\(\\rightarrow\\) tighter estimates"
  },
  {
    "objectID": "lectures/03-Estimators-01/031-estimators.html#the-bias-variance-tradeoff",
    "href": "lectures/03-Estimators-01/031-estimators.html#the-bias-variance-tradeoff",
    "title": "EC 320 - Intro. Econometrics",
    "section": "The Bias-Variance Tradeoff",
    "text": "The Bias-Variance Tradeoff\nMuch like everything, there are tradeoffs from gaining one thing over another.\nShould we be willing to take a bit of bias to reduce the variance?\nIn economics/causal inference, we emphasize unbiasedness"
  },
  {
    "objectID": "lectures/03-Estimators-01/031-estimators.html#unbiased-estimators",
    "href": "lectures/03-Estimators-01/031-estimators.html#unbiased-estimators",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Unbiased estimators",
    "text": "Unbiased estimators\nIn addition to the sample mean, there are other unbiased estimators we will often use\n\n\nSample variance estimates the variance \\(\\sigma^{2}\\)\n\n\n\nSample covariance setimates the covariance \\(\\sigma_{XY}\\)\n\n\n\nSample correlation estimates the pop. correlation coefficient \\(\\rho_{XY}\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/031-estimators.html#sample-variance",
    "href": "lectures/03-Estimators-01/031-estimators.html#sample-variance",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Sample Variance",
    "text": "Sample Variance\nThe sample variance, \\(S_{X}^{2}\\), is an unbiased estimator of the population variance\n\n\\[\n    S_{X}^{2} = \\dfrac{1}{n - 1} \\sum_{i=1}^{n} (X_{i} - \\bar{X})^{2}.\n\\]"
  },
  {
    "objectID": "lectures/03-Estimators-01/031-estimators.html#sample-covariance",
    "href": "lectures/03-Estimators-01/031-estimators.html#sample-covariance",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Sample Covariance",
    "text": "Sample Covariance\nThe sample covariance, \\(S_{XY}\\), is an unbiaed estimator of the population covariance\n\n\\[\n    S_{XY} = \\dfrac{1}{n-1} \\sum_{i=1}^{n} (X_{i} - \\bar{X})(Y_{i} - \\bar{Y}).\n\\]"
  },
  {
    "objectID": "lectures/03-Estimators-01/031-estimators.html#sample-correlation",
    "href": "lectures/03-Estimators-01/031-estimators.html#sample-correlation",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Sample Correlation",
    "text": "Sample Correlation\nSample correlation, \\(r_{XY}\\), is an unbiased estimator of the population correlation coefficient\n\n\\[\n    r_{XY} = \\dfrac{S_{XY}}{\\sqrt{S_{X}^{2}}\\sqrt{S_{Y}^{2}}}.\n\\]"
  },
  {
    "objectID": "lectures/03-Estimators-01/033-linear-ex.html#ex.-effect-of-police-on-crime",
    "href": "lectures/03-Estimators-01/033-linear-ex.html#ex.-effect-of-police-on-crime",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Ex. Effect of police on crime",
    "text": "Ex. Effect of police on crime\n\nEmpirical question:\n\nDoes the number of on-campus police officers affect campus crime rates? If so, by how much?\n\n\n. . .\n\nAlways plot your data first"
  },
  {
    "objectID": "lectures/03-Estimators-01/033-linear-ex.html#ex.-effect-of-police-on-crime-1",
    "href": "lectures/03-Estimators-01/033-linear-ex.html#ex.-effect-of-police-on-crime-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Ex. Effect of police on crime",
    "text": "Ex. Effect of police on crime\nThe scatter plot suggest that a weak positive relationship exists\n\nA sample correlation of 0.14 confirms this\n\n\n. . .\nBut correlation does not imply causation\n. . .\n\nLets estimate a statistical model"
  },
  {
    "objectID": "lectures/03-Estimators-01/033-linear-ex.html#ex.-effect-of-police-on-crime-2",
    "href": "lectures/03-Estimators-01/033-linear-ex.html#ex.-effect-of-police-on-crime-2",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Ex. Effect of police on crime",
    "text": "Ex. Effect of police on crime\nWe express the relationship between a dependent variable and an independent variable as linear:\n\\[\n{\\text{Crime}_i} = \\beta_0 + \\beta_1 \\text{Police}_i + u_i.\n\\]\n\n\\(\\beta_0\\) is the intercept or constant.\n\\(\\beta_1\\) is the slope coefficient.\n\\(u_i\\) is an error term or disturbance term."
  },
  {
    "objectID": "lectures/03-Estimators-01/033-linear-ex.html#ex.-effect-of-police-on-crime-3",
    "href": "lectures/03-Estimators-01/033-linear-ex.html#ex.-effect-of-police-on-crime-3",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Ex. Effect of police on crime",
    "text": "Ex. Effect of police on crime\nThe intercept tells us the expected value of \\(\\text{Crime}_i\\) when \\(\\text{Police}_i = 0\\).\n\\[\n\\text{Crime}_i = {\\color{#BF616A} \\beta_{0}} + \\beta_1\\text{Police}_i + u_i\n\\]\nUsually not the focus of an analysis."
  },
  {
    "objectID": "lectures/03-Estimators-01/033-linear-ex.html#ex.-effect-of-police-on-crime-4",
    "href": "lectures/03-Estimators-01/033-linear-ex.html#ex.-effect-of-police-on-crime-4",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Ex. Effect of police on crime",
    "text": "Ex. Effect of police on crime\nThe slope coefficient tells us the expected change in \\(\\text{Crime}_i\\) when \\(\\text{Police}_i\\) increases by one.\n\\[\n\\text{Crime}_i = \\beta_0 + {\\color{#BF616A} \\beta_1} \\text{Police}_i + u_i\n\\]\n“A one-unit increase in \\(\\text{Police}_i\\) is associated with a \\(\\color{#BF616A}{\\beta_1}\\)-unit increase in \\(\\text{Crime}_i\\).”\n. . .\nInterpretation of this parameter is crucial\n. . .\nUnder certain (strong) assumptions1, \\(\\color{#BF616A}{\\beta_1}\\) is the effect of \\(X_i\\) on \\(Y_i\\).\n\nOtherwise, it’s the association of \\(X_i\\) with \\(Y_i\\)."
  },
  {
    "objectID": "lectures/03-Estimators-01/033-linear-ex.html#ex.-effect-of-police-on-crime-5",
    "href": "lectures/03-Estimators-01/033-linear-ex.html#ex.-effect-of-police-on-crime-5",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Ex. Effect of police on crime",
    "text": "Ex. Effect of police on crime\nThe error term reminds us that \\(\\text{Police}_i\\) does not perfectly explain \\(Y_i\\).\n\\[\n\\text{Crime}_i = \\beta_0 + \\beta_1\\text{Police}_i + {\\color{#BF616A} u_i}\n\\]\nRepresents all other factors that explain \\(\\text{Crime}_i\\).\n\nUseful mnemonic: pretend that \\(u\\) stands for “unobserved” or “unexplained.”"
  },
  {
    "objectID": "lectures/03-Estimators-01/033-linear-ex.html#ex.-effect-of-police-on-crime-6",
    "href": "lectures/03-Estimators-01/033-linear-ex.html#ex.-effect-of-police-on-crime-6",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Ex. Effect of police on crime",
    "text": "Ex. Effect of police on crime\nHow might we apply the simple linear regression model to our question about the effect of on-campus police on campus crime?\n\\[\n\\text{Crime}_i = \\beta_0 + \\beta_1\\text{Police}_i + u_i.\n\\]\n\n\\(\\beta_0\\) is the crime rate for colleges without police.\n\\(\\beta_1\\) is the increase in the crime rate for an additional police officer per 1000 students."
  },
  {
    "objectID": "lectures/03-Estimators-01/033-linear-ex.html#ex.-effect-of-police-on-crime-7",
    "href": "lectures/03-Estimators-01/033-linear-ex.html#ex.-effect-of-police-on-crime-7",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Ex. Effect of police on crime",
    "text": "Ex. Effect of police on crime\nHow might we apply the simple linear regression model to our question?\n\\[\n\\text{Crime}_i = \\beta_0 + \\beta_1\\text{Police}_i + u_i\n\\]\n\\(\\beta_0\\) and \\(\\beta_1\\) are the unobserved population parameters we want\n. . .\n\nWe estimate\n\n\\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) generate predictions of \\(\\text{Crime}_i\\) called \\(\\widehat{\\text{Crime}_i}\\).\nWe call the predictions of the dependent variable fitted values.\n\n. . .\n\nTogether, these trace a line: \\(\\widehat{\\text{Crime}_i} = \\hat{\\beta_0} + \\hat{\\beta_1}\\text{Police}_i\\)."
  },
  {
    "objectID": "lectures/03-Estimators-01/033-linear-ex.html#section-1",
    "href": "lectures/03-Estimators-01/033-linear-ex.html#section-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "So, the question becomes, how do I pick \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/033-linear-ex.html#section-2",
    "href": "lectures/03-Estimators-01/033-linear-ex.html#section-2",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Let’s take some guesses: \\(\\hat{\\beta_0} = 60\\) and \\(\\hat{\\beta}_{1} = -7\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/033-linear-ex.html#section-3",
    "href": "lectures/03-Estimators-01/033-linear-ex.html#section-3",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Let’s take some guesses: \\(\\hat{\\beta_0} = 30\\) and \\(\\hat{\\beta}_{1} = 0\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/033-linear-ex.html#section-4",
    "href": "lectures/03-Estimators-01/033-linear-ex.html#section-4",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Let’s take some guesses: \\(\\hat{\\beta_0} = 15.6\\) and \\(\\hat{\\beta}_{1} = 7.94\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/033-linear-ex.html#residuals",
    "href": "lectures/03-Estimators-01/033-linear-ex.html#residuals",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Residuals",
    "text": "Residuals\nUsing \\(\\hat{\\beta}_{0}\\) and \\(\\hat{\\beta}_{1}\\) to make \\(\\hat{y}_{i}\\) generates misses.\n\n\n\n \\(\\hat{\\beta_0} = 60 \\;\\) Guess\n\n \\(\\hat{\\beta_0} = 30 \\;\\) Guess\n\n \\(\\hat{\\beta_0} = 15 \\;\\) Guess"
  },
  {
    "objectID": "lectures/03-Estimators-01/033-linear-ex.html#residuals-sum-of-squares-rss",
    "href": "lectures/03-Estimators-01/033-linear-ex.html#residuals-sum-of-squares-rss",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Residuals Sum of Squares (RSS)",
    "text": "Residuals Sum of Squares (RSS)\nWhat if we picked an estimator that minimizes the residuals?\nWhy do we not minimize:\n\\[\n    \\sum_{i=1}^{n} \\hat{u}_{i}^{2}\n\\]\nso that the estimator makes fewer big misses?\nThis estimator, the residual sum of squares (RSS), is convenient because squared numbers are never negative so we can minimze an absolut sum of the residuals\nRSS will give bigger penalties to bigger residuals"
  },
  {
    "objectID": "lectures/03-Estimators-01/033-linear-ex.html#minimizing-rss",
    "href": "lectures/03-Estimators-01/033-linear-ex.html#minimizing-rss",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Minimizing RSS",
    "text": "Minimizing RSS\nWe could test thousands of guesses of \\(\\beta_0\\) and \\(\\beta_1\\) an pick the pair the has the smallest RSS\nWe could painstakingly do that, and eventually figure out which one fits best.\nOr… We could just do a little math"
  },
  {
    "objectID": "lectures/03-Estimators-01/033-linear-ex.html#footnotes",
    "href": "lectures/03-Estimators-01/033-linear-ex.html#footnotes",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAssumptions regarding the error term↩︎"
  },
  {
    "objectID": "lectures/03-Estimators-01/035-ols-interpretation.html",
    "href": "lectures/03-Estimators-01/035-ols-interpretation.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "There are two stages of interpretation of a regression equation\n\nInterpret regression estimates into words\nDeciding whether this interpretation should be taken at face value\n\n\nBoth stages are important, but for now, we will focus on the first\nLet’s revisit our crime example"
  },
  {
    "objectID": "lectures/03-Estimators-01/035-ols-interpretation.html#interpretation",
    "href": "lectures/03-Estimators-01/035-ols-interpretation.html#interpretation",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "There are two stages of interpretation of a regression equation\n\nInterpret regression estimates into words\nDeciding whether this interpretation should be taken at face value\n\n\nBoth stages are important, but for now, we will focus on the first\nLet’s revisit our crime example"
  },
  {
    "objectID": "lectures/03-Estimators-01/035-ols-interpretation.html#ex-effect-of-police-on-crime",
    "href": "lectures/03-Estimators-01/035-ols-interpretation.html#ex-effect-of-police-on-crime",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Ex: Effect of Police on Crime",
    "text": "Ex: Effect of Police on Crime\nUsing the OLS formulas, we get \\(\\hat{\\beta}_{0} = 18.41\\) and \\(\\hat{\\beta}_{1} = 1.76\\)"
  },
  {
    "objectID": "lectures/03-Estimators-01/035-ols-interpretation.html#coefficient-interpretation",
    "href": "lectures/03-Estimators-01/035-ols-interpretation.html#coefficient-interpretation",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Coefficient Interpretation",
    "text": "Coefficient Interpretation\nHow do I interpret \\(\\hat{\\beta}_{0} = 18.41\\) and \\(\\hat{\\beta}_{1} = 1.76\\)?\nThe general interpration of the intercept is the estimated value of \\(y_{i}\\) when \\(x_{i} = 0\\)\nAnd the general interpretation of the slope parameter is the estimated change \\(y_{i}\\) for the marginal increase \\(x_{i}\\)\n. . .\nFirst, it is important to understand the units:\n\n\\(\\widehat{\\text{Crime}}_{i}\\) is measured as a crime rate, the number of crimes per 1,000 students on campus\n\\(\\text{Police}_{i}\\) is also measured as a rate, the number of police officers per 1,000 students on campus"
  },
  {
    "objectID": "lectures/03-Estimators-01/035-ols-interpretation.html#coefficient-interpretation-1",
    "href": "lectures/03-Estimators-01/035-ols-interpretation.html#coefficient-interpretation-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Coefficient Interpretation",
    "text": "Coefficient Interpretation\nUsing OLS gives us the fitted line\n\\[\n\\widehat{\\text{Crime}_i} = \\hat{\\beta}_1 + \\hat{\\beta}_2\\text{Police}_i.\n\\]\nWhat does \\(\\hat{\\beta_0}\\) = \\(18.41\\) tell us? Without any police on campus, the crime rate is \\(18.41\\) per 1,000 people on campus\n. . .\nWhat does \\(\\hat{\\beta_1}\\) = \\(1.76\\) tell us? For each additional police officer per 1,000, there is an associated increase in the crime rate by \\(1.76\\) crimes per 1,000 people on campus.\n. . .\nDoes this mean that police cause crime? Probably not.\nThis is where deciding if the interpretation should be taken at face value. It now becomes your job to bring reason to the values."
  },
  {
    "objectID": "lectures/04-Estimators-02/041-ols-properties.html",
    "href": "lectures/04-Estimators-02/041-ols-properties.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "There are three important OLS properties\n\n\nThe point \\((\\bar{x}, \\bar{y})\\) is always on the regression line\n\n\n\nResiduals sum to zero: \\(\\sum_{i}^{n} \\hat{u}_{i} = 0\\)\n\n\n\nThe sample covariance between the independent variable and the residuals is zero: \\(\\sum_{i}^{n} x_{i}\\hat{u}_{i} = 0\\)"
  },
  {
    "objectID": "lectures/04-Estimators-02/041-ols-properties.html#important-properties",
    "href": "lectures/04-Estimators-02/041-ols-properties.html#important-properties",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "There are three important OLS properties\n\n\nThe point \\((\\bar{x}, \\bar{y})\\) is always on the regression line\n\n\n\nResiduals sum to zero: \\(\\sum_{i}^{n} \\hat{u}_{i} = 0\\)\n\n\n\nThe sample covariance between the independent variable and the residuals is zero: \\(\\sum_{i}^{n} x_{i}\\hat{u}_{i} = 0\\)"
  },
  {
    "objectID": "lectures/04-Estimators-02/041-ols-properties.html#property-1---proof",
    "href": "lectures/04-Estimators-02/041-ols-properties.html#property-1---proof",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Property 1 - Proof",
    "text": "Property 1 - Proof\nThe point \\((\\bar{x}, \\bar{y})\\) is always on the regression line\n\nStart with the regression line: \\(\\hat{y}_{i} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1} x_{i}\\)\nRecall that \\(\\hat{\\beta}_{0} = \\bar{y} - \\hat{\\beta}_{1}\\bar{x}\\)\nPlug that in \\(\\hat{\\beta}_{0}\\) and substitute \\(\\bar{x}\\) for \\(x_{i}\\):\n\n\\[\\begin{align*}\n    \\hat{y}_{i} &= \\bar{y} - \\hat{\\beta}_{1}\\bar{x} + \\hat{\\beta}_{1} \\bar{x} \\\\\n    \\hat{y}_{i} &= \\bar{y}\n\\end{align*}\\]"
  },
  {
    "objectID": "lectures/04-Estimators-02/041-ols-properties.html#property-2---proof",
    "href": "lectures/04-Estimators-02/041-ols-properties.html#property-2---proof",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Property 2 - Proof",
    "text": "Property 2 - Proof\nResiduals sum to zero: \\(\\sum_{i}^{n} \\hat{u}_{i} = 0\\)\n\nRecall a couple of things we have derived:\n\n\\[\n    \\hat{y}_{i} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1}x_{i} \\;\\; \\text{and} \\;\\; \\hat{u}_{i} = y_{i} - \\hat{y}_{i}\n\\]\n\nThe sum of residuals is:\n\n\\[\n    \\sum_{i} \\hat{u}_{i} = \\sum_{i} (y_{i} - \\hat{y}_{i}) = \\sum_{i} y_{i} - \\sum \\hat{y}_{i}\n\\]\n\nRecall the fact that \\(\\sum_{i} y_{i} = n\\bar{y}\\) and also:\n\n\\[\\begin{align*}\n    \\sum_{i} \\hat{y}_{i} &= \\sum_{i} (\\hat{\\beta}_{0} + \\hat{\\beta}_{1}x_{i})\n    = n \\hat{\\beta}_{0} + \\hat{\\beta}_{1} \\sum_{i} x_{i} \\\\\n    &= n (\\bar{y}_{i} - \\hat{\\beta}_{1}\\bar{x}) + \\hat{\\beta}_{1} n\\bar{x} = n\\bar{y}_{i}\n\\end{align*}\\]\n\nSo:\n\n\\[\n    \\sum_{i} \\hat{u}_{i} = n\\bar{y}_{i} - n\\bar{y}_{i} = 0\n\\]"
  },
  {
    "objectID": "lectures/04-Estimators-02/041-ols-properties.html#property-3---proof",
    "href": "lectures/04-Estimators-02/041-ols-properties.html#property-3---proof",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Property 3 - Proof",
    "text": "Property 3 - Proof\nThe sample covariance between the independent variable and the residuals is zero: \\(\\sum_{i}^{n} x_{i}\\hat{u}_{i} = 0\\)\n\nStart with our residuals: \\(\\hat{u}_{i} = y_{i} - \\hat{\\beta}_{0} - \\hat{\\beta}_{1}x_{i}\\)\nMultiply both sides by \\(x_{i}\\) and sum them:\n\n\\[\n    \\sum_{i} x_{i}\\hat{u}_{i} = \\sum_{i} x_{i}y_{i} - \\hat{\\beta}_{0}\\sum_{i} x_{i} - \\hat{\\beta}_{1}\\sum_{i} x_{i}^{2}\n\\]\n\nRecall from our \\(\\hat{\\beta}_{1}\\) derivation that \\(\\sum_{i} x_{i}y_{i} = \\hat{\\beta}_{0}\\sum_{i} x_{i} + \\hat{\\beta}_{1}\\sum_{i} x_{i}^{2}\\)\n\nSo: \\(\\sum_{i}^{n} x_{i}\\hat{u}_{i} = \\hat{\\beta}_{0}\\sum_{i} x_{i} + \\hat{\\beta}_{1}\\sum_{i} x_{i}^{2} - \\hat{\\beta}_{0}\\sum_{i} x_{i} - \\hat{\\beta}_{1}\\sum_{i} x_{i}^{2} = 0\\)"
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "The most important assumptions concern the error term \\(u_{i}\\).\nImportant: An error \\(u_{i}\\) and a residual \\(\\hat{u}_{i}\\) are related, but different.\nTake for example, a model of the effects of education on wages.\n\nError:\n\nDifference between the wage of a worker with 11 years of education and the expected wage with 11 years of education\n\nResidual:\n\nDifference between the wage of a worker with 11 years of education and the average wage of workers with 11 years of education\n\n. . .\n\nPopulation vs. Sample"
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#residuals-vs-errors",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#residuals-vs-errors",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "The most important assumptions concern the error term \\(u_{i}\\).\nImportant: An error \\(u_{i}\\) and a residual \\(\\hat{u}_{i}\\) are related, but different.\nTake for example, a model of the effects of education on wages.\n\nError:\n\nDifference between the wage of a worker with 11 years of education and the expected wage with 11 years of education\n\nResidual:\n\nDifference between the wage of a worker with 11 years of education and the average wage of workers with 11 years of education\n\n. . .\n\nPopulation vs. Sample"
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#residuals-vs-errors-1",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#residuals-vs-errors-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Residuals vs Errors",
    "text": "Residuals vs Errors\nA residual tells us how a worker’s wages comapre to the average wages of workers in the sample with the same level of education"
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#residuals-vs-errors-2",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#residuals-vs-errors-2",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Residuals vs Errors",
    "text": "Residuals vs Errors\nA residual tells us how a worker’s wages comapre to the average wages of workers in the sample with the same level of education"
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#residuals-vs-errors-3",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#residuals-vs-errors-3",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Residuals vs Errors",
    "text": "Residuals vs Errors\nAn error tells us how a worker’s wages compare to the expected wages of workers in the population with the same level of education"
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#classical-assumptions-of-ols",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#classical-assumptions-of-ols",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Classical Assumptions of OLS",
    "text": "Classical Assumptions of OLS\nA1. Linearity: The population relationship is linear in parameters with an additive error term\nA2. Sample Variation: There is variation in \\(X\\)\nA3. Exogeneity: The \\(X\\) variable is exogenous\nA4. Homosekdasticity: The error term has the same variance for each value of the independent variable\nA5. Non-Autocorrelation: The values of error terms have independent distributions\nA6. Normality: The population error term is normally distributed with mean zero and variance \\(\\sigma^{2}\\)"
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#a1.-linearity",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#a1.-linearity",
    "title": "EC 320 - Intro. Econometrics",
    "section": "A1. Linearity",
    "text": "A1. Linearity\n\nThe population relationship is linear in parameters with an additive error term\n\nExamples\n\n\\(\\text{Wage}_i = \\beta_1 + \\beta_2 \\text{Experience}_i + u_i\\)\n\n. . .\n\n\\(\\log(\\text{Happiness}_i) = \\beta_1 + \\beta_2 \\log(\\text{Money}_i) + u_i\\)\n\n. . .\n\n\\(\\sqrt{\\text{Convictions}_i} = \\beta_1 + \\beta_2 (\\text{Early Childhood Lead Exposure})_i + u_i\\)\n\n. . .\n\n\\(\\log(\\text{Earnings}_i) = \\beta_1 + \\beta_2 \\text{Education}_i + u_i\\)"
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#a1.-linearity-1",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#a1.-linearity-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "A1. Linearity",
    "text": "A1. Linearity\n\nThe population relationship is linear in parameters with an additive error term.\n\nViolations\n\n\\(\\text{Wage}_i = (\\beta_1 + \\beta_2 \\text{Experience}_i)u_i\\)\n\n. . .\n\n\\(\\text{Consumption}_i = \\frac{1}{\\beta_1 + \\beta_2 \\text{Income}_i} + u_i\\)\n\n. . .\n\n\\(\\text{Population}_i = \\frac{\\beta_1}{1 + e^{\\beta_2 + \\beta_3 \\text{Food}_i}} + u_i\\)\n\n. . .\n\n\\(\\text{Batting Average}_i = \\beta_1 (\\text{Wheaties Consumption})_i^{\\beta_2} + u_i\\)"
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#a2.-sample-variation",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#a2.-sample-variation",
    "title": "EC 320 - Intro. Econometrics",
    "section": "A2. Sample Variation",
    "text": "A2. Sample Variation\n\nThere is variation in \\(X\\).\n\nExample"
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#a2.-sample-variation-1",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#a2.-sample-variation-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "A2. Sample Variation",
    "text": "A2. Sample Variation\n\nThere is variation in \\(X\\).\n\nViolation\n\n\n\n\n\nWe will see later that variation matters for inference as well"
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#a3.-exogeneity",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#a3.-exogeneity",
    "title": "EC 320 - Intro. Econometrics",
    "section": "A3. Exogeneity",
    "text": "A3. Exogeneity\n\nThe \\(X\\) variable is exogenous\n\nWe can write this as:\n\\[\n    \\mathbb{E}[(u|X)] = 0\n\\]\nWhich essentially says that the expected value of the errors term, conditional on the variable \\(X\\) is 0. The assignment of \\(X\\) is effectively random.\nA significant implication of this is no selection bias or omitted variable bias"
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#a3.-exogeneity-1",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#a3.-exogeneity-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "A3. Exogeneity",
    "text": "A3. Exogeneity\n\nThe \\(X\\) variable is exogenous\n\n\\[\n    \\mathbb{E}[(u|X)] = 0\n\\]\nExample\nIn the labor market, an important component of \\(u\\) is unobserved ability\n\n\\(\\mathbb{E}(u|\\text{Education} = 12) = 0\\) and \\(\\mathbb{E}(u|\\text{Education} = 20) = 0\\)\n\\(\\mathbb{E}(u|\\text{Education} = 0) = 0\\) and \\(\\mathbb{E}(u|\\text{Education} = 40) = 0\\)\n\nnote: This is an assumption that does not necessarily hold true in real life, but with enough observations we can comfortably assume something like this"
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#a3.-exogeneity-2",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#a3.-exogeneity-2",
    "title": "EC 320 - Intro. Econometrics",
    "section": "A3. Exogeneity",
    "text": "A3. Exogeneity\n\n\nValid Exogeneity\n\\[\n    \\mathbb{E}[(u|X)] = 0\n\\]\n\n\n\n\n\n\nInvalid Exogeneity\n\\[\n    \\mathbb{E}[(u|X)] \\neq 0\n\\]"
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#interlude-unbiasedness-of-ols",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#interlude-unbiasedness-of-ols",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Interlude: Unbiasedness of OLS",
    "text": "Interlude: Unbiasedness of OLS\nWhen can we trust OLS?\nIn estimators, the concept of bias means that the expected value of the estimate is different from the true population parameter.\nGraphically we have:\n\n\nUnbiased estimator: \\(\\mathop{\\mathbb{E}}\\left[ \\hat{\\beta} \\right] = \\beta\\)\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\nBiased estimator: \\(\\mathop{\\mathbb{E}}\\left[ \\hat{\\beta} \\right] \\neq \\beta\\)"
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#is-ols-unbiased",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#is-ols-unbiased",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Is OLS Unbiased?",
    "text": "Is OLS Unbiased?\nWe require our first 3 assumptions for unbaised OLS estimator\nA1. Linearity: The population relationship is linear in parameters with an additive error term\nA2. Sample Variation: There is variation in \\(X\\)\nA3. Exogeneity: The \\(X\\) variable is exogenous\nAnd we can mathematically prove it!"
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#proving-unbiasedness-of-ols",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#proving-unbiasedness-of-ols",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Proving Unbiasedness of OLS",
    "text": "Proving Unbiasedness of OLS\nSuppose we have the following model\n\\[\n    y_{i} = \\beta_{1} + \\beta_{2}x_{i} + u_{i}\n\\]\n. . .\nThe slope parameter follows as:\n\\[\n\\hat{\\beta}_2 = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum(x_i - \\bar{x})^2}\n\\]\n. . .\n(As shown in section 2.3 in ItE) that the estimator \\(\\hat{\\beta_2}\\), can be broken up into a nonrandom and a random component:\n\nProving unbiasedness of simple OLS\nSubstitute for \\(y_i\\):\n\\[\n\\hat{\\beta}_2 = \\frac{\\sum((\\beta_1 + \\beta_2x_i + u_i) - \\bar{y})(x_i - \\bar{x})}{\\sum(x_i - \\bar{x})^2}\n\\]\n. . .\nSubstitute \\(\\bar{y} = \\beta_1 + \\beta_2\\bar{x}\\):\n\\[\n\\hat{\\beta}_2 = \\frac{\\sum(u_i(x_i - \\bar{x}))}{\\sum(x_i - \\bar{x})^2} + \\frac{\\sum(\\beta_2x_i(x_i - \\bar{x}))}{\\sum(x_i - \\bar{x})^2}\n\\]\n. . .\nThe non-random component, \\(\\beta_2\\), is factored out:\n\\[\n\\hat{\\beta}_2 = \\frac{\\sum(u_i(x_i - \\bar{x}))}{\\sum(x_i - \\bar{x})^2} + \\beta_2\\frac{\\sum(x_i(x_i - \\bar{x}))}{\\sum(x_i - \\bar{x})^2}\n\\]\n\nProving unbiasedness of simple OLS\nObserve that the second term is equal to 1. Thus, we have:\n\\[\n\\hat{\\beta}_2 = \\beta_2 + \\frac{\\sum(u_i(x_i - \\bar{x}))}{\\sum(x_i - \\bar{x})^2}\n\\]\n. . .\nTaking the expectation,\n\\[\n\\mathbb{E}[\\hat{\\beta_2}] = \\mathbb{E}[\\beta] + \\mathbb{E} \\left[\\frac{\\sum \\hat{u_i} (x_i - \\bar{x})}{\\sum(x_i - \\bar{x})^2} \\right]\n\\]\n. . .\nBy Rules 01 and 02 of expected value and A3:\n\\[\n\\begin{equation*}\n  \\mathbb{E}[\\hat{\\beta_2}] = \\beta + \\frac{\\sum \\mathbb{E}[\\hat{u_i}] (x_i - \\bar{x})}{\\sum(x_i - \\bar{x})^2} = \\beta\n\\end{equation*}\n\\]"
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#required-assumptions",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#required-assumptions",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Required Assumptions",
    "text": "Required Assumptions\nA1. Linearity: The population relationship is linear in parameters with an additive error term.\nA2. Sample Variation: There is variation in \\(X\\).\nA3. Exogeniety: The \\(X\\) variable is exogenous\n\nA3 implies random sampling.\n\n\nResult: OLS is unbiased."
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#classical-assumptions-of-ols-1",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#classical-assumptions-of-ols-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Classical Assumptions of OLS",
    "text": "Classical Assumptions of OLS\nA1. Linearity: The population relationship is linear in parameters with an additive error term.\nA2. Sample Variation: There is variation in \\(X\\).\nA3. Exogeniety: The \\(X\\) variable is exogenous\n. . .\n \n\nThe following 2 assumptions are not required for unbiasedness…\n\n\nBut they are important for an efficient estimator\n\n\nLet’s talk about why variance matters"
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#why-variance-matters",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#why-variance-matters",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Why variance matters",
    "text": "Why variance matters\nUnbiasedness tells us that OLS gets it right, on average. But we can’t tell whether our sample is “typical.”\n\n. . .\nVariance tells us how far OLS can deviate from the population mean.\n\nHow tight is OLS centered on its expected value?\nThis determines the efficiency of our estimator."
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#why-variance-matters-1",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#why-variance-matters-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Why variance matters",
    "text": "Why variance matters\nUnbiasedness tells us that OLS gets it right, on average. But we can’t tell whether our sample is “typical.”\n\nThe smaller the variance, the closer OLS gets, on average, to the true population parameters on any sample.\n\nGiven two unbiased estimators, we want the one with smaller variance.\nIf two more assumptions are satisfied, we are using the most efficient linear estimator."
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#classical-assumptions-of-ols-2",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#classical-assumptions-of-ols-2",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Classical Assumptions of OLS",
    "text": "Classical Assumptions of OLS\nA1. Linearity: The population relationship is linear in parameters with an additive error term.\nA2. Sample Variation: There is variation in \\(X\\).\nA3. Exogeniety: The \\(X\\) variable is exogenous\n. . .\nA4. Homoskedasticity: The error term has the same variance for each value of the independent variable"
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#a4.-homoskedasticity",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#a4.-homoskedasticity",
    "title": "EC 320 - Intro. Econometrics",
    "section": "A4. Homoskedasticity",
    "text": "A4. Homoskedasticity\n\nThe error term has the same variance for each value of the independent variable \\(x_{i}\\)\n\n\\[\n    Var(u|X) = \\sigma^{2}.\n\\]\nExample:"
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#a4.-homoskedasticity-1",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#a4.-homoskedasticity-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "A4. Homoskedasticity",
    "text": "A4. Homoskedasticity\n\nThe error term has the same variance for each value of the independent variable \\(x_{i}\\)\n\n\\[\n    Var(u|X) = \\sigma^{2}.\n\\]\nViolation:"
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#a4.-homoskedasticity-2",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#a4.-homoskedasticity-2",
    "title": "EC 320 - Intro. Econometrics",
    "section": "A4. Homoskedasticity",
    "text": "A4. Homoskedasticity\n\nThe error term has the same variance for each value of the independent variable \\(x_{i}\\)\n\n\\[\n    Var(u|X) = \\sigma^{2}.\n\\]\nViolation:"
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#heteroskedasticity-example",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#heteroskedasticity-example",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Heteroskedasticity Example",
    "text": "Heteroskedasticity Example\nSuppose we study the following relationship:\n\\[\n\\text{Luxury Expenditure}_i = \\beta_1 + \\beta_2 \\text{Income}_i + u_i\n\\]\n\nAs income increases, variation in luxury expenditures increase\n\nVariance of \\(u_i\\) is likely larger for higher-income households\nPlot of the residuals against the household income would likely reveal a funnel-shaped pattern"
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#section",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#section",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Common test for heteroskedasticity… Plot the residuals across \\(X\\)"
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#classical-assumptions-of-ols-3",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#classical-assumptions-of-ols-3",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Classical Assumptions of OLS",
    "text": "Classical Assumptions of OLS\nA1. Linearity: The population relationship is linear in parameters with an additive error term.\nA2.Sample Variation: There is variation in \\(X\\).\nA3. Exogeniety: The \\(X\\) variable is exogenous\nA4. Homoskedasticity: The error term has the same variance for each value of the independent variable\n. . .\nA5. Non-autocorrelation: The values of error terms have independent distributions"
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#a5.-non-autocorrelation",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#a5.-non-autocorrelation",
    "title": "EC 320 - Intro. Econometrics",
    "section": "A5. Non-Autocorrelation",
    "text": "A5. Non-Autocorrelation\n\nThe values of error terms have independent distributions1\n\n\\[\nE[u_i u_j]=0, \\forall i \\text{ s.t. } i \\neq j\n\\]\n. . .\nOr…\n\\[\n\\begin{align*}\n\\mathop{\\text{Cov}}(u_i, u_j) &= E[(u_i - \\mu_u)(u_j - \\mu_u)]\\\\\n                              &= E[u_i u_j] = E[u_i] E[u_j]  = 0, \\text{where } i \\neq j\n\\end{align*}\n\\]"
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#a5.-non-autocorrelation-1",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#a5.-non-autocorrelation-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "A5. Non-Autocorrelation",
    "text": "A5. Non-Autocorrelation\n\nThe values of error terms have independent distributions\n\n\\[\nE[u_i u_j]=0, \\forall i \\text{ s.t. } i \\neq j\n\\]\n\nImplies no systematic association between pairs of individual \\(u_i\\)\nAlmost always some unobserved correlation across individuals2\nReferred to as clustering problem.\nAn easy solution exists where we can adjust our standard errors"
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#section-1",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#section-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Let’s take a moment to talk about the variance of the OLS estimator\n\n\\[\n    Var(\\hat{\\beta}_{1}) = \\dfrac{\n        \\sigma^{2}\n        }{\n        \\sum (x_{i} - \\bar{x})^{2}\n        }\n\\]"
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#classical-assumptions-of-ols-4",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#classical-assumptions-of-ols-4",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Classical Assumptions of OLS",
    "text": "Classical Assumptions of OLS\nA1. Linearity: The population relationship is linear in parameters with an additive error term.\nA2. Sample Variation: There is variation in \\(X\\).\nA3. Exogeniety: The \\(X\\) variable is exogenous\nA4. Homoskedasticity: The error term has the same variance for each value of the independent variable\nA5. Non-autocorrelation: The values of error terms have independent distributions\n\nIf A4 and A5 are satisfied, along with A1, A2, and A3, then we are using the most efficient linear estimator"
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#classical-assumptions-of-ols-5",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#classical-assumptions-of-ols-5",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Classical Assumptions of OLS",
    "text": "Classical Assumptions of OLS\nA1. Linearity: The population relationship is linear in parameters with an additive error term.\nA2. Sample Variation: There is variation in \\(X\\).\nA3. Exogeniety: The \\(X\\) variable is exogenous\nA4. Homoskedasticity: The error term has the same variance for each value of the independent variable\nA5. Non-autocorrelation: The values of error terms have independent distributions\n. . .\nA6. Normality The population error term in normally distributed with mean zero and variance \\(\\sigma^{2}\\)"
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#a6.-normality",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#a6.-normality",
    "title": "EC 320 - Intro. Econometrics",
    "section": "A6. Normality",
    "text": "A6. Normality\n\nThe population error term in normally distributed with mean zero and variance \\(\\sigma^{2}\\)\n\nAlso known as:\n\\[\n    u \\sim N(0,\\sigma^{2})\n\\]\nWhere \\(\\sim\\) means distributed by and \\(N\\) stands for normal distribution\nHowever, A6 is not required for efficiency nor unbiasedness"
  },
  {
    "objectID": "lectures/04-Estimators-02/043-classical-assumptions.html#footnotes",
    "href": "lectures/04-Estimators-02/043-classical-assumptions.html#footnotes",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNotes: \\(\\forall i = \\text{for all} \\: i\\), \\(\\text{s.t.} = \\text{such that}\\), \\(i \\neq j \\: \\text{means} \\: i \\: \\text{is not equal to} \\: j\\)↩︎\n(e.g. common correlation in unobservables among individuals within a given US state)↩︎"
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#ols",
    "href": "lectures/05-Inference/050-compile.html#ols",
    "title": "Inference",
    "section": "OLS",
    "text": "OLS\nUp to now, we have been focusing on OLS considering:\n\nHow we model regressions with this estimator\nHow the estimator is derived and what properties it demonstrates\nHow the classical assumptions make the estimator BLUE\n\n\nWe have mostly ignored drawing conclusions about the true population parameters from the estimates of the sample data\n\nThis is inference"
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#ols-1",
    "href": "lectures/05-Inference/050-compile.html#ols-1",
    "title": "Inference",
    "section": "OLS",
    "text": "OLS\nThus far we have fit an OLS model to find an answer to the following questions:\n\nHow much does an additional year of schooling increase earnings?\nDoes the number of police officers affect campus crime rates?\n\n\nUp to now, we have not discussed our confidence in our fitted relationship\nEven if all 6 Assumptions hold, sample selection might generate the incorrect conclusions in a completely unbiased, coincidental fashion."
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#sampling-distribution",
    "href": "lectures/05-Inference/050-compile.html#sampling-distribution",
    "title": "Inference",
    "section": "Sampling distribution",
    "text": "Sampling distribution\n\nThe probability distribution of the OLS estimators obtained from repeatedly drawing random samples of the same size from a population and fitting point estimates each time.\n\nProvides information about their variability, accuracy, and precision across different samples.\n\n\nPoint estimates\n\nThe fitted values of the OLS estimator (e.g., \\(\\hat{\\beta}_0, \\hat{\\beta}_1\\))"
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#sampling-distribution-properties",
    "href": "lectures/05-Inference/050-compile.html#sampling-distribution-properties",
    "title": "Inference",
    "section": "Sampling distribution properties",
    "text": "Sampling distribution properties\n1. Unbiasedness: If the Gauss-Markov assumptions hold, the OLS estimators are unbiased (i.e., \\(E(\\hat{\\beta}_0) = \\beta_0\\) and \\(E(\\hat{\\beta}_1) = \\beta_1\\))\n\n2. Variance: The variance of the OLS estimators describes their dispersion around the true population parameters.\n\n\n3. Normality: If the errors are normally distributed or the sample size is large enough, by the Central Limit Theorem, the sampling distribution of the OLS estimators will be approximately normal.1\n\nUseful for making inferences, constructing confidence intervals, and performing hypothesis tests using the t-distribution."
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#sampling-distribution-1",
    "href": "lectures/05-Inference/050-compile.html#sampling-distribution-1",
    "title": "Inference",
    "section": "Sampling distribution",
    "text": "Sampling distribution\nThe sampling distribution of \\(\\hat{\\beta}\\) to conduct hypothesis tests.\nUse all 6 classical assumptions to show that OLS is normally distributed:\n\\[\n\\hat{\\beta} \\sim \\mathop{N}\\left( \\beta, \\frac{\\sigma^2}{\\sum_{i=1}^n (X_i - \\bar{X})^2} \\right)\n\\]\n\nLet’s look at a simulation"
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#inference-1",
    "href": "lectures/05-Inference/050-compile.html#inference-1",
    "title": "Inference",
    "section": "Inference",
    "text": "Inference\nOur current workflow consists of:\n\n\nGet data (points with \\(X\\) and \\(Y\\) values)\nRegress \\(Y\\) on \\(X\\) (aka \\(Y = \\beta_{0} + \\beta_{1}X\\))\nPlot the point estimates \\((\\hat{\\beta}_{0},\\hat{\\beta}_{1})\\) and report them\n\n\n\nBut when do we learn something? We are still missing a step.\n\nFor \\(\\hat{\\beta}_{1}\\), can we rule out a previously hypothesized values?\nHow condifent should we be in the precision of our estimates?\n\nWe need to be careful about our sample being atypical \\(\\Rightarrow\\) uncertainty"
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#section-1",
    "href": "lectures/05-Inference/050-compile.html#section-1",
    "title": "Inference",
    "section": "",
    "text": "However, there is a problem.\n\n \nRecall the variance of the point estimate \\(\\hat{\\beta_1}\\) \\[\n\\mathop{\\text{Var}}(\\hat{\\beta}_1) = \\frac{\\sigma^2}{\\sum_{i=1}^n (X_i - \\bar{X})^2}\n\\]\nThe problem is that \\(\\color{#BF616A}{\\sigma^2}\\) is unobserved. So what do we do? Estimate it."
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#estimating-error-variance",
    "href": "lectures/05-Inference/050-compile.html#estimating-error-variance",
    "title": "Inference",
    "section": "Estimating error variance",
    "text": "Estimating error variance\nWe can estimate the variance of \\(u_i\\) (\\(\\color{#BF616A}{\\sigma^2}\\)) using the sum of squared residuals (RSS):\n\\[\ns^2_u = \\dfrac{\\sum_i \\hat{u}_i^2}{n - k}\n\\]\nwhere \\(n\\) is the number of observations and \\(k\\) is the number of regression parameters. (In a simple linear regression, \\(k=2\\))\n\nIf the assumptions from Gauss-Markov hold, then \\(s^2_u\\) is an unbiased estimator of \\(\\sigma^2\\).\n\n\nIn essence, we are learning from our prediction errors"
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#ols-variance",
    "href": "lectures/05-Inference/050-compile.html#ols-variance",
    "title": "Inference",
    "section": "OLS Variance",
    "text": "OLS Variance\nWith \\(s^2_u = \\dfrac{\\sum_i \\hat{u}_i^2}{n - k}\\), we can calculate the estimated variance of \\(\\hat{\\beta}_1\\)\n\\[\n\\mathop{\\text{Var}}(\\hat{\\beta}_1) = \\frac{s^2_u}{\\sum_{i=1}^n (X_i - \\bar{X})^2}\n\\]\n\nTaking the square root, we get the standard error of the OLS estimator:\n\\[\n\\mathop{\\widehat{\\text{SE}}} \\left( \\hat{\\beta}_1 \\right) = \\sqrt{ \\frac{s^2_u}{\\sum_{i=1}^n (X_i - \\bar{X})^2} }\n\\]\nThe standard error is the standard deviation of the sampling distribution."
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#inference-2",
    "href": "lectures/05-Inference/050-compile.html#inference-2",
    "title": "Inference",
    "section": "Inference",
    "text": "Inference\nAfter deriving the distribution of \\(\\hat{\\beta}_1\\)1, we have two (related) options for formal statistical inference (learning) about our unknown parameter \\(\\beta_1\\):\n\n\nHypothesis testing: Determine whether there is statistically significant evidence to reject a hypothesized value or range of values.\nConfidence intervals: Use the estimate and its standard error to create an interval that will generally2 contain the true parameter.\n\nHint: It’s normal with mean \\(\\beta_1\\) and variance \\(\\frac{\\sigma^2}{\\sum_{i=1}^n (X_i - \\bar{X})^2}\\).E.g., similarly constructed 95% confidence intervals will contain the true parameter 95% of the time."
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#hypothesis-tests",
    "href": "lectures/05-Inference/050-compile.html#hypothesis-tests",
    "title": "Inference",
    "section": "Hypothesis Tests",
    "text": "Hypothesis Tests\nSystematic procedure that gives us evidence to hang our hat on. Starting with a Null hypothesis (\\(H_0\\)) and an Alternative hypothesis (\\(H_1\\))\n\\[\n\\begin{align*}\nH_0:& \\beta_1 = 0 \\\\\nH_1:& \\beta_1 \\neq 0\n\\end{align*}\n\\]\n\nIn the context of the wage regression:\n\\[\n\\text{Wage}_i = \\beta_0 + \\beta_1 \\cdot \\text{Education}_i + u_i\n\\]\n\n\\(H_0\\): Education has no effect on wage\n\n\n\\(H_1\\): Education has an effect on wage"
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#possible-outcomes",
    "href": "lectures/05-Inference/050-compile.html#possible-outcomes",
    "title": "Inference",
    "section": "Possible outcomes",
    "text": "Possible outcomes\nWithin this structure, four possible outcomes exist:\n\n\n1. We fail to reject the null hypothesis and the null is true.\n\n\nEx. Education has no effect on wage and, correctly, we fail to reject \\(H_0\\)."
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#possible-outcomes-1",
    "href": "lectures/05-Inference/050-compile.html#possible-outcomes-1",
    "title": "Inference",
    "section": "Possible outcomes",
    "text": "Possible outcomes\nWithin this structure, four possible outcomes exist:\n\n1. We fail to reject the null hypothesis and the null is true.\n2. We reject the null hypothesis and the null is false.\n\nEx. Education has an effect on wage and, correctly, we reject \\(H_0\\)."
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#possible-outcomes-2",
    "href": "lectures/05-Inference/050-compile.html#possible-outcomes-2",
    "title": "Inference",
    "section": "Possible outcomes",
    "text": "Possible outcomes\nWithin this structure, four possible outcomes exist:\n\n1. We fail to reject the null hypothesis and the null is true.\n2. We reject the null hypothesis and the null is false.\n3. We reject the null hypothesis, but the null is actually true.\n\nEx. Education has no effect on wage, but we incorrectly reject \\(H_0\\).\nThis is an error. Defined as a Type I error."
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#possible-outcomes-3",
    "href": "lectures/05-Inference/050-compile.html#possible-outcomes-3",
    "title": "Inference",
    "section": "Possible outcomes",
    "text": "Possible outcomes\nWithin this structure, four possible outcomes exist:\n\n1. We fail to reject the null hypothesis and the null is true.\n2. We reject the null hypothesis and the null is false.\n3. We reject the null hypothesis, but the null is actually true.\n4. We fail to reject the null hypothesis, but the null is actually false.\n\nEx. Education has an effect on wage, but we incorrectly fail to reject \\(H_0\\).\nThis is an error. Defined as a Type II error."
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#possible-outcomes-4",
    "href": "lectures/05-Inference/050-compile.html#possible-outcomes-4",
    "title": "Inference",
    "section": "Possible outcomes",
    "text": "Possible outcomes\nWithin this structure, four possible outcomes exist:\n\n1. We fail to reject the null hypothesis and the null is true.\n2. We reject the null hypothesis and the null is false.\n3. We reject the null hypothesis, but the null is actually true.1\n4. We fail to reject the null hypothesis, but the null is actually false.2\nType I errorType II error"
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#hypothesis-tests-1",
    "href": "lectures/05-Inference/050-compile.html#hypothesis-tests-1",
    "title": "Inference",
    "section": "Hypothesis Tests",
    "text": "Hypothesis Tests\nGoal: Make a statement about \\(\\beta_1\\) using information on \\(\\hat{\\beta}_1\\).\n\n\\(\\hat{\\beta}_1\\) is random—it could be anything, even if \\(\\beta_1 = 0\\) is true.\n\nBut if \\(\\beta_1 = 0\\) is true, then \\(\\hat{\\beta}_1\\) is unlikely to take values far from zero.\nAs the standard error shrinks, we are even less likely to observe “extreme” values of \\(\\hat{\\beta}_1\\) (assuming \\(\\beta_1 = 0\\)).\n\n\n\nHypothesis testing takes extreme values of \\(\\hat{\\beta}_1\\) as evidence against the null hypothesis, but it will weight them by information about variance the estimated variance of \\(\\hat{\\beta}_1\\)."
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#hypothesis-tests-2",
    "href": "lectures/05-Inference/050-compile.html#hypothesis-tests-2",
    "title": "Inference",
    "section": "Hypothesis Tests",
    "text": "Hypothesis Tests\n\n\\(H_0\\): \\(\\beta_1 = 0\\)\n\n\n\\(H_1\\): \\(\\beta \\neq 0\\)\n\nTo conduct the test, we calculate a \\(t\\)-statistic1:\n\\[\nt = \\frac{\\hat{\\beta}_1 - \\beta_1^0}{\\mathop{\\hat{\\text{SE}}} \\left( \\hat{\\beta}_1 \\right)}\n\\]\nDistributed by a \\(t\\)-distribution with \\(n-2\\) degrees of freedom2.\n\\(\\beta_1^0\\) is the value of \\(\\beta_1\\) in our null hypothesis (e.g., \\(\\beta_1^0 = 0\\)).represents the number of independent values in a sample that are free to vary when estimating statistical parameters."
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#hypothesis-testing-1",
    "href": "lectures/05-Inference/050-compile.html#hypothesis-testing-1",
    "title": "Inference",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\nNormal distribution vs. \\(t\\) distribution\n\nA normal distribution has the same shape for any sample size.\nThe shape of the t distribution depends the degrees of freedom.\n\n\n\nDegrees of freedom = 5."
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#hypothesis-testing-2",
    "href": "lectures/05-Inference/050-compile.html#hypothesis-testing-2",
    "title": "Inference",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\nNormal distribution vs. \\(t\\) distribution\n\nA normal distribution has the same shape for any sample size.\nThe shape of the t distribution depends the degrees of freedom.\n\n\n\nDegrees of freedom = 50."
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#hypothesis-testing-3",
    "href": "lectures/05-Inference/050-compile.html#hypothesis-testing-3",
    "title": "Inference",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\nNormal distribution vs. \\(t\\) distribution\n\nA normal distribution has the same shape for any sample size.\nThe shape of the t distribution depends the degrees of freedom.\n\n\n\nDegrees of freedom = 500."
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#hypothesis-testing-4",
    "href": "lectures/05-Inference/050-compile.html#hypothesis-testing-4",
    "title": "Inference",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\nTwo sided t Tests\nTo conduct a t test, compare the \\(t\\) statistic to the appropriate critical value of the t distribution.\n\nTo find the critical value in a t table, we need the degrees of freedom and the significance level \\(\\alpha\\).\n\nReject (\\(\\text{H}_0\\)) at the \\(\\alpha \\cdot 100\\)-percent level if\n\\[\n\\left| t \\right| = \\left| \\dfrac{\\hat{\\mu} - \\mu_0}{\\mathop{\\text{SE}}(\\hat{\\mu})} \\right| &gt; t_\\text{crit}.\n\\]"
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#hypothesis-tests-3",
    "href": "lectures/05-Inference/050-compile.html#hypothesis-tests-3",
    "title": "Inference",
    "section": "Hypothesis Tests",
    "text": "Hypothesis Tests\nNext, we use the \\(\\color{#434C5E}{t}\\)-statistic to calculate a \\(\\color{#B48EAD}{p}\\)-value.\n\nDescribes the probability of seeing a \\(\\color{#434C5E}{t}\\)-statistic as extreme as the one we observe if the null hypothesis is actually true.\n\nBut…we still need some benchmark to compare our \\(\\color{#B48EAD}{p}\\)-value against."
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#hypothesis-tests-4",
    "href": "lectures/05-Inference/050-compile.html#hypothesis-tests-4",
    "title": "Inference",
    "section": "Hypothesis Tests",
    "text": "Hypothesis Tests\nWe worry mostly about false positives, so we conduct hypothesis tests based on the probability of making a Type I error1.\n\nHow? We select a significance level, \\(\\color{#434C5E}{\\alpha}\\), that specifies our tolerance for false positives (i.e., the probability of Type I error we choose to live with).\n\n\n\n\n\n\n\n\nWe reject the null hypothesis, but the null is actually true."
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#hypothesis-tests-5",
    "href": "lectures/05-Inference/050-compile.html#hypothesis-tests-5",
    "title": "Inference",
    "section": "Hypothesis Tests",
    "text": "Hypothesis Tests\nWe then compare \\(\\color{#434C5E}{\\alpha}\\) to the \\(\\color{#B48EAD}{p}\\)-value of our test.\n\nIf the \\(\\color{#B48EAD}{p}\\)-value is less than \\(\\color{#434C5E}{\\alpha}\\), then we reject the null hypothesis at the \\(\\color{#434C5E}{\\alpha}\\cdot100\\) percent level.\nIf the \\(\\color{#B48EAD}{p}\\)-value is greater than \\(\\color{#434C5E}{\\alpha}\\), then we fail to reject the null hypothesis at the \\(\\color{#434C5E}{\\alpha}\\cdot100\\) percent level.1\n\nNote: Fail to reject \\(\\neq\\) accept."
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#hypothesis-tests-6",
    "href": "lectures/05-Inference/050-compile.html#hypothesis-tests-6",
    "title": "Inference",
    "section": "Hypothesis Tests",
    "text": "Hypothesis Tests\n\\(\\color{#B48EAD}{p}\\)-values are difficult to calculate by hand.\nAlternative: Compare \\(\\color{#434C5E}{t}\\)-statistic to critical values from the \\({\\color{#434C5E} t}\\)-distribution."
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#hypothesis-tests-7",
    "href": "lectures/05-Inference/050-compile.html#hypothesis-tests-7",
    "title": "Inference",
    "section": "Hypothesis Tests",
    "text": "Hypothesis Tests\nNotation: \\(t_{1-\\alpha/2, n-2}\\) or \\(t_\\text{crit}\\).\n\nFind in a \\(t\\)-table using \\(\\color{#434C5E}{\\alpha}\\) and \\(n-2\\) degrees of freedom.\n\nCompare the the critical value to your \\(t\\)-statistic:\n\nIf \\(|t| &gt; |t_{1-\\alpha/2, n-2}|\\), then reject the null.\nIf \\(|t| &lt; |t_{1-\\alpha/2, n-2}|\\), then fail to reject the null."
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#two-sided-tests",
    "href": "lectures/05-Inference/050-compile.html#two-sided-tests",
    "title": "Inference",
    "section": "Two-sided tests",
    "text": "Two-sided tests\nBased on a critical value of \\(t_{1-\\alpha/2, n-2} = t_{0.975, 100} = 1.98\\) we can identify a rejection region on the \\(\\color{#434C5E}{t}\\)-distribution.\n\n\nIf our \\(\\color{#434C5E}{t}\\)-statistic is in the rejection region, then we reject the null hypothesis at the 5 percent level."
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#one-sided-tests",
    "href": "lectures/05-Inference/050-compile.html#one-sided-tests",
    "title": "Inference",
    "section": "One-sided tests",
    "text": "One-sided tests\nWe might be confident in a parameter being non-negative/non-positive.\nOne-sided tests assume that the parameter of interest is either greater than/less than \\(H_0\\).\n\nOption 1 \\(H_0\\): \\(\\beta_1 = 0\\) vs. \\(H_1\\): \\(\\beta_1 &gt; 0\\)\nOption 2 \\(H_0\\): \\(\\beta_1 = 0\\) vs. \\(H_1\\): \\(\\beta_1 &lt; 0\\)\n\n\nIf this assumption is reasonable, then our rejection region changes.\n\nSame \\(\\alpha\\)."
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#one-sided-tests-1",
    "href": "lectures/05-Inference/050-compile.html#one-sided-tests-1",
    "title": "Inference",
    "section": "One-sided tests",
    "text": "One-sided tests\nLeft-tailed: Based on a critical value of \\(t_{1-\\alpha, n-2} = t_{0.95, 100} = 1.66\\), we can identify a rejection region on the \\(t\\)-distribution.\n\n\nIf our \\(t\\) statistic is in the rejection region, then we reject the null hypothesis at the 5 percent level."
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#one-sided-tests-2",
    "href": "lectures/05-Inference/050-compile.html#one-sided-tests-2",
    "title": "Inference",
    "section": "One-sided tests",
    "text": "One-sided tests\nRight-tailed: Based on a critical value of \\(t_{1-\\alpha, n-2} = t_{0.95, 100} = 1.66\\), we can identify a rejection region on the \\(t\\)-distribution.\n\n\nIf our \\(t\\) statistic is in the rejection region, then we reject the null hypothesis at the 5 percent level."
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#confidence-intervals-1",
    "href": "lectures/05-Inference/050-compile.html#confidence-intervals-1",
    "title": "Inference",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nUntil now, we have considered point estimates of population parameters.\n\nSometimes a range of values is more interesting/honest.\n\n\nWe can construct \\((1-\\alpha)\\cdot100\\)-percent level confidence intervals for \\(\\beta_1\\)\n\\[\n\\hat{\\beta}_1 \\pm t_{1-\\alpha/2, n-2} \\, \\mathop{\\hat{\\text{SE}}} \\left( \\hat{\\beta}_1 \\right)\n\\]\n\n\n\\(t_{1-\\alpha/2,n-2}\\) denotes the \\(1-\\alpha/2\\) quantile of a \\(t\\) distribution with \\(n-2\\) degrees of freedom."
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#confidence-intervals-2",
    "href": "lectures/05-Inference/050-compile.html#confidence-intervals-2",
    "title": "Inference",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nQ: Where does the confidence interval formula come from?\n\nA: Formula is a result from the rejection condition of a two-sided test.\nReject \\(H_0\\) if\n\\[\n|t| &gt; t_\\text{crit}\n\\]\n\n\nThe test condition implies that we:\nFail to reject \\(H_0\\) if\n\\[\n|t| \\leq t_\\text{crit}\n\\]\nor, \\[\n-t_\\text{crit} \\leq t \\leq t_\\text{crit}\n\\]"
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#confidence-intervals-3",
    "href": "lectures/05-Inference/050-compile.html#confidence-intervals-3",
    "title": "Inference",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nReplacing \\(t\\) with its formula gives:\nFail to reject \\(H_0\\) if\n\\[-t_\\text{crit} \\leq \\frac{\\hat{\\beta}_1 - \\beta_1^0}{\\mathop{\\hat{\\text{SE}}} \\left( \\hat{\\beta}_1 \\right)} \\leq t_\\text{crit}\n\\]\n\nStandard errors are always positive, so the inequalities do not flip when we multiply by \\(\\mathop{\\hat{\\text{SE}}} \\left( \\hat{\\beta}_1 \\right)\\):\nFail to reject \\(H_0\\) if \\[\n-t_\\text{crit} \\mathop{\\hat{\\text{SE}}} \\left( \\hat{\\beta}_1 \\right) \\leq \\hat{\\beta}_1 - \\beta_1^0\\leq t_\\text{crit} \\mathop{\\hat{\\text{SE}}} \\left( \\hat{\\beta}_1 \\right)\n\\]"
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#confidence-intervals-4",
    "href": "lectures/05-Inference/050-compile.html#confidence-intervals-4",
    "title": "Inference",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nSubtracting \\(\\hat{\\beta}_1\\) yields\nFail to reject \\(H_0\\) if \\[\n-\\hat{\\beta}_1 -t_\\text{crit} \\mathop{\\hat{\\text{SE}}} \\left( \\hat{\\beta}_1 \\right) \\leq - \\beta_1^0 \\leq - \\hat{\\beta}_1 + t_\\text{crit} \\mathop{\\hat{\\text{SE}}} \\left( \\hat{\\beta}_1 \\right)\n\\]\n\nMultiplying by -1 and rearranging gives\nFail to reject \\(H_0\\) if\n\\[\n\\hat{\\beta}_1 - t_\\text{crit} \\mathop{\\hat{\\text{SE}}} \\left( \\hat{\\beta}_1 \\right) \\leq \\beta_1^0 \\leq \\hat{\\beta}_1 + t_\\text{crit} \\mathop{\\hat{\\text{SE}}} \\left( \\hat{\\beta}_1 \\right)\n\\]"
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#confidence-intervals-5",
    "href": "lectures/05-Inference/050-compile.html#confidence-intervals-5",
    "title": "Inference",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nReplacing \\(\\beta_1^0\\) with \\(\\beta_1\\) and dropping the test condition yields the interval:\n\\[\n\\hat{\\beta}_1 - t_\\text{crit} \\mathop{\\hat{\\text{SE}}} \\left( \\hat{\\beta}_1 \\right) \\leq \\beta_1 \\leq \\hat{\\beta}_1 + t_\\text{crit} \\mathop{\\hat{\\text{SE}}} \\left( \\hat{\\beta}_1 \\right)\n\\]\nwhich is equivalent to\n\\[\n\\hat{\\beta}_1 \\pm t_\\text{crit} \\, \\mathop{\\hat{\\text{SE}}} \\left( \\hat{\\beta}_1 \\right)\n\\]"
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#confidence-intervals-6",
    "href": "lectures/05-Inference/050-compile.html#confidence-intervals-6",
    "title": "Inference",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nMain insight:\n\nIf a 95 percent confidence interval contains zero, then we fail to reject the null hypothesis at the 5 percent level.\nIf a 95 percent confidence interval does not contain zero, then we reject the null hypothesis at the 5 percent level.\n\n \nGenerally, a \\((1- \\alpha) \\cdot 100\\) percent confidence interval embeds a two-sided test at the \\(\\alpha \\cdot 100\\) level."
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#confidence-intervals-ex.",
    "href": "lectures/05-Inference/050-compile.html#confidence-intervals-ex.",
    "title": "Inference",
    "section": "Confidence intervals Ex.",
    "text": "Confidence intervals Ex.\n\n\n\n\n\n\n\n\n\n\n\n\n\n95% confidence interval for \\(\\beta_1\\) is:\n\\[\n0.567 \\pm 1.98 \\times 0.0793 = \\left[ 0.410,\\, 0.724 \\right]\n\\]"
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#confidence-intervals-7",
    "href": "lectures/05-Inference/050-compile.html#confidence-intervals-7",
    "title": "Inference",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nWe have a confidence interval for \\(\\beta_1\\), i.e., \\(\\left[ 0.410,\\, 0.724 \\right]\\)\n\nWhat does it mean?\n\n\nInformally: The confidence interval gives us a region (interval) in which we can place some trust (confidence) for containing the parameter.\n\n\nMore formally: If we repeatedly sample from our population and construct confidence intervals for each of these samples, then \\((1-\\alpha) \\cdot100\\) percent of our intervals (e.g., 95%) will contain the population parameter somewhere in the interval."
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#confidence-intervals-8",
    "href": "lectures/05-Inference/050-compile.html#confidence-intervals-8",
    "title": "Inference",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nGoing back to our simulation…\n\nWe drew 10,000 samples (each of size \\(n = 30\\)) from our population and estimated our regression model for each sample:\n\\[\nY_i = \\hat{\\beta}_1 + \\hat{\\beta}_1 X_i + \\hat{u}_i\n\\]\n\n(repeated 10,000 times)\n\n\n\nThe true parameter values are \\(\\beta_0 = 0\\) and \\(\\beta_1 = 0.5\\)\n\n\nLet’s estimate 95% confidence intervals for each of these intervals…"
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#confidence-intervals-9",
    "href": "lectures/05-Inference/050-compile.html#confidence-intervals-9",
    "title": "Inference",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nFrom our previous simulation, 97.7% of 95% confidence intervals contain the true parameter value of \\(\\beta_1\\)."
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#ex.-association-of-police-with-crime",
    "href": "lectures/05-Inference/050-compile.html#ex.-association-of-police-with-crime",
    "title": "Inference",
    "section": "Ex. Association of police with crime",
    "text": "Ex. Association of police with crime\nYou can instruct tidy to return a 95 percent confidence interval for the association of campus police with campus crime:"
  },
  {
    "objectID": "lectures/05-Inference/050-compile.html#ex.-association-of-police-with-crime-1",
    "href": "lectures/05-Inference/050-compile.html#ex.-association-of-police-with-crime-1",
    "title": "Inference",
    "section": "Ex. Association of police with crime",
    "text": "Ex. Association of police with crime\n\nFour confidence intervals for the same coefficient."
  },
  {
    "objectID": "lectures/05-Inference/052-inference.html",
    "href": "lectures/05-Inference/052-inference.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Our current workflow consists of:\n\n\nGet data (points with \\(X\\) and \\(Y\\) values)\nRegress \\(Y\\) on \\(X\\) (aka \\(Y = \\beta_{0} + \\beta_{1}X\\))\nPlot the point estimates \\((\\hat{\\beta}_{0},\\hat{\\beta}_{1})\\) and report them\n\n\n. . .\nBut when do we learn something? We are still missing a step.\n\nFor \\(\\hat{\\beta}_{1}\\), can we rule out a previously hypothesized values?\nHow condifent should we be in the precision of our estimates?\n\nWe need to be careful about our sample being atypical \\(\\Rightarrow\\) uncertainty"
  },
  {
    "objectID": "lectures/05-Inference/052-inference.html#inference",
    "href": "lectures/05-Inference/052-inference.html#inference",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Our current workflow consists of:\n\n\nGet data (points with \\(X\\) and \\(Y\\) values)\nRegress \\(Y\\) on \\(X\\) (aka \\(Y = \\beta_{0} + \\beta_{1}X\\))\nPlot the point estimates \\((\\hat{\\beta}_{0},\\hat{\\beta}_{1})\\) and report them\n\n\n. . .\nBut when do we learn something? We are still missing a step.\n\nFor \\(\\hat{\\beta}_{1}\\), can we rule out a previously hypothesized values?\nHow condifent should we be in the precision of our estimates?\n\nWe need to be careful about our sample being atypical \\(\\Rightarrow\\) uncertainty"
  },
  {
    "objectID": "lectures/05-Inference/052-inference.html#section",
    "href": "lectures/05-Inference/052-inference.html#section",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "However, there is a problem.\n. . .\n \nRecall the variance of the point estimate \\(\\hat{\\beta_1}\\) \\[\n\\mathop{\\text{Var}}(\\hat{\\beta}_1) = \\frac{\\sigma^2}{\\sum_{i=1}^n (X_i - \\bar{X})^2}\n\\]\nThe problem is that \\(\\color{#BF616A}{\\sigma^2}\\) is unobserved. So what do we do? Estimate it."
  },
  {
    "objectID": "lectures/05-Inference/052-inference.html#estimating-error-variance",
    "href": "lectures/05-Inference/052-inference.html#estimating-error-variance",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Estimating error variance",
    "text": "Estimating error variance\nWe can estimate the variance of \\(u_i\\) (\\(\\color{#BF616A}{\\sigma^2}\\)) using the sum of squared residuals (RSS):\n\\[\ns^2_u = \\dfrac{\\sum_i \\hat{u}_i^2}{n - k}\n\\]\nwhere \\(n\\) is the number of observations and \\(k\\) is the number of regression parameters. (In a simple linear regression, \\(k=2\\))\n. . .\nIf the assumptions from Gauss-Markov hold, then \\(s^2_u\\) is an unbiased estimator of \\(\\sigma^2\\).\n. . .\nIn essence, we are learning from our prediction errors"
  },
  {
    "objectID": "lectures/05-Inference/052-inference.html#ols-variance",
    "href": "lectures/05-Inference/052-inference.html#ols-variance",
    "title": "EC 320 - Intro. Econometrics",
    "section": "OLS Variance",
    "text": "OLS Variance\nWith \\(s^2_u = \\dfrac{\\sum_i \\hat{u}_i^2}{n - k}\\), we can calculate the estimated variance of \\(\\hat{\\beta}_1\\)\n\\[\n\\mathop{\\text{Var}}(\\hat{\\beta}_1) = \\frac{s^2_u}{\\sum_{i=1}^n (X_i - \\bar{X})^2}\n\\]\n. . .\nTaking the square root, we get the standard error of the OLS estimator:\n\\[\n\\mathop{\\widehat{\\text{SE}}} \\left( \\hat{\\beta}_1 \\right) = \\sqrt{ \\frac{s^2_u}{\\sum_{i=1}^n (X_i - \\bar{X})^2} }\n\\]\nThe standard error is the standard deviation of the sampling distribution."
  },
  {
    "objectID": "lectures/05-Inference/052-inference.html#inference-1",
    "href": "lectures/05-Inference/052-inference.html#inference-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Inference",
    "text": "Inference\nAfter deriving the distribution of \\(\\hat{\\beta}_1\\)1, we have two (related) options for formal statistical inference (learning) about our unknown parameter \\(\\beta_1\\):\n\n\nHypothesis testing: Determine whether there is statistically significant evidence to reject a hypothesized value or range of values.\nConfidence intervals: Use the estimate and its standard error to create an interval that will generally2 contain the true parameter."
  },
  {
    "objectID": "lectures/05-Inference/052-inference.html#footnotes",
    "href": "lectures/05-Inference/052-inference.html#footnotes",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHint: It’s normal with mean \\(\\beta_1\\) and variance \\(\\frac{\\sigma^2}{\\sum_{i=1}^n (X_i - \\bar{X})^2}\\).↩︎\nE.g., similarly constructed 95% confidence intervals will contain the true parameter 95% of the time.↩︎"
  },
  {
    "objectID": "lectures/05-Inference/054-conf-intervals.html",
    "href": "lectures/05-Inference/054-conf-intervals.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Until now, we have considered point estimates of population parameters.\n\nSometimes a range of values is more interesting/honest.\n\n. . .\nWe can construct \\((1-\\alpha)\\cdot100\\)-percent level confidence intervals for \\(\\beta_1\\)\n\\[\n\\hat{\\beta}_1 \\pm t_{1-\\alpha/2, n-2} \\, \\mathop{\\hat{\\text{SE}}} \\left( \\hat{\\beta}_1 \\right)\n\\]\n. . .\n\\(t_{1-\\alpha/2,n-2}\\) denotes the \\(1-\\alpha/2\\) quantile of a \\(t\\) distribution with \\(n-2\\) degrees of freedom."
  },
  {
    "objectID": "lectures/05-Inference/054-conf-intervals.html#confidence-intervals",
    "href": "lectures/05-Inference/054-conf-intervals.html#confidence-intervals",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Until now, we have considered point estimates of population parameters.\n\nSometimes a range of values is more interesting/honest.\n\n. . .\nWe can construct \\((1-\\alpha)\\cdot100\\)-percent level confidence intervals for \\(\\beta_1\\)\n\\[\n\\hat{\\beta}_1 \\pm t_{1-\\alpha/2, n-2} \\, \\mathop{\\hat{\\text{SE}}} \\left( \\hat{\\beta}_1 \\right)\n\\]\n. . .\n\\(t_{1-\\alpha/2,n-2}\\) denotes the \\(1-\\alpha/2\\) quantile of a \\(t\\) distribution with \\(n-2\\) degrees of freedom."
  },
  {
    "objectID": "lectures/05-Inference/054-conf-intervals.html#confidence-intervals-1",
    "href": "lectures/05-Inference/054-conf-intervals.html#confidence-intervals-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nQ: Where does the confidence interval formula come from?\n. . .\nA: Formula is a result from the rejection condition of a two-sided test.\nReject \\(H_0\\) if\n\\[\n|t| &gt; t_\\text{crit}\n\\]\n. . .\nThe test condition implies that we:\nFail to reject \\(H_0\\) if\n\\[\n|t| \\leq t_\\text{crit}\n\\]\nor, \\[\n-t_\\text{crit} \\leq t \\leq t_\\text{crit}\n\\]"
  },
  {
    "objectID": "lectures/05-Inference/054-conf-intervals.html#confidence-intervals-2",
    "href": "lectures/05-Inference/054-conf-intervals.html#confidence-intervals-2",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nReplacing \\(t\\) with its formula gives:\nFail to reject \\(H_0\\) if\n\\[-t_\\text{crit} \\leq \\frac{\\hat{\\beta}_1 - \\beta_1^0}{\\mathop{\\hat{\\text{SE}}} \\left( \\hat{\\beta}_1 \\right)} \\leq t_\\text{crit}\n\\]\n. . .\nStandard errors are always positive, so the inequalities do not flip when we multiply by \\(\\mathop{\\hat{\\text{SE}}} \\left( \\hat{\\beta}_1 \\right)\\):\nFail to reject \\(H_0\\) if \\[\n-t_\\text{crit} \\mathop{\\hat{\\text{SE}}} \\left( \\hat{\\beta}_1 \\right) \\leq \\hat{\\beta}_1 - \\beta_1^0\\leq t_\\text{crit} \\mathop{\\hat{\\text{SE}}} \\left( \\hat{\\beta}_1 \\right)\n\\]"
  },
  {
    "objectID": "lectures/05-Inference/054-conf-intervals.html#confidence-intervals-3",
    "href": "lectures/05-Inference/054-conf-intervals.html#confidence-intervals-3",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nSubtracting \\(\\hat{\\beta}_1\\) yields\nFail to reject \\(H_0\\) if \\[\n-\\hat{\\beta}_1 -t_\\text{crit} \\mathop{\\hat{\\text{SE}}} \\left( \\hat{\\beta}_1 \\right) \\leq - \\beta_1^0 \\leq - \\hat{\\beta}_1 + t_\\text{crit} \\mathop{\\hat{\\text{SE}}} \\left( \\hat{\\beta}_1 \\right)\n\\]\n. . .\nMultiplying by -1 and rearranging gives\nFail to reject \\(H_0\\) if\n\\[\n\\hat{\\beta}_1 - t_\\text{crit} \\mathop{\\hat{\\text{SE}}} \\left( \\hat{\\beta}_1 \\right) \\leq \\beta_1^0 \\leq \\hat{\\beta}_1 + t_\\text{crit} \\mathop{\\hat{\\text{SE}}} \\left( \\hat{\\beta}_1 \\right)\n\\]"
  },
  {
    "objectID": "lectures/05-Inference/054-conf-intervals.html#confidence-intervals-4",
    "href": "lectures/05-Inference/054-conf-intervals.html#confidence-intervals-4",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nReplacing \\(\\beta_1^0\\) with \\(\\beta_1\\) and dropping the test condition yields the interval:\n\\[\n\\hat{\\beta}_1 - t_\\text{crit} \\mathop{\\hat{\\text{SE}}} \\left( \\hat{\\beta}_1 \\right) \\leq \\beta_1 \\leq \\hat{\\beta}_1 + t_\\text{crit} \\mathop{\\hat{\\text{SE}}} \\left( \\hat{\\beta}_1 \\right)\n\\]\nwhich is equivalent to\n\\[\n\\hat{\\beta}_1 \\pm t_\\text{crit} \\, \\mathop{\\hat{\\text{SE}}} \\left( \\hat{\\beta}_1 \\right)\n\\]"
  },
  {
    "objectID": "lectures/05-Inference/054-conf-intervals.html#confidence-intervals-5",
    "href": "lectures/05-Inference/054-conf-intervals.html#confidence-intervals-5",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nMain insight:\n\nIf a 95 percent confidence interval contains zero, then we fail to reject the null hypothesis at the 5 percent level.\nIf a 95 percent confidence interval does not contain zero, then we reject the null hypothesis at the 5 percent level.\n\n \nGenerally, a \\((1- \\alpha) \\cdot 100\\) percent confidence interval embeds a two-sided test at the \\(\\alpha \\cdot 100\\) level."
  },
  {
    "objectID": "lectures/05-Inference/054-conf-intervals.html#confidence-intervals-ex.",
    "href": "lectures/05-Inference/054-conf-intervals.html#confidence-intervals-ex.",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Confidence intervals Ex.",
    "text": "Confidence intervals Ex.\n\n\n\n\n\n. . .\n\n\n\n\n\n. . .\n95% confidence interval for \\(\\beta_1\\) is:\n\\[\n0.567 \\pm 1.98 \\times 0.0793 = \\left[ 0.410,\\, 0.724 \\right]\n\\]"
  },
  {
    "objectID": "lectures/05-Inference/054-conf-intervals.html#confidence-intervals-6",
    "href": "lectures/05-Inference/054-conf-intervals.html#confidence-intervals-6",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nWe have a confidence interval for \\(\\beta_1\\), i.e., \\(\\left[ 0.410,\\, 0.724 \\right]\\)\n\nWhat does it mean?\n\n. . .\nInformally: The confidence interval gives us a region (interval) in which we can place some trust (confidence) for containing the parameter.\n. . .\nMore formally: If we repeatedly sample from our population and construct confidence intervals for each of these samples, then \\((1-\\alpha) \\cdot100\\) percent of our intervals (e.g., 95%) will contain the population parameter somewhere in the interval."
  },
  {
    "objectID": "lectures/05-Inference/054-conf-intervals.html#confidence-intervals-7",
    "href": "lectures/05-Inference/054-conf-intervals.html#confidence-intervals-7",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nGoing back to our simulation…\n. . .\nWe drew 10,000 samples (each of size \\(n = 30\\)) from our population and estimated our regression model for each sample:\n\\[\nY_i = \\hat{\\beta}_1 + \\hat{\\beta}_1 X_i + \\hat{u}_i\n\\]\n\n(repeated 10,000 times)\n\n. . .\nThe true parameter values are \\(\\beta_0 = 0\\) and \\(\\beta_1 = 0.5\\)\n. . .\nLet’s estimate 95% confidence intervals for each of these intervals…"
  },
  {
    "objectID": "lectures/05-Inference/054-conf-intervals.html#confidence-intervals-8",
    "href": "lectures/05-Inference/054-conf-intervals.html#confidence-intervals-8",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nFrom our previous simulation, 97.7% of 95% confidence intervals contain the true parameter value of \\(\\beta_1\\)."
  },
  {
    "objectID": "lectures/05-Inference/054-conf-intervals.html#ex.-association-of-police-with-crime",
    "href": "lectures/05-Inference/054-conf-intervals.html#ex.-association-of-police-with-crime",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Ex. Association of police with crime",
    "text": "Ex. Association of police with crime\nYou can instruct tidy to return a 95 percent confidence interval for the association of campus police with campus crime:"
  },
  {
    "objectID": "lectures/05-Inference/054-conf-intervals.html#ex.-association-of-police-with-crime-1",
    "href": "lectures/05-Inference/054-conf-intervals.html#ex.-association-of-police-with-crime-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Ex. Association of police with crime",
    "text": "Ex. Association of police with crime\n\n\n\n\n\nFour confidence intervals for the same coefficient."
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/061-prologue.html",
    "href": "lectures/06-Multiple-Var-Reg/061-prologue.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "The Regression Model\nWe can estimate the effect of \\(X\\) on \\(Y\\) by estimating a regression model:\n\\[Y_i = \\beta_0 + \\beta_1 X_i + u_i\\]\n\n\\(Y_i\\) is the outcome variable.\n\\(X_i\\) is the treatment variable (continuous).\n\\(\\beta_0\\) is the intercept parameter. \\(\\mathop{\\mathbb{E}}\\left[ {Y_i | X_i=0} \\right] = \\beta_0\\)\n\\(\\beta_1\\) is the slope parameter, which under the correct causal setting represents marginal change in \\(X_i\\)’s effect on \\(Y_i\\). \\(\\frac{\\partial Y_i}{\\partial X_i} = \\beta_1\\)\n\\(u_i\\) is an error term including all other (omitted) factors affecting \\(Y_i\\)."
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/061-prologue.html#quick-recap",
    "href": "lectures/06-Multiple-Var-Reg/061-prologue.html#quick-recap",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "The Regression Model\nWe can estimate the effect of \\(X\\) on \\(Y\\) by estimating a regression model:\n\\[Y_i = \\beta_0 + \\beta_1 X_i + u_i\\]\n\n\\(Y_i\\) is the outcome variable.\n\\(X_i\\) is the treatment variable (continuous).\n\\(\\beta_0\\) is the intercept parameter. \\(\\mathop{\\mathbb{E}}\\left[ {Y_i | X_i=0} \\right] = \\beta_0\\)\n\\(\\beta_1\\) is the slope parameter, which under the correct causal setting represents marginal change in \\(X_i\\)’s effect on \\(Y_i\\). \\(\\frac{\\partial Y_i}{\\partial X_i} = \\beta_1\\)\n\\(u_i\\) is an error term including all other (omitted) factors affecting \\(Y_i\\)."
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/061-prologue.html#the-error-term",
    "href": "lectures/06-Multiple-Var-Reg/061-prologue.html#the-error-term",
    "title": "EC 320 - Intro. Econometrics",
    "section": "The error term",
    "text": "The error term\n\\(u_i\\) is quite special\n\nConsider the data generating process of variable \\(Y_i\\),\n\n\\(u_i\\) captures all unobserved relationships that explain variation in \\(Y_i\\).\n\n\nSome error will exist in all models, our aim is to minimize error under a set of constraints. This error is the price we are willing to accept for simplified model"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/061-prologue.html#the-error-term-1",
    "href": "lectures/06-Multiple-Var-Reg/061-prologue.html#the-error-term-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "The error term",
    "text": "The error term\nFive items contribute to the existence of the disturbance term:\n1. Omission of explanatory variables\n2. Aggregation of Variables\n3. Model misspecificiation\n4. Functional misspecificiation\n5. Measurement error"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/061-prologue.html#running-regressions",
    "href": "lectures/06-Multiple-Var-Reg/061-prologue.html#running-regressions",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Running regressions",
    "text": "Running regressions\nUsing an estimator with data on \\(X_i\\) and \\(Y_i\\), we can estimate a fitted regression line:\n\\[\n\\hat{Y}_{i} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1} X_i\n\\]\n\n\\(\\hat{Y}_{i}\\) is the fitted value of \\(Y_i\\).\n\\(\\hat{\\beta}_{0}\\) is the estimated intercept.\n\\(\\hat{\\beta}_{1}\\) is the estimated slope.\n\n. . .\nThis procedure produces misses, known as residuals, \\(Y_{i} - \\hat{Y}_{i}\\)"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/061-prologue.html#gauss-markov-theorem",
    "href": "lectures/06-Multiple-Var-Reg/061-prologue.html#gauss-markov-theorem",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Gauss-Markov Theorem",
    "text": "Gauss-Markov Theorem\n\nOLS is the Best Linear Unbiased Estimator (BLUE) when the following assumptions hold:\n\nA1. Linearity: The population relationship is linear in parameters with an additive error term.\nA2. Sample Variation: There is variation in \\(X\\).\nA3. Exogeniety: The \\(X\\) variable is exogenous\nA4. Homoskedasticity: The error term has the same variance for each value of the independent variable\nA5. Non-autocorrelation: The values of error terms have independent distributions"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/061-prologue.html#section",
    "href": "lectures/06-Multiple-Var-Reg/061-prologue.html#section",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Consider the following example."
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/063-ovb.html",
    "href": "lectures/06-Multiple-Var-Reg/063-ovb.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Bias that occurs in statistical models when a relevant variable is not included in the model.\n\n. . .\n\nConsequence: Leads to the incorrect estimation of the relationships between variables, which may affect the reliability of the model’s predictions and inferences.\n\n. . .\nSolution: “Control” for the omitted variable(s)."
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/063-ovb.html#omitted-variable-bias",
    "href": "lectures/06-Multiple-Var-Reg/063-ovb.html#omitted-variable-bias",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Bias that occurs in statistical models when a relevant variable is not included in the model.\n\n. . .\n\nConsequence: Leads to the incorrect estimation of the relationships between variables, which may affect the reliability of the model’s predictions and inferences.\n\n. . .\nSolution: “Control” for the omitted variable(s)."
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/063-ovb.html#section",
    "href": "lectures/06-Multiple-Var-Reg/063-ovb.html#section",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Class funding (U) confounds our estimates of smaller class sizes (X) on test scores (Y).  \n\n\n\n\n\nAny unobserved variable that connects a backdoor path between class size (X) and test scores (Y) will bias our point estimate of \\(\\beta_1\\)."
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/063-ovb.html#section-1",
    "href": "lectures/06-Multiple-Var-Reg/063-ovb.html#section-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Class funding (U) confounds our estimates of smaller class sizes (X) on test scores (Y). Including data on school funding (U) in a multiple linear regression allows us to close this backdoor path.\n\n\n\n\n\n. . .\nWith all backdoor paths closed, point estimates of \\(\\beta_1\\) will no longer be biased and will return the population parameter of interest\n\n\nIn a little more detail, we can derive the bias mathematically."
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/063-ovb.html#omitted-variable-bias-1",
    "href": "lectures/06-Multiple-Var-Reg/063-ovb.html#omitted-variable-bias-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Omitted Variable Bias",
    "text": "Omitted Variable Bias\nImagine we have a population model of the form:\n\\[\nY_i = \\beta_0 + \\beta_1 X_i + \\beta_2 Z_i + u_i\n\\]\nwhere \\(Z_i\\) is a relevant variable that is omitted from the model.\nand suppose we estimate the following model:\n\\[\nY_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i + v_i\n\\]\nwhere \\(v_i\\) is the new error term that absorbs the effect of \\(Z_i\\)"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/063-ovb.html#omitted-variable-bias-2",
    "href": "lectures/06-Multiple-Var-Reg/063-ovb.html#omitted-variable-bias-2",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Omitted Variable Bias",
    "text": "Omitted Variable Bias\nTo derive the bias of \\(\\hat{\\beta}_1\\), we need to understand the relationship between \\(Z_i\\) and \\(X_i\\). Assume that:\n\\[\nZ_i = \\gamma_0 + \\gamma_1 X_i + \\varepsilon_i\n\\]\nwhere \\(\\varepsilon_i\\) is the part of \\(Z_i\\) that is uncorrelated with \\(X_i\\)\n. . .\n\nIf we substitute \\(Z_i\\) into the population model, we get:\n\\[\n\\begin{align*}\nY_i &= \\beta_0 + \\beta_1 X_i + \\beta_2 \\left( \\gamma_0 + \\gamma_1 X_i + \\varepsilon_i \\right) + u_i \\\\\n    &= \\beta_0 + \\beta_2 \\gamma_0 + \\left( \\beta_1 + \\beta_2 \\gamma_1 \\right) X_i + \\beta_2 \\varepsilon_i + u_i\n\\end{align*}\n\\]"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/063-ovb.html#omitted-variable-bias-3",
    "href": "lectures/06-Multiple-Var-Reg/063-ovb.html#omitted-variable-bias-3",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Omitted Variable Bias",
    "text": "Omitted Variable Bias\nWe can rewrite this expression:\n\\[\n\\begin{align*}\nY_i &= \\beta_0 + \\beta_1 X_i + \\beta_2 \\left( \\gamma_0 + \\gamma_1 X_i + \\varepsilon_i \\right) + u_i \\\\\n    &= \\beta_0 + \\beta_2 \\gamma_0 + \\left( \\beta_1 + \\beta_2 \\gamma_1 \\right) X_i + \\beta_2 \\varepsilon_i + u_i\n\\end{align*}\n\\]\nas:\n\\[\nY_i = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 X_i + v_i\n\\]\nwhere:\n\n\\(\\widehat{\\beta}_0 = \\beta_0 + \\beta_2 \\gamma_0\\)\n\\(\\widehat{\\beta}_1 = \\beta_1 + \\beta_2 \\gamma_1\\)\n\\(v_i = \\beta_2 \\varepsilon_i + u_i\\)\n\nThus, we can see how \\(Z_i\\) will bias our estimate of \\(\\beta_1\\)"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/063-ovb.html#omitted-variable-bias-4",
    "href": "lectures/06-Multiple-Var-Reg/063-ovb.html#omitted-variable-bias-4",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Omitted Variable Bias",
    "text": "Omitted Variable Bias\nRecall that we define the bias of an estimator as:\n\\[\n\\mathop{\\text{Bias}}_\\theta \\left( W \\right) = \\mathop{\\boldsymbol{E}}\\left[ W \\right] - \\theta\n\\]\n. . .\nThe bias of the estimator \\(\\hat{\\beta}_1\\) is given by:\n\\[\n\\begin{align*}\n\\mathop{\\text{Bias}}_{\\beta_1} \\left( \\hat{\\beta}_1 \\right) &= \\mathop{\\boldsymbol{E}}\\left[ \\hat{\\beta}_1 \\right] - \\beta_1 \\\\\n&= \\mathop{\\boldsymbol{E}}\\left[ \\beta_1 + \\beta_2 \\gamma_1 \\right] - \\beta_1 \\\\\n&= \\beta_2 \\gamma_1\n\\end{align*}\n\\]"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/063-ovb.html#omitted-variable-bias-5",
    "href": "lectures/06-Multiple-Var-Reg/063-ovb.html#omitted-variable-bias-5",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Omitted Variable Bias",
    "text": "Omitted Variable Bias\nFinally, we can write the bias of \\(\\hat{\\beta}_1\\) in terms of the correlation between \\(X_i\\) and \\(Z_i\\):\n\\[\n\\gamma_1 = \\frac{\\text{Cov}\\left( X_i, Z_i \\right)}{\\text{Var}\\left( X_i \\right)}\n\\]\n. . .\nTherefore, we can write the bias of \\(\\hat{\\beta}_1\\) as:\n\\[\n\\mathop{\\text{Bias}}_{\\beta_1} \\left( \\hat{\\beta}_1 \\right) = \\beta_2 \\frac{\\text{Cov}\\left( X_i, Z_i \\right)}{\\text{Var}\\left( X_i \\right)}\n\\]"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/063-ovb.html#signing-the-bias",
    "href": "lectures/06-Multiple-Var-Reg/063-ovb.html#signing-the-bias",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Signing the Bias",
    "text": "Signing the Bias\nSometimes we’re stuck with omitted variable bias.\n\\[\n\\mathop{\\boldsymbol{E}} \\left[ \\hat{\\beta}_1 \\right] = \\beta_1 + \\beta_2 \\dfrac{ \\mathop{\\text{Cov}} \\left( X_i,\\, Z_i \\right)}{\\mathop{\\text{Var}} \\left( X_i \\right)}\n\\]\nWhen this happens, we can often at least know the direction of the bias."
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/063-ovb.html#signing-the-bias-1",
    "href": "lectures/06-Multiple-Var-Reg/063-ovb.html#signing-the-bias-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Signing the Bias",
    "text": "Signing the Bias\nBegin with\n\\[\n\\mathop{\\boldsymbol{E}} \\left[ \\hat{\\beta}_1 \\right] = \\beta_1 + \\beta_2 \\dfrac{ \\mathop{\\text{Cov}} \\left( X_i,\\, Z_i \\right)}{\\mathop{\\text{Var}} \\left( X_i \\right)}\n\\]\nWe know \\(\\color{#8FBCBB}{\\mathop{\\text{Var}} \\left( X_i \\right) &gt; 0}\\). Suppose \\(\\color{#81A1C1}{\\beta_2 &gt; 0}\\) and \\(\\color{#EBCB8B}{\\mathop{\\text{Cov}} \\left( X_i,\\,Z_i \\right) &gt; 0}\\). Then\n. . .\n\\[\n\\begin{align}\n\\mathop{\\boldsymbol{E}} \\left[ \\hat{\\beta}_1 \\right] = \\beta_1 + \\color{#81A1C1}{(+)} \\dfrac{\\color{#EBCB8B}{(+)}}{\\color{#8FBCBB}{(+)}} \\implies \\mathop{\\boldsymbol{E}} \\left[ \\hat{\\beta}_1 \\right] &gt; \\beta_1\n\\end{align}\n\\] ∴ In this case, OLS is biased upward (estimates are too large).\n. . .\n\\[\n\\begin{matrix}\n\\enspace & \\color{#EBCB8B}{\\text{Cov}(X_i,\\,Z_i)&gt; 0} & \\color{#EBCB8B}{\\text{Cov}(X_i,\\,Z_i)&lt; 0} \\\\\n\\color{#81A1C1}{\\beta_2 &gt; 0} & \\text{Upward} &  \\\\\n\\color{#81A1C1}{\\beta_2 &lt; 0} &  &\n\\end{matrix}\n\\]"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/063-ovb.html#signing-the-bias-2",
    "href": "lectures/06-Multiple-Var-Reg/063-ovb.html#signing-the-bias-2",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Signing the Bias",
    "text": "Signing the Bias\nBegin with\n\\[\n\\mathop{\\boldsymbol{E}} \\left[ \\hat{\\beta}_1 \\right] = \\beta_1 + \\beta_2 \\dfrac{ \\mathop{\\text{Cov}} \\left( X_i,\\, Z_i \\right)}{\\mathop{\\text{Var}} \\left( X_i \\right)}\n\\]\nWe know \\(\\color{#8FBCBB}{\\mathop{\\text{Var}} \\left( X_i \\right) &gt; 0}\\). Suppose \\(\\color{#81A1C1}{\\beta_2 &lt; 0}\\) and \\(\\color{#EBCB8B}{\\mathop{\\text{Cov}} \\left( X_i,\\,Z_i \\right) &gt; 0}\\). Then\n. . .\n\\[\n\\begin{align}\n\\mathop{\\boldsymbol{E}} \\left[ \\hat{\\beta}_1 \\right] = \\beta_1 + \\color{#81A1C1}{(-)} \\dfrac{\\color{#EBCB8B}{(+)}}{\\color{#8FBCBB}{(+)}} \\implies \\mathop{\\boldsymbol{E}} \\left[ \\hat{\\beta}_1 \\right] &lt; \\beta_1\n\\end{align}\n\\] ∴ In this case, OLS is biased downward (estimates are too small).\n\\[\n\\begin{matrix}\n\\enspace & \\color{#EBCB8B}{\\text{Cov}(X_i,\\,Z_i)&gt; 0} & \\color{#EBCB8B}{\\text{Cov}(X_i,\\,Z_i)&lt; 0} \\\\\n\\color{#81A1C1}{\\beta_2 &gt; 0} & \\text{Upward} &  \\\\\n\\color{#81A1C1}{\\beta_2 &lt; 0} & \\text{Downward} &\n\\end{matrix}\n\\]"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/063-ovb.html#signing-the-bias-3",
    "href": "lectures/06-Multiple-Var-Reg/063-ovb.html#signing-the-bias-3",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Signing the Bias",
    "text": "Signing the Bias\nBegin with\n\\[\n\\mathop{\\boldsymbol{E}} \\left[ \\hat{\\beta}_1 \\right] = \\beta_1 + \\beta_2 \\dfrac{ \\mathop{\\text{Cov}} \\left( X_i,\\, Z_i \\right)}{\\mathop{\\text{Var}} \\left( X_i \\right)}\n\\]\nWe know \\(\\color{#8FBCBB}{\\mathop{\\text{Var}} \\left( X_i \\right) &gt; 0}\\). Suppose \\(\\color{#81A1C1}{\\beta_2 &gt; 0}\\) and \\(\\color{#EBCB8B}{\\mathop{\\text{Cov}} \\left( X_i,\\,Z_i \\right) &lt; 0}\\). Then\n\\[\n\\begin{align}\n\\mathop{\\boldsymbol{E}} \\left[ \\hat{\\beta}_1 \\right] = \\beta_1 + \\color{#81A1C1}{(+)} \\dfrac{\\color{#EBCB8B}{(-)}}{\\color{#8FBCBB}{(+)}} \\implies \\mathop{\\boldsymbol{E}} \\left[ \\hat{\\beta}_1 \\right] &lt; \\beta_1\n\\end{align}\n\\] ∴ In this case, OLS is biased downward (estimates are too small).\n\\[\n\\begin{matrix}\n\\enspace & \\color{#EBCB8B}{\\text{Cov}(X_i,\\,Z_i)&gt; 0} & \\color{#EBCB8B}{\\text{Cov}(X_i,\\,Z_i)&lt; 0} \\\\\n\\color{#81A1C1}{\\beta_2 &gt; 0} & \\text{Upward} & \\text{Downward} \\\\\n\\color{#81A1C1}{\\beta_2 &lt; 0} & \\text{Downward} &\n\\end{matrix}\n\\]"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/063-ovb.html#signing-the-bias-4",
    "href": "lectures/06-Multiple-Var-Reg/063-ovb.html#signing-the-bias-4",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Signing the Bias",
    "text": "Signing the Bias\nBegin with\n\\[\n\\mathop{\\boldsymbol{E}} \\left[ \\hat{\\beta}_1 \\right] = \\beta_1 + \\beta_2 \\dfrac{ \\mathop{\\text{Cov}} \\left( X_i,\\, Z_i \\right)}{\\mathop{\\text{Var}} \\left( X_i \\right)}\n\\]\nWe know \\(\\color{#8FBCBB}{\\mathop{\\text{Var}} \\left( X_i \\right) &gt; 0}\\). Suppose \\(\\color{#81A1C1}{\\beta_2 &lt; 0}\\) and \\(\\color{#EBCB8B}{\\mathop{\\text{Cov}} \\left( X_i,\\,Z_i \\right) &lt; 0}\\). Then\n\\[\n\\begin{align}\n\\mathop{\\boldsymbol{E}} \\left[ \\hat{\\beta}_1 \\right] = \\beta_1 + \\color{#81A1C1}{(-)} \\dfrac{\\color{#EBCB8B}{(-)}}{\\color{#8FBCBB}{(+)}} \\implies \\mathop{\\boldsymbol{E}} \\left[ \\hat{\\beta}_1 \\right] &gt; \\beta_1\n\\end{align}\n\\] ∴ In this case, OLS is biased upward (estimates are too large).\n\\[\n\\begin{matrix}\n\\enspace & \\color{#EBCB8B}{\\text{Cov}(X_i,\\,Z_i)&gt; 0} & \\color{#EBCB8B}{\\text{Cov}(X_i,\\,Z_i)&lt; 0} \\\\\n\\color{#81A1C1}{\\beta_2 &gt; 0} & \\text{Upward} & \\text{Downward} \\\\\n\\color{#81A1C1}{\\beta_2 &lt; 0} & \\text{Downward} & \\text{Upward}\n\\end{matrix}\n\\]"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/063-ovb.html#signing-the-bias-5",
    "href": "lectures/06-Multiple-Var-Reg/063-ovb.html#signing-the-bias-5",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Signing the Bias",
    "text": "Signing the Bias\nThus, in cases where we have a sense of\n\nthe sign of \\(\\mathop{\\text{Cov}} \\left( X_i,\\,Z_i \\right)\\)\nthe sign of \\(\\beta_2\\)\n\nwe know in which direction bias pushes our estimates.\nDirection of Bias\n\\[\n\\begin{matrix}\n\\enspace & \\color{#EBCB8B}{\\text{Cov}(X_i,\\,Z_i)&gt; 0} & \\color{#EBCB8B}{\\text{Cov}(X_i,\\,Z_i)&lt; 0} \\\\\n\\color{#81A1C1}{\\beta_2 &gt; 0} & \\text{Upward} & \\text{Downward} \\\\\n\\color{#81A1C1}{\\beta_2 &lt; 0} & \\text{Downward} & \\text{Upward}\n\\end{matrix}\n\\]"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/065-multicollinearity.html",
    "href": "lectures/06-Multiple-Var-Reg/065-multicollinearity.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Multiple regression model:\n\\[\nY_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\cdots + \\beta_{k} X_{ki} + u_i\n\\]\nIt can be shown that the estimator \\(\\hat{\\beta}_j\\) on independent variable \\(X_j\\) is:\n\\[\n\\mathop{\\text{Var}} \\left( \\hat{\\beta_j} \\right) = \\dfrac{\\sigma^2}{\\left( 1 - R^2_j \\right)\\sum_{i=1}^n \\left( X_{ji} - \\bar{X}_j \\right)^2},\n\\]\nwhere \\(R^2_j\\) is the \\(R^2\\) from a regression of \\(X_j\\) on the other independent variables and the intercept"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/065-multicollinearity.html#ols-variances",
    "href": "lectures/06-Multiple-Var-Reg/065-multicollinearity.html#ols-variances",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Multiple regression model:\n\\[\nY_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\cdots + \\beta_{k} X_{ki} + u_i\n\\]\nIt can be shown that the estimator \\(\\hat{\\beta}_j\\) on independent variable \\(X_j\\) is:\n\\[\n\\mathop{\\text{Var}} \\left( \\hat{\\beta_j} \\right) = \\dfrac{\\sigma^2}{\\left( 1 - R^2_j \\right)\\sum_{i=1}^n \\left( X_{ji} - \\bar{X}_j \\right)^2},\n\\]\nwhere \\(R^2_j\\) is the \\(R^2\\) from a regression of \\(X_j\\) on the other independent variables and the intercept"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/065-multicollinearity.html#ols-variances-1",
    "href": "lectures/06-Multiple-Var-Reg/065-multicollinearity.html#ols-variances-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "OLS variances",
    "text": "OLS variances\n\\[\n\\mathop{\\text{Var}} \\left( \\hat{\\beta_j} \\right) = \\dfrac{{\\color{#81A1C1}\\sigma^2}}{\\left( 1 - \\color{#81A1C1}{R_j^2} \\right)\\color{#BF616A}{\\sum_{i=1}^n \\left( X_{ji} - \\bar{X}_j \\right)^2}},\n\\]\nMoving parts:\n\n1. Error variance: As \\(\\color{#81A1C1}{\\sigma^2}\\) increases, \\(Var(\\hat{\\beta}_j)\\) increases\n2. Total variation in \\(X_j\\): As \\(\\color{#BF616A}{\\sum_{i=1}^n \\left( X_{ji} - \\bar{X}_j \\right)^2}\\) increases, \\(Var(\\hat{\\beta}_j)\\) decreases\n3. Relationship across \\(X_i\\): As \\(\\color{#81A1C1}{R_j^2}\\) increases, \\(Var(\\hat{\\beta}_j)\\) increases\n\n\n. . .\n3. is better known as Multicollinearity"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/065-multicollinearity.html#multicollinearity",
    "href": "lectures/06-Multiple-Var-Reg/065-multicollinearity.html#multicollinearity",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Multicollinearity",
    "text": "Multicollinearity\n\nCase in which two or more independent variables in a regression model are highly correlated.\n\n. . .\n\nOne independent variable can predict most of the variation in another independent variable.\n\n. . .\nMulticollinearity leads to imprecise estimates. Becomes difficult to distinguish between individual effects from of independent variables."
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/065-multicollinearity.html#ols-assumptions",
    "href": "lectures/06-Multiple-Var-Reg/065-multicollinearity.html#ols-assumptions",
    "title": "EC 320 - Intro. Econometrics",
    "section": "OLS Assumptions",
    "text": "OLS Assumptions\nClassical assumptions for OLS change slightly for multiple OLS\nA1. Linearity: The population relationship is linear in parameters with an additive error term.\nA2. Sample Variation: No \\(X\\) variable is a perfect linear combination of the others\nA3. Exogeniety: The \\(X\\) variable is exogenous\nA4. Homoskedasticity: The error term has the same variance for each value of the independent variable\nA5. Non-autocorrelation: The values of error terms have independent distributions"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/065-multicollinearity.html#perfect-collinearity",
    "href": "lectures/06-Multiple-Var-Reg/065-multicollinearity.html#perfect-collinearity",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Perfect Collinearity",
    "text": "Perfect Collinearity\n\nCase in which two or more independent variables in a regression model are perfectly correlated.\n\nEx. 2016 Election\nOLS simultaneously cannot estimate parameters for white and nonwhite.\n. . .\n\n\n\n\n\nR drops perfectly collinear variables for you."
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/065-multicollinearity.html#multicollinearity-ex.",
    "href": "lectures/06-Multiple-Var-Reg/065-multicollinearity.html#multicollinearity-ex.",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Multicollinearity Ex.",
    "text": "Multicollinearity Ex.\nSuppose that we want to understand the relationship between crime rates and poverty rates in US cities. We could estimate the model\n\\[\n\\text{Crime}_i = \\beta_0 + \\beta_1 \\text{Poverty}_i + \\beta_2 \\text{Income}_i + u_i\n\\]\n. . .\nBefore obtaining standard errors, we need:\n\\[\n\\mathop{\\text{Var}} \\left( \\hat{\\beta}_1 \\right) = \\dfrac{\\sigma^2}{\\left( 1 - R^2_1 \\right)\\sum_{i=1}^n \\left( \\text{Poverty}_{i} - \\overline{\\text{Poverty}} \\right)^2}\n\\]\n. . .\n\\(R^2_1\\) is the \\(R^2\\) from a regression of poverty on median income:\n\\[\n\\text{Poverty}_i = \\gamma_0 + \\gamma_1 \\text{Income}_i + v_i\n\\]"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/065-multicollinearity.html#multicollinearity-1",
    "href": "lectures/06-Multiple-Var-Reg/065-multicollinearity.html#multicollinearity-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Multicollinearity",
    "text": "Multicollinearity\nScenario 1: \\(\\text{Income}_i\\) explains most variation in \\(\\text{Poverty}_i\\), then \\(R^2_1 \\rightarrow 1\\)\n\nViolates the no perfect collinearity assumption\n\n. . .\nScenario 2: If \\(\\text{Income}_i\\) explains no variation in \\(\\text{Poverty}_i\\), then \\(R^2_1 = 0\\)\n. . .\nQ. In which scenario is the variance of the poverty coefficient smaller?\n\\[\n\\mathop{\\text{Var}} \\left( \\hat{\\beta}_1 \\right) = \\dfrac{\\sigma^2}{\\left( 1 - R^2_1 \\right)\\sum_{i=1}^n \\left( \\text{Poverty}_{i} - \\overline{\\text{Poverty}} \\right)^2}\n\\]\n. . .\nA. Scenario 2."
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/065-multicollinearity.html#multicollinearity-2",
    "href": "lectures/06-Multiple-Var-Reg/065-multicollinearity.html#multicollinearity-2",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Multicollinearity",
    "text": "Multicollinearity\nAs the relationships between the variables increase, \\(R^2_j\\) increases.\nFor high \\(R^2_j\\), \\(\\mathop{\\text{Var}} \\left( \\hat{\\beta_j} \\right)\\) is large:\n\\[\n\\mathop{\\text{Var}} \\left( \\hat{\\beta_j} \\right) = \\dfrac{\\sigma^2}{\\left( 1 - R^2_j \\right)\\sum_{i=1}^n \\left( X_{ji} - \\bar{X}_j \\right)^2}\n\\]\n. . .\n\nSome view multicollinearity as a “problem” to be solved.\nEither increase power (\\(n\\)) or drop correlated variables\nWarning: Dropping variables can generate omitted variable bias."
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/065-multicollinearity.html#irrelevant-variables",
    "href": "lectures/06-Multiple-Var-Reg/065-multicollinearity.html#irrelevant-variables",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Irrelevant Variables",
    "text": "Irrelevant Variables\nSuppose that the true relationship between birth weight and in utero exposure to toxic air pollution is\n\\[\n(\\text{Birth Weight})_i = \\beta_0 + \\beta_1 \\text{Pollution}_i + u_i\n\\]\n. . .\nSuppose that an “analyst” estimates\n\\[\n(\\text{Birth Weight})_i = \\tilde{\\beta_0} + \\tilde{\\beta_1} \\text{Pollution}_i + \\tilde{\\beta_2}\\text{NBA}_i + u_i\n\\]\n. . .\nOne can show that \\(\\mathop{\\mathbb{E}} \\left( \\hat{\\tilde{\\beta_1}} \\right) = \\beta_1\\) (i.e., \\(\\hat{\\tilde{\\beta_1}}\\) is unbiased).\nHowever, the variances of \\(\\hat{\\tilde{\\beta_1}}\\) and \\(\\hat{\\beta_1}\\) differ."
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/065-multicollinearity.html#irrelevant-variables-1",
    "href": "lectures/06-Multiple-Var-Reg/065-multicollinearity.html#irrelevant-variables-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Irrelevant Variables",
    "text": "Irrelevant Variables\n\n\n\n\n\nWe can reasonably say that the NBA has no direct impact on birth weight, so it is doing more damage to the model than helping"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/065-multicollinearity.html#irrelevant-variables-2",
    "href": "lectures/06-Multiple-Var-Reg/065-multicollinearity.html#irrelevant-variables-2",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Irrelevant Variables",
    "text": "Irrelevant Variables\nThe variance of \\(\\hat{\\beta}_1\\) from estimating the “true model” is\n\\[\n\\mathop{\\text{Var}} \\left( \\hat{\\beta_1} \\right) = \\dfrac{\\sigma^2}{\\sum_{i=1}^n \\left( \\text{Pollution}_{i} - \\overline{\\text{Pollution}} \\right)^2}\n\\]\nThe variance of \\(\\hat{\\tilde\\beta}_1\\) from estimating the model with the irrelevant variable is\n\\[\n\\mathop{\\text{Var}} \\left( \\hat{\\tilde{\\beta_1}} \\right) = \\dfrac{\\sigma^2}{\\left( 1 - R^2_1 \\right)\\sum_{i=1}^n \\left( \\text{Pollution}_{i} - \\overline{\\text{Pollution}} \\right)^2}\n\\]"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/065-multicollinearity.html#irrelevant-variables-3",
    "href": "lectures/06-Multiple-Var-Reg/065-multicollinearity.html#irrelevant-variables-3",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Irrelevant Variables",
    "text": "Irrelevant Variables\nNotice that \\(\\mathop{\\text{Var}} \\left( \\hat{\\beta_1} \\right) \\leq \\mathop{\\text{Var}} \\left( \\hat{\\tilde{\\beta_1}} \\right)\\) since,\n\\[\n\\sum_{i=1}^n \\left( \\text{Poll.}_{i} - \\overline{\\text{Poll.}} \\right)^2\n\\geq\n\\left( 1 - R^2_1 \\right)\\sum_{i=1}^n \\left( \\text{Poll.}_{i} - \\overline{\\text{Poll.}} \\right)^2\n\\]\n. . .\n\nA tradeoff exists when including more control variables. Make sure you have good reason for your controls because including irrelevant control variables increase variances"
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/065-multicollinearity.html#estimating-error-variance",
    "href": "lectures/06-Multiple-Var-Reg/065-multicollinearity.html#estimating-error-variance",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Estimating Error Variance",
    "text": "Estimating Error Variance\nWe cannot observe \\(\\sigma^2\\), so we must estimate it using the residuals from an estimated regression:\n\\[\ns_u^2 = \\dfrac{\\sum_{i=1}^n \\hat{u}_i^2}{n - k - 1}\n\\]\n\n\\(k+1\\) is the number of parameters (one “slope” for each \\(X\\) variable and an intercept).\n\\(n - k - 1\\) = degrees of freedom.\nUsing the first 5 OLS assumptions, one can prove that \\(s_u^2\\) is an unbiased estimator of \\(\\sigma^2\\)."
  },
  {
    "objectID": "lectures/06-Multiple-Var-Reg/065-multicollinearity.html#standard-errors",
    "href": "lectures/06-Multiple-Var-Reg/065-multicollinearity.html#standard-errors",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Standard Errors",
    "text": "Standard Errors\nThe formula for the standard error is the square root of \\(\\mathop{\\text{Var}} \\left( \\hat{\\beta_j} \\right)\\):\n\\[\n\\mathop{\\text{SE}}(\\hat{\\beta_j}) = \\sqrt{ \\frac{s^2_u}{(  1 - R^2_j ) \\sum_{i=1}^n ( X_{ji} - \\bar{X}_j )^2} }\n\\]"
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/070-compile.html#linear-regression",
    "href": "lectures/07-Non-Linear-Models/070-compile.html#linear-regression",
    "title": "Non-Linear Models",
    "section": "Linear regression",
    "text": "Linear regression\nSuppose we would like to estimate the degree to which an increase in GDP correlates with Life expectancy. We set up our model as follows:\n\\[\n{\\text{Life Expectancy}_i} = \\beta_0 + \\beta_1 \\text{GDP}_i + u_i\n\\]\n\nUsing the gapminder package in R, we could quickly generate estimates to get at the correlation But first, as always, let’s plot it before running the regression"
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/070-compile.html#linear-regression-1",
    "href": "lectures/07-Non-Linear-Models/070-compile.html#linear-regression-1",
    "title": "Non-Linear Models",
    "section": "Linear regression",
    "text": "Linear regression\nUsing the gapminder, we could quickly generate estimates for\n\\[\n\\widehat{\\text{Life Expectancy}_i} = \\hat{\\beta_0} + \\hat{\\beta_1} \\cdot \\text{GDP}_i\n\\]\n\n\n\nlibrary(gapminder)\nlibrary(broom)\n\nm1 = lm(lifeExp ~ gdpPercap, data = gapminder) \n\ntidy(m1)\n\n# A tibble: 2 × 5\n  term         estimate std.error statistic   p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept) 54.0      0.315         171.  0        \n2 gdpPercap    0.000765 0.0000258      29.7 3.57e-156"
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/070-compile.html#linearity-in-ols",
    "href": "lectures/07-Non-Linear-Models/070-compile.html#linearity-in-ols",
    "title": "Non-Linear Models",
    "section": "Linearity in OLS",
    "text": "Linearity in OLS\nUp to this point, we’ve acknowledged OLS as a “linear” estimator.\n\n\nMany economic relationships are nonlinear.\n\ne.g., most production functions, profit, diminishing marginal utility, tax revenue as a function of the tax rate, etc.\n\n\n\n\nThe “linear” in simple linear regression refers to the linearity of the parameters or coefficients, not the predictors themselves."
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/070-compile.html#linearity-in-ols-1",
    "href": "lectures/07-Non-Linear-Models/070-compile.html#linearity-in-ols-1",
    "title": "Non-Linear Models",
    "section": "Linearity in OLS",
    "text": "Linearity in OLS\nOLS is flexible and can accommodate a subset of nonlinear relationships.\n\nUnderlying model must be linear-in-parameters.\nNonlinear transformations of variables are okay.\nModeling some nonlinear relationships requires advanced estimation techniques, such as maximum likelihood1\n\n\n\nPut different, independent variables can be a linear combination of the parameters, regardless of any nonlinear transformations\n\nBeyond the scope of this class."
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/070-compile.html#linearity",
    "href": "lectures/07-Non-Linear-Models/070-compile.html#linearity",
    "title": "Non-Linear Models",
    "section": "Linearity",
    "text": "Linearity\nLinear-in-parameters: Parameters enter model as a weighted sum, where the weights are functions of the variables.\n\nOne of the assumptions required for the unbiasedness of OLS.\n\nLinear-in-variables: Variables enter the model as a weighted sum, where the weights are functions of the parameters.\n\nNot required for the unbiasedness of OLS.\n\n\nThe standard linear regression model satisfies both properties:\n\\[Y_i = \\beta_0 + \\beta_1X_{1i} + \\beta_2X_{2i} + \\dots + \\beta_kX_{ki} + u_i\\]"
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/070-compile.html#linearity-1",
    "href": "lectures/07-Non-Linear-Models/070-compile.html#linearity-1",
    "title": "Non-Linear Models",
    "section": "Linearity",
    "text": "Linearity\nWhich of the following are an example of linear-in-parameters, linear-in-variables, or neither?\n\n1. \\(Y_i = \\beta_0 + \\beta_1X_{i} + \\beta_2X_{i}^2 + \\dots + \\beta_kX_{i}^k + u_i\\)\n2. \\(Y_i = \\beta_0X_i^{\\beta_1}v_i\\)\n3. \\(Y_i = \\beta_0 + \\beta_1\\beta_2X_{i} + u_i\\)"
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/070-compile.html#linearity-2",
    "href": "lectures/07-Non-Linear-Models/070-compile.html#linearity-2",
    "title": "Non-Linear Models",
    "section": "Linearity",
    "text": "Linearity\nWhich of the following are an example of linear-in-parameters, linear-in-variables, or neither?\n\n1. \\(\\color{#A3BE8C}{Y_i = \\beta_0 + \\beta_1X_{i} + \\beta_2X_{i}^2 + \\dots + \\beta_kX_{i}^k + u_i}\\)\n2. \\(Y_i = \\beta_0X_i^{\\beta_1}v_i\\)\n3. \\(Y_i = \\beta_0 + \\beta_1\\beta_2X_{i} + u_i\\)\n\nModel 1 is linear-in-parameters, but not linear-in-variables."
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/070-compile.html#linearity-3",
    "href": "lectures/07-Non-Linear-Models/070-compile.html#linearity-3",
    "title": "Non-Linear Models",
    "section": "Linearity",
    "text": "Linearity\nWhich of the following are an example of linear-in-parameters, linear-in-variables, or neither?\n\n1. \\(\\color{#A3BE8C}{Y_i = \\beta_0 + \\beta_1X_{i} + \\beta_2X_{i}^2 + \\dots + \\beta_kX_{i}^k + u_i}\\)\n2. \\(\\color{#434C5E}{Y_i = \\beta_0X_i^{\\beta_1}v_i}\\)\n3. \\(Y_i = \\beta_0 + \\beta_1\\beta_2X_{i} + u_i\\)\n\nModel 1 is linear-in-parameters, but not linear-in-variables.\nModel 2 is neither."
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/070-compile.html#linearity-4",
    "href": "lectures/07-Non-Linear-Models/070-compile.html#linearity-4",
    "title": "Non-Linear Models",
    "section": "Linearity",
    "text": "Linearity\nWhich of the following are an example of linear-in-parameters, linear-in-variables, or neither?\n\n1. \\(\\color{#A3BE8C}{Y_i = \\beta_0 + \\beta_1X_{i} + \\beta_2X_{i}^2 + \\dots + \\beta_kX_{i}^k + u_i}\\)\n2. \\(\\color{#434C5E}{Y_i = \\beta_0X_i^{\\beta_1}v_i}\\)\n3. \\(\\color{#B48EAD}{Y_i = \\beta_0 + \\beta_1\\beta_2X_{i} + u_i}\\)\n\nModel 1 is linear-in-parameters, but not linear-in-variables.\nModel 2 is neither.\nModel 3 is linear-in-variables, but not linear-in-parameters."
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/070-compile.html#were-going-to-take-logs",
    "href": "lectures/07-Non-Linear-Models/070-compile.html#were-going-to-take-logs",
    "title": "Non-Linear Models",
    "section": "We’re going to take logs",
    "text": "We’re going to take logs\nThe natural log is the inverse function for the exponential function:\n\\[\n\\quad \\log(e^x) = x \\quad \\text{for} \\quad x&gt;0\n\\]\n\n(Natural) Log rules:\n1. Product rule: \\(\\log(AB) = \\log(A) + \\log(B)\\).\n\n\n2. Quotient rule: \\(\\log(A/B) = \\log(A) - \\log(B)\\).\n\n\n3. Power rule: \\(\\log(A^B) = B \\cdot \\log(A)\\).\n\n\n4. Derivative: \\(f(x) = \\log(x)\\) =&gt; \\(f'(x) = \\dfrac{1}{x}\\).\n\n\nNote: \\(\\log(e) = 1\\), \\(\\log(1) = 0\\), and \\(\\log(x)\\) is undefined for \\(x \\leq 0\\)."
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/070-compile.html#log-linear-model",
    "href": "lectures/07-Non-Linear-Models/070-compile.html#log-linear-model",
    "title": "Non-Linear Models",
    "section": "Log-Linear model",
    "text": "Log-Linear model\nNonlinear Model\n\\[\nY_i = \\alpha e^{\\beta_1 X_i}v_i\n\\]\n\n\\(Y &gt; 0\\), \\(X\\) is continuous, and \\(v_i\\) is a multiplicative error term.\nCannot estimate parameters with OLS directly.\n\n\n\n\nLogarithmic Transformation\n\\[\n\\log(Y_i) = \\log(\\alpha) + \\beta_1 X_i + \\log(v_i)\n\\]\nRedefine \\(\\log(\\alpha) \\equiv \\beta_0\\), \\(\\log(v_i) \\equiv u_i\\).\n\nTransformed (Linear) Model\n\\[\n\\log(Y_i) = \\beta_0 + \\beta_1 X_i + u_i\n\\]\nCan estimate with OLS, but interpretation changes."
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/070-compile.html#log-linear-model-1",
    "href": "lectures/07-Non-Linear-Models/070-compile.html#log-linear-model-1",
    "title": "Non-Linear Models",
    "section": "Log-Linear model",
    "text": "Log-Linear model\nRegression Model\n\\[\n\\log(Y_i) = \\beta_0 + \\beta_1 X_i + u_i\n\\]\nInterpretation\n\nA one-unit increase in the explanatory variable increases the outcome variable by approximately \\(\\beta_1\\times 100\\) percent, on average.\n\n\nEx.\n\nIf \\(\\log(\\hat{\\text{Pay}_i}) = 2.9 + 0.03 \\cdot \\text{School}_i\\), then an additional year of schooling increases pay by approximately 3 percent, on average."
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/070-compile.html#log-linear-model-2",
    "href": "lectures/07-Non-Linear-Models/070-compile.html#log-linear-model-2",
    "title": "Non-Linear Models",
    "section": "Log-Linear model",
    "text": "Log-Linear model\nDerivation Consider the log-linear model\n\\[\n\\log(Y) = \\beta_0 + \\beta_1 \\, X + u\n\\]\nand differentiate\n\\[\n\\dfrac{dY}{Y} = \\beta_1 dX\n\\]\n\nMarginal change in \\(X\\) (\\(dX\\)) leads to a \\(\\beta_1 dX\\) proportionate change in \\(Y\\).\n\nMultiply by 100 to get the percentage change in \\(Y\\)."
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/070-compile.html#log-linear-ex",
    "href": "lectures/07-Non-Linear-Models/070-compile.html#log-linear-ex",
    "title": "Non-Linear Models",
    "section": "Log-Linear Ex",
    "text": "Log-Linear Ex\n\\[\n    log(\\hat{Y}_{i}) = 10.02 + 0.73 \\cdot X_{i}\n\\]"
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/070-compile.html#log-linear-ex-1",
    "href": "lectures/07-Non-Linear-Models/070-compile.html#log-linear-ex-1",
    "title": "Non-Linear Models",
    "section": "Log-Linear Ex",
    "text": "Log-Linear Ex\n\\[\n    log(\\hat{Y}_{i}) = 10.02 + 0.73 \\cdot X_{i}\n\\]"
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/070-compile.html#log-linear",
    "href": "lectures/07-Non-Linear-Models/070-compile.html#log-linear",
    "title": "Non-Linear Models",
    "section": "Log-Linear",
    "text": "Log-Linear\nNote: If you have a log-linear model with a binary indicator variable, the interpretation of the coefficient on that variable changes. Consider\n\n\\[\n\\log(Y_i) = \\beta_0 + \\beta_1 X_i + u_i\n\\]\nfor binary variable \\(X\\).\n\n\nInterpretation of \\(\\beta_1\\):\n\nWhen \\(X\\) changes from 0 to 1, \\(Y\\) will increase by \\(100 \\times \\left( e^{\\beta_1} -1 \\right)\\)%\nWhen \\(X\\) changes from 1 to 0, \\(Y\\) will decrease by \\(100 \\times \\left( e^{-\\beta_1} -1 \\right)\\)%"
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/070-compile.html#log-linear-ex-2",
    "href": "lectures/07-Non-Linear-Models/070-compile.html#log-linear-ex-2",
    "title": "Non-Linear Models",
    "section": "Log-Linear Ex",
    "text": "Log-Linear Ex\nTake a binary explanatory variable: trained\n\ntrained = 1 if employee \\(i\\) received training\ntrained = 0 if employee \\(i\\) did not receive training\n\n\n\n\nTerm\nEstimate\nStd. Error\nStatistic\nP-value\n\n\n\n\nIntercept\n9.94\n0.0446\n223\n0\n\n\nTrained\n0.557\n0.0631\n8.83\n4.72e-18\n\n\n\nQ. How do we interpret the coefficient on trained?\n\nA1: Trained workers are 74.52 percent more productive than untrained workers.\n\n\nA2: Untrained workers are 42.7 percent less productive than trained workers."
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/070-compile.html#log-log-model",
    "href": "lectures/07-Non-Linear-Models/070-compile.html#log-log-model",
    "title": "Non-Linear Models",
    "section": "Log-Log model",
    "text": "Log-Log model\nNonlinear Model\n\\[\nY_i = \\alpha  X_i^{\\beta_1}v_i\n\\]\n\n\\(Y &gt; 0\\), \\(X &gt; 0\\), and \\(v_i\\) is a multiplicative error term.\nCannot estimate parameters with OLS directly.\n\n\n\n\nLogarithmic Transformation\n\\[\n\\begin{align*}\n\\log(Y_i) = \\log(\\alpha) +& \\beta_1 \\log(X_i) \\\\\n                         +& \\log(v_i)\n\\end{align*}\n\\]\n\nRedefine \\(\\log(\\alpha) \\equiv \\beta_0\\), \\(\\log(v_i) \\equiv u_i\\).\n\n\nTransformed (Linear) Model\n\\[\n\\log(Y_i) = \\beta_0 + \\beta_1 \\log(X_i) + u_i\n\\]\nCan estimate with OLS, but interpretation changes."
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/070-compile.html#log-log-regression-model",
    "href": "lectures/07-Non-Linear-Models/070-compile.html#log-log-regression-model",
    "title": "Non-Linear Models",
    "section": "Log-Log regression model",
    "text": "Log-Log regression model\n\\[\n\\log(Y_i) = \\beta_0 + \\beta_1 \\log(X_i) + u_i\n\\]\nInterpretation\n\nA one-percent increase in the explanatory variable leads to a \\(\\beta_1\\)-percent change in the outcome variable, on average.\nOften interpreted as an elasticity.\n\n\nEx.\n\nIf \\(\\log(\\widehat{\\text{Quantity Demanded}}_i) = 0.45 - 0.31 \\cdot \\log(\\text{Income}_i)\\), then each one-percent increase in income decreases quantity demanded by 0.31 percent."
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/070-compile.html#log-log-derivation",
    "href": "lectures/07-Non-Linear-Models/070-compile.html#log-log-derivation",
    "title": "Non-Linear Models",
    "section": "Log-Log derivation",
    "text": "Log-Log derivation\nConsider the log-log model\n\\[\n\\log(Y_i) = \\beta_0 + \\beta_1 \\log(X_i) + u\n\\]\nand differentiate\n\\[\n\\dfrac{dY}{Y} = \\beta_1 \\dfrac{dX}{X}\n\\]\nA one-percent increase in \\(X\\) leads to a \\(\\beta_1\\)-percent increase in \\(Y\\).\n\nRearrange to show elasticity interpretation:\n\n\\[\n\\dfrac{dY}{dX} \\dfrac{X}{Y} = \\beta_1\n\\]"
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/070-compile.html#log-log-example",
    "href": "lectures/07-Non-Linear-Models/070-compile.html#log-log-example",
    "title": "Non-Linear Models",
    "section": "Log-Log Example",
    "text": "Log-Log Example\n\\[\n    log(\\hat{Y}_{i}) = 0.01 + 2.99 \\cdot log(X_{i})\n\\]"
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/070-compile.html#log-log-example-1",
    "href": "lectures/07-Non-Linear-Models/070-compile.html#log-log-example-1",
    "title": "Non-Linear Models",
    "section": "Log-Log Example",
    "text": "Log-Log Example\n\\[\n    log(\\hat{Y}_{i}) = 0.01 + 2.99 \\cdot log(X_{i})\n\\]"
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/070-compile.html#linear-log-model",
    "href": "lectures/07-Non-Linear-Models/070-compile.html#linear-log-model",
    "title": "Non-Linear Models",
    "section": "Linear-Log Model",
    "text": "Linear-Log Model\nNonlinear Model\n\\[\ne^{Y_i} = \\alpha  X_i^{\\beta_1}v_i\n\\]\n\n\\(X &gt; 0\\) and \\(v_i\\) is a multiplicative error term.\nCannot estimate parameters with OLS directly.\n\n\n\n\nLogarithmic Transformation\n\\[\nY_i = \\log(\\alpha) + \\beta_1 \\log(X_i) + \\log(v_i)\n\\]\nRedefine \\(\\log(\\alpha) \\equiv \\beta_0\\), \\(\\log(v_i) \\equiv u_i\\).\n\nTransformed (Linear) Model\n\\[\nY_i = \\beta_0 + \\beta_1 \\log(X_i) + u_i\n\\]\nCan estimate with OLS, but interpretation changes."
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/070-compile.html#linear-log-model-1",
    "href": "lectures/07-Non-Linear-Models/070-compile.html#linear-log-model-1",
    "title": "Non-Linear Models",
    "section": "Linear-Log Model",
    "text": "Linear-Log Model\nRegression Model\n\\[\nY_i = \\beta_0 + \\beta_1 \\log(X_i) + u_i\n\\]\nInterpretation\n\nA one-percent increase in the explanatory variable increases the outcome variable by approximately \\(\\beta_1 \\div 100\\), on average.\n\n\nEx.\n\nIf \\(\\widehat{(\\text{Blood Pressure})_i} = 150 - 9.1 \\log(\\text{Income}_i)\\), then a one-percent increase in income decrease blood pressure by 0.091 points."
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/070-compile.html#linear-log-derivation",
    "href": "lectures/07-Non-Linear-Models/070-compile.html#linear-log-derivation",
    "title": "Non-Linear Models",
    "section": "Linear-Log derivation",
    "text": "Linear-Log derivation\nConsider the log-linear model\n\\[\nY = \\beta_0 + \\beta_1 \\log(X) + u\n\\]\nand differentiate\n\\[\ndY = \\beta_1 \\dfrac{dX}{X}\n\\]\n\nA one-percent increase in \\(X\\) leads to a \\(\\beta_1 \\div 100\\) change in \\(Y\\)."
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/070-compile.html#linear-log-ex",
    "href": "lectures/07-Non-Linear-Models/070-compile.html#linear-log-ex",
    "title": "Non-Linear Models",
    "section": "Linear-Log Ex",
    "text": "Linear-Log Ex\n\\[\n    \\hat{Y}_{i} = 0 + 0.99 \\cdot log(X_{i})\n\\]"
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/070-compile.html#linear-log-ex-1",
    "href": "lectures/07-Non-Linear-Models/070-compile.html#linear-log-ex-1",
    "title": "Non-Linear Models",
    "section": "Linear-Log Ex",
    "text": "Linear-Log Ex\n\\[\n    \\hat{Y}_{i} = 0 + 0.99 \\cdot log(X_{i})\n\\]"
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/070-compile.html#approximate-coefficient-interpretation",
    "href": "lectures/07-Non-Linear-Models/070-compile.html#approximate-coefficient-interpretation",
    "title": "Non-Linear Models",
    "section": "(Approximate) Coefficient Interpretation",
    "text": "(Approximate) Coefficient Interpretation\nFor Model Type:\n\n\nLinear-Linear \\(\\rightarrow Y_{i} = \\beta_{0} + \\beta_{1} X_{i} + u_{i}\\)\n\n\\(\\beta_{1} \\rightarrow \\Delta Y = \\beta_{1} \\cdot \\Delta X \\rightarrow\\) A one-unit increase in \\(X\\) leads to a \\(\\beta_{1}\\)-unit increase in \\(Y\\)\n\nLog-Linear \\(\\rightarrow log(Y_{i}) = \\beta_{0} + \\beta_{1} X_{i} + u_{i}\\)\n\n\\(\\beta_{1} \\rightarrow \\% \\Delta Y = 100 \\cdot \\beta_{1} \\cdot \\Delta X \\rightarrow\\) A one-unit increase in \\(X\\) leads to a \\(\\beta_{1} \\cdot 100\\)-percent increase in \\(Y\\)\n\nLog-Log \\(\\rightarrow log(Y_{i}) = \\beta_{0} + \\beta_{1} log(X_{i}) + u_{i}\\)\n\n\\(\\beta_{1} \\rightarrow \\% \\Delta Y = \\beta_{1} \\cdot \\% \\Delta X \\rightarrow\\) A one-percent increase in \\(X\\) leads to a \\(\\beta_{1}\\)-percent increase in \\(Y\\)\n\nLinear-Log \\(\\rightarrow Y_{i} = \\beta_{0} + \\beta_{1} log(X_{i}) + u_{i}\\)\n\n\\(\\beta_{1} \\rightarrow \\Delta Y = (\\beta_{1} \\div 100) \\cdot \\% \\Delta X \\rightarrow\\) A one-percent increase in \\(X\\) leads to a \\(\\beta_{1} \\div\\)-unit increase in \\(Y\\)"
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/070-compile.html#so-can-we-do-better",
    "href": "lectures/07-Non-Linear-Models/070-compile.html#so-can-we-do-better",
    "title": "Non-Linear Models",
    "section": "So … Can we Do Better?",
    "text": "So … Can we Do Better?\n\\[\n(\\widehat{\\text{Life Expectancy})_i} = 53.96 + 8 \\times 10^{-4} \\cdot \\text{GDP}_i \\quad\\quad R^2 = 0.34\n\\]"
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/070-compile.html#so-can-we-do-better-1",
    "href": "lectures/07-Non-Linear-Models/070-compile.html#so-can-we-do-better-1",
    "title": "Non-Linear Models",
    "section": "So … Can we Do Better?",
    "text": "So … Can we Do Better?\n\\[\nlog(\\widehat{\\text{Life Expectancy})_i} = 3.97 + 1.3 \\times 10^{-5} \\cdot \\text{GDP}_i \\quad\\quad R^2 = 0.3\n\\]"
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/070-compile.html#so-can-we-do-better-2",
    "href": "lectures/07-Non-Linear-Models/070-compile.html#so-can-we-do-better-2",
    "title": "Non-Linear Models",
    "section": "So … Can we Do Better?",
    "text": "So … Can we Do Better?\n\\[\nlog(\\widehat{\\text{Life Expectancy})_i} = 2.86 + 0.15 \\cdot log(\\text{GDP}_i) \\quad\\quad R^2 = 0.61\n\\]"
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/070-compile.html#so-can-we-do-better-3",
    "href": "lectures/07-Non-Linear-Models/070-compile.html#so-can-we-do-better-3",
    "title": "Non-Linear Models",
    "section": "So … Can we Do Better?",
    "text": "So … Can we Do Better?\n\\[\n(\\widehat{\\text{Life Expectancy})_i} = -9.1 + 8.41 \\cdot log(\\text{GDP}_i) \\quad\\quad R^2 = 0.65\n\\]"
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/070-compile.html#practical-considerations",
    "href": "lectures/07-Non-Linear-Models/070-compile.html#practical-considerations",
    "title": "Non-Linear Models",
    "section": "Practical Considerations",
    "text": "Practical Considerations\nConsideration 01 Does your data take negative numbers or zeros as values?\n\n\\(log(0) = \\infty\\)\n\nConsideration 02 What coefficient intepretation do you want?\n\nUnit change? Unit-free percentage change?\n\nConsideration 03 Are your data skewed?"
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/070-compile.html#quadratic-variables",
    "href": "lectures/07-Non-Linear-Models/070-compile.html#quadratic-variables",
    "title": "Non-Linear Models",
    "section": "Quadratic variables",
    "text": "Quadratic variables\nLet’s talk about a wage regression again. Suppose we would like to estimate the effect of age on earnings. We estimate the following SLR:\n\n\\[\n\\text{Wage}_i = \\beta_0 + \\beta_1 \\text{Age}_i + u_i\n\\]\n\n\nHowever, maybe we believe that \\(\\text{Wage}_i\\) and \\(\\text{Age}_i\\) have some nonlinear relationship—the effect of an additional year of experience, when age is 27 vs age is 67, might be different. So instead, we might estimate:\n\n\n\\[\n\\text{Wage}_i = \\beta_0 + \\beta_1 \\text{Age}_i + \\beta_2 \\text{Age}^2_i + u_i\n\\]"
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/070-compile.html#quadratic-variables-1",
    "href": "lectures/07-Non-Linear-Models/070-compile.html#quadratic-variables-1",
    "title": "Non-Linear Models",
    "section": "Quadratic variables",
    "text": "Quadratic variables\nIn this model:\n\\[\n\\text{Wage}_i = \\beta_0 + \\beta_1 \\text{Age}_i + \\beta_2 \\text{Age}^2_i + u_i\n\\]\nthe effect of \\(\\text{Age}_i\\) on \\(\\text{Wage}_i\\) would be:\n\n\\[\n\\frac{\\partial \\text{Wage}_i}{\\partial \\text{Age}_i} = \\beta_1 + 2\\beta_2 \\text{Age}_i\n\\]"
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/070-compile.html#quadratic-regression",
    "href": "lectures/07-Non-Linear-Models/070-compile.html#quadratic-regression",
    "title": "Non-Linear Models",
    "section": "Quadratic regression",
    "text": "Quadratic regression\nRegression Model\n\\[\nY_i = \\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + u_i\n\\]\n\nInterpretation\nSign of \\(\\beta_2\\) indicates whether the relationship is convex (+) or concave (-)\n\n\nSign of \\(\\beta_1\\)? 🤷\n\n\nPartial derivative of \\(Y\\) wrt. \\(X\\) is the marginal effect of \\(X\\) on \\(Y\\):\n\\[\n\\color{#B48EAD}{\\dfrac{\\partial Y}{\\partial X} = \\beta_1 + 2 \\beta_2 X}\n\\]\n\nEffect of \\(X\\) depends on the level of \\(X\\)"
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/070-compile.html#quadratic-regression-1",
    "href": "lectures/07-Non-Linear-Models/070-compile.html#quadratic-regression-1",
    "title": "Non-Linear Models",
    "section": "Quadratic Regression",
    "text": "Quadratic Regression\n\n\n\nTerm\nEstimate\nStd. Error\nStatistic\nP-value\n\n\n\n\nIntercept\n30,046\n138\n218\n0\n\n\nX\n158.89\n5.81\n27.3\n2.58e-123\n\n\n\\(X^{2}\\)\n-1.50\n0.0564\n-26.6\n6.19e-118\n\n\n\nWhat is the marginal effect of \\(X\\) on \\(Y\\)?\n\n\\[\n    \\hat{\\dfrac{\\partial Y}{\\partial X}} = \\hat{\\beta}_{1} + 2 \\hat{\\beta}_{2} X =\n    158.89 + 2(-1.50)X =\n    158.89 - 3X\n\\]\nDepends on level of \\(X\\)"
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/070-compile.html#quadratic-regression-2",
    "href": "lectures/07-Non-Linear-Models/070-compile.html#quadratic-regression-2",
    "title": "Non-Linear Models",
    "section": "Quadratic Regression",
    "text": "Quadratic Regression\n\n\n\nTerm\nEstimate\nStd. Error\nStatistic\nP-value\n\n\n\n\nIntercept\n30,046\n138\n218\n0\n\n\nX\n158.89\n5.81\n27.3\n2.58e-123\n\n\n\\(X^{2}\\)\n-1.50\n0.0564\n-26.6\n6.19e-118\n\n\n\nWhat is the marginal effect of \\(X\\) on \\(Y\\), when \\(X = 0\\)?\n\n\\[\n    \\widehat{\\dfrac{\\partial \\text{Y}}{\\partial \\text{X}} }\\Bigg|_{\\small \\text{X}=0} = \\hat{\\beta}_{1} = 158.89\n\\]"
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/070-compile.html#quadratic-regression-3",
    "href": "lectures/07-Non-Linear-Models/070-compile.html#quadratic-regression-3",
    "title": "Non-Linear Models",
    "section": "Quadratic Regression",
    "text": "Quadratic Regression\n\n\n\nTerm\nEstimate\nStd. Error\nStatistic\nP-value\n\n\n\n\nIntercept\n30,046\n138\n218\n0\n\n\nX\n158.89\n5.81\n27.3\n2.58e-123\n\n\n\\(X^{2}\\)\n-1.50\n0.0564\n-26.6\n6.19e-118\n\n\n\nWhat is the marginal effect of \\(X\\) on \\(Y\\), when \\(X = 2\\)?\n\n\\[\n    \\widehat{\\dfrac{\\partial \\text{Y}}{\\partial \\text{X}} }\\Bigg|_{\\small \\text{X}=2} =\n    \\hat{\\beta}_{1} + 2 \\hat{\\beta}_{2} \\cdot (2) =\n    158.89 - 5.99 =\n    152.9\n\\]"
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/070-compile.html#quadratic-regression-4",
    "href": "lectures/07-Non-Linear-Models/070-compile.html#quadratic-regression-4",
    "title": "Non-Linear Models",
    "section": "Quadratic Regression",
    "text": "Quadratic Regression\n\n\n\nTerm\nEstimate\nStd. Error\nStatistic\nP-value\n\n\n\n\nIntercept\n30,046\n138\n218\n0\n\n\nX\n158.89\n5.81\n27.3\n2.58e-123\n\n\n\\(X^{2}\\)\n-1.50\n0.0564\n-26.6\n6.19e-118\n\n\n\nWhat is the marginal effect of \\(X\\) on \\(Y\\), when \\(X = 7\\)?\n\n\\[\n    \\widehat{\\dfrac{\\partial \\text{Y}}{\\partial \\text{X}} }\\Bigg|_{\\small \\text{X}=7} =\n    \\hat{\\beta}_{1} + 2 \\hat{\\beta}_{2} \\cdot (7) =\n    158.89 - 20.98 =\n    137.91\n\\]"
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/070-compile.html#fitted-regression-line",
    "href": "lectures/07-Non-Linear-Models/070-compile.html#fitted-regression-line",
    "title": "Non-Linear Models",
    "section": "Fitted Regression Line",
    "text": "Fitted Regression Line"
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/070-compile.html#marginal-effect-of-x-on-y",
    "href": "lectures/07-Non-Linear-Models/070-compile.html#marginal-effect-of-x-on-y",
    "title": "Non-Linear Models",
    "section": "Marginal Effect of \\(X\\) on \\(Y\\)",
    "text": "Marginal Effect of \\(X\\) on \\(Y\\)"
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/070-compile.html#quadratic-regression-5",
    "href": "lectures/07-Non-Linear-Models/070-compile.html#quadratic-regression-5",
    "title": "Non-Linear Models",
    "section": "Quadratic regression",
    "text": "Quadratic regression\nWhere does the regression \\(\\hat{Y_i} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i + \\hat{\\beta}_2 X_i^2\\) turn?\n\n\nStep 1: Take the derivative and set equal to zero.\n\\[\n\\widehat{\\dfrac{\\partial \\text{Y}}{\\partial \\text{X}} } = \\hat{\\beta}_1 + 2\\hat{\\beta}_2 X = 0\n\\]\n\n\nStep 1: Solve for \\(X\\).\n\\[\nX = -\\dfrac{\\hat{\\beta}_1}{2\\hat{\\beta}_2}\n\\]\n\n\nEx. Peak of previous regression occurs at \\(X = 53.02\\)."
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/070-compile.html#anscombes-quartet-aka-plot-your-data",
    "href": "lectures/07-Non-Linear-Models/070-compile.html#anscombes-quartet-aka-plot-your-data",
    "title": "Non-Linear Models",
    "section": "Anscombe’s Quartet AKA Plot Your Data",
    "text": "Anscombe’s Quartet AKA Plot Your Data\nFour “identical” regressions: Intercept \\(= 3\\), Slope \\(= 0.5\\), \\(R^{2} = 0.67\\)\n\nSame results, but with very different distributions only visible when you scatter them"
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/072-logarithms.html",
    "href": "lectures/07-Non-Linear-Models/072-logarithms.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "The natural log is the inverse function for the exponential function:\n\\[\n\\quad \\log(e^x) = x \\quad \\text{for} \\quad x&gt;0\n\\]\n. . .\n(Natural) Log rules:\n1. Product rule: \\(\\log(AB) = \\log(A) + \\log(B)\\).\n. . .\n2. Quotient rule: \\(\\log(A/B) = \\log(A) - \\log(B)\\).\n. . .\n3. Power rule: \\(\\log(A^B) = B \\cdot \\log(A)\\).\n. . .\n4. Derivative: \\(f(x) = \\log(x)\\) =&gt; \\(f'(x) = \\dfrac{1}{x}\\).\n. . .\nNote: \\(\\log(e) = 1\\), \\(\\log(1) = 0\\), and \\(\\log(x)\\) is undefined for \\(x \\leq 0\\)."
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/072-logarithms.html#were-going-to-take-logs",
    "href": "lectures/07-Non-Linear-Models/072-logarithms.html#were-going-to-take-logs",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "The natural log is the inverse function for the exponential function:\n\\[\n\\quad \\log(e^x) = x \\quad \\text{for} \\quad x&gt;0\n\\]\n. . .\n(Natural) Log rules:\n1. Product rule: \\(\\log(AB) = \\log(A) + \\log(B)\\).\n. . .\n2. Quotient rule: \\(\\log(A/B) = \\log(A) - \\log(B)\\).\n. . .\n3. Power rule: \\(\\log(A^B) = B \\cdot \\log(A)\\).\n. . .\n4. Derivative: \\(f(x) = \\log(x)\\) =&gt; \\(f'(x) = \\dfrac{1}{x}\\).\n. . .\nNote: \\(\\log(e) = 1\\), \\(\\log(1) = 0\\), and \\(\\log(x)\\) is undefined for \\(x \\leq 0\\)."
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/072-logarithms.html#log-linear-model",
    "href": "lectures/07-Non-Linear-Models/072-logarithms.html#log-linear-model",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Log-Linear model",
    "text": "Log-Linear model\nNonlinear Model\n\\[\nY_i = \\alpha e^{\\beta_1 X_i}v_i\n\\]\n\n\\(Y &gt; 0\\), \\(X\\) is continuous, and \\(v_i\\) is a multiplicative error term.\nCannot estimate parameters with OLS directly.\n\n. . .\n. . .\n\n\nLogarithmic Transformation\n\\[\n\\log(Y_i) = \\log(\\alpha) + \\beta_1 X_i + \\log(v_i)\n\\]\nRedefine \\(\\log(\\alpha) \\equiv \\beta_0\\), \\(\\log(v_i) \\equiv u_i\\).\n\nTransformed (Linear) Model\n\\[\n\\log(Y_i) = \\beta_0 + \\beta_1 X_i + u_i\n\\]\nCan estimate with OLS, but interpretation changes."
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/072-logarithms.html#log-linear-model-1",
    "href": "lectures/07-Non-Linear-Models/072-logarithms.html#log-linear-model-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Log-Linear model",
    "text": "Log-Linear model\nRegression Model\n\\[\n\\log(Y_i) = \\beta_0 + \\beta_1 X_i + u_i\n\\]\nInterpretation\n\nA one-unit increase in the explanatory variable increases the outcome variable by approximately \\(\\beta_1\\times 100\\) percent, on average.\n\n\nEx.\n\nIf \\(\\log(\\hat{\\text{Pay}_i}) = 2.9 + 0.03 \\cdot \\text{School}_i\\), then an additional year of schooling increases pay by approximately 3 percent, on average."
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/072-logarithms.html#log-linear-model-2",
    "href": "lectures/07-Non-Linear-Models/072-logarithms.html#log-linear-model-2",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Log-Linear model",
    "text": "Log-Linear model\nDerivation Consider the log-linear model\n\\[\n\\log(Y) = \\beta_0 + \\beta_1 \\, X + u\n\\]\nand differentiate\n\\[\n\\dfrac{dY}{Y} = \\beta_1 dX\n\\]\n. . .\nMarginal change in \\(X\\) (\\(dX\\)) leads to a \\(\\beta_1 dX\\) proportionate change in \\(Y\\).\n\nMultiply by 100 to get the percentage change in \\(Y\\)."
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/072-logarithms.html#log-linear-ex",
    "href": "lectures/07-Non-Linear-Models/072-logarithms.html#log-linear-ex",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Log-Linear Ex",
    "text": "Log-Linear Ex\n\\[\n    log(\\hat{Y}_{i}) = 10.02 + 0.73 \\cdot X_{i}\n\\]"
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/072-logarithms.html#log-linear-ex-1",
    "href": "lectures/07-Non-Linear-Models/072-logarithms.html#log-linear-ex-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Log-Linear Ex",
    "text": "Log-Linear Ex\n\\[\n    log(\\hat{Y}_{i}) = 10.02 + 0.73 \\cdot X_{i}\n\\]"
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/072-logarithms.html#log-linear",
    "href": "lectures/07-Non-Linear-Models/072-logarithms.html#log-linear",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Log-Linear",
    "text": "Log-Linear\nNote: If you have a log-linear model with a binary indicator variable, the interpretation of the coefficient on that variable changes. Consider\n. . .\n\\[\n\\log(Y_i) = \\beta_0 + \\beta_1 X_i + u_i\n\\]\nfor binary variable \\(X\\).\n. . .\nInterpretation of \\(\\beta_1\\):\n\nWhen \\(X\\) changes from 0 to 1, \\(Y\\) will increase by \\(100 \\times \\left( e^{\\beta_1} -1 \\right)\\)%\nWhen \\(X\\) changes from 1 to 0, \\(Y\\) will decrease by \\(100 \\times \\left( e^{-\\beta_1} -1 \\right)\\)%"
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/072-logarithms.html#log-linear-ex-2",
    "href": "lectures/07-Non-Linear-Models/072-logarithms.html#log-linear-ex-2",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Log-Linear Ex",
    "text": "Log-Linear Ex\nTake a binary explanatory variable: trained\n\ntrained = 1 if employee \\(i\\) received training\ntrained = 0 if employee \\(i\\) did not receive training\n\n\n\n\nTerm\nEstimate\nStd. Error\nStatistic\nP-value\n\n\n\n\nIntercept\n9.94\n0.0446\n223\n0\n\n\nTrained\n0.557\n0.0631\n8.83\n4.72e-18\n\n\n\nQ. How do we interpret the coefficient on trained?\n. . .\nA1: Trained workers are 74.52 percent more productive than untrained workers.\n. . .\nA2: Untrained workers are 42.7 percent less productive than trained workers."
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/072-logarithms.html#log-log-model",
    "href": "lectures/07-Non-Linear-Models/072-logarithms.html#log-log-model",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Log-Log model",
    "text": "Log-Log model\nNonlinear Model\n\\[\nY_i = \\alpha  X_i^{\\beta_1}v_i\n\\]\n\n\\(Y &gt; 0\\), \\(X &gt; 0\\), and \\(v_i\\) is a multiplicative error term.\nCannot estimate parameters with OLS directly.\n\n. . .\n\n\nLogarithmic Transformation\n\\[\n\\begin{align*}\n\\log(Y_i) = \\log(\\alpha) +& \\beta_1 \\log(X_i) \\\\\n                         +& \\log(v_i)\n\\end{align*}\n\\]\n\nRedefine \\(\\log(\\alpha) \\equiv \\beta_0\\), \\(\\log(v_i) \\equiv u_i\\).\n\n\nTransformed (Linear) Model\n\\[\n\\log(Y_i) = \\beta_0 + \\beta_1 \\log(X_i) + u_i\n\\]\nCan estimate with OLS, but interpretation changes."
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/072-logarithms.html#log-log-regression-model",
    "href": "lectures/07-Non-Linear-Models/072-logarithms.html#log-log-regression-model",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Log-Log regression model",
    "text": "Log-Log regression model\n\\[\n\\log(Y_i) = \\beta_0 + \\beta_1 \\log(X_i) + u_i\n\\]\nInterpretation\n\nA one-percent increase in the explanatory variable leads to a \\(\\beta_1\\)-percent change in the outcome variable, on average.\nOften interpreted as an elasticity.\n\n\nEx.\n\nIf \\(\\log(\\widehat{\\text{Quantity Demanded}}_i) = 0.45 - 0.31 \\cdot \\log(\\text{Income}_i)\\), then each one-percent increase in income decreases quantity demanded by 0.31 percent."
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/072-logarithms.html#log-log-derivation",
    "href": "lectures/07-Non-Linear-Models/072-logarithms.html#log-log-derivation",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Log-Log derivation",
    "text": "Log-Log derivation\nConsider the log-log model\n\\[\n\\log(Y_i) = \\beta_0 + \\beta_1 \\log(X_i) + u\n\\]\nand differentiate\n\\[\n\\dfrac{dY}{Y} = \\beta_1 \\dfrac{dX}{X}\n\\]\nA one-percent increase in \\(X\\) leads to a \\(\\beta_1\\)-percent increase in \\(Y\\).\n\nRearrange to show elasticity interpretation:\n\n\\[\n\\dfrac{dY}{dX} \\dfrac{X}{Y} = \\beta_1\n\\]"
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/072-logarithms.html#log-log-example",
    "href": "lectures/07-Non-Linear-Models/072-logarithms.html#log-log-example",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Log-Log Example",
    "text": "Log-Log Example\n\\[\n    log(\\hat{Y}_{i}) = 0.01 + 2.99 \\cdot log(X_{i})\n\\]"
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/072-logarithms.html#log-log-example-1",
    "href": "lectures/07-Non-Linear-Models/072-logarithms.html#log-log-example-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Log-Log Example",
    "text": "Log-Log Example\n\\[\n    log(\\hat{Y}_{i}) = 0.01 + 2.99 \\cdot log(X_{i})\n\\]"
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/072-logarithms.html#linear-log-model",
    "href": "lectures/07-Non-Linear-Models/072-logarithms.html#linear-log-model",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Linear-Log Model",
    "text": "Linear-Log Model\nNonlinear Model\n\\[\ne^{Y_i} = \\alpha  X_i^{\\beta_1}v_i\n\\]\n\n\\(X &gt; 0\\) and \\(v_i\\) is a multiplicative error term.\nCannot estimate parameters with OLS directly.\n\n. . .\n\n\nLogarithmic Transformation\n\\[\nY_i = \\log(\\alpha) + \\beta_1 \\log(X_i) + \\log(v_i)\n\\]\nRedefine \\(\\log(\\alpha) \\equiv \\beta_0\\), \\(\\log(v_i) \\equiv u_i\\).\n\nTransformed (Linear) Model\n\\[\nY_i = \\beta_0 + \\beta_1 \\log(X_i) + u_i\n\\]\nCan estimate with OLS, but interpretation changes."
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/072-logarithms.html#linear-log-model-1",
    "href": "lectures/07-Non-Linear-Models/072-logarithms.html#linear-log-model-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Linear-Log Model",
    "text": "Linear-Log Model\nRegression Model\n\\[\nY_i = \\beta_0 + \\beta_1 \\log(X_i) + u_i\n\\]\nInterpretation\n\nA one-percent increase in the explanatory variable increases the outcome variable by approximately \\(\\beta_1 \\div 100\\), on average.\n\n\nEx.\n\nIf \\(\\widehat{(\\text{Blood Pressure})_i} = 150 - 9.1 \\log(\\text{Income}_i)\\), then a one-percent increase in income decrease blood pressure by 0.091 points."
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/072-logarithms.html#linear-log-derivation",
    "href": "lectures/07-Non-Linear-Models/072-logarithms.html#linear-log-derivation",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Linear-Log derivation",
    "text": "Linear-Log derivation\nConsider the log-linear model\n\\[\nY = \\beta_0 + \\beta_1 \\log(X) + u\n\\]\nand differentiate\n\\[\ndY = \\beta_1 \\dfrac{dX}{X}\n\\]\n. . .\nA one-percent increase in \\(X\\) leads to a \\(\\beta_1 \\div 100\\) change in \\(Y\\)."
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/072-logarithms.html#linear-log-ex",
    "href": "lectures/07-Non-Linear-Models/072-logarithms.html#linear-log-ex",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Linear-Log Ex",
    "text": "Linear-Log Ex\n\\[\n    \\hat{Y}_{i} = 0 + 0.99 \\cdot log(X_{i})\n\\]"
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/072-logarithms.html#linear-log-ex-1",
    "href": "lectures/07-Non-Linear-Models/072-logarithms.html#linear-log-ex-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Linear-Log Ex",
    "text": "Linear-Log Ex\n\\[\n    \\hat{Y}_{i} = 0 + 0.99 \\cdot log(X_{i})\n\\]"
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/072-logarithms.html#approximate-coefficient-interpretation",
    "href": "lectures/07-Non-Linear-Models/072-logarithms.html#approximate-coefficient-interpretation",
    "title": "EC 320 - Intro. Econometrics",
    "section": "(Approximate) Coefficient Interpretation",
    "text": "(Approximate) Coefficient Interpretation\nFor Model Type:\n\n\nLinear-Linear \\(\\rightarrow Y_{i} = \\beta_{0} + \\beta_{1} X_{i} + u_{i}\\)\n\n\\(\\beta_{1} \\rightarrow \\Delta Y = \\beta_{1} \\cdot \\Delta X \\rightarrow\\) A one-unit increase in \\(X\\) leads to a \\(\\beta_{1}\\)-unit increase in \\(Y\\)\n\nLog-Linear \\(\\rightarrow log(Y_{i}) = \\beta_{0} + \\beta_{1} X_{i} + u_{i}\\)\n\n\\(\\beta_{1} \\rightarrow \\% \\Delta Y = 100 \\cdot \\beta_{1} \\cdot \\Delta X \\rightarrow\\) A one-unit increase in \\(X\\) leads to a \\(\\beta_{1} \\cdot 100\\)-percent increase in \\(Y\\)\n\nLog-Log \\(\\rightarrow log(Y_{i}) = \\beta_{0} + \\beta_{1} log(X_{i}) + u_{i}\\)\n\n\\(\\beta_{1} \\rightarrow \\% \\Delta Y = \\beta_{1} \\cdot \\% \\Delta X \\rightarrow\\) A one-percent increase in \\(X\\) leads to a \\(\\beta_{1}\\)-percent increase in \\(Y\\)\n\nLinear-Log \\(\\rightarrow Y_{i} = \\beta_{0} + \\beta_{1} log(X_{i}) + u_{i}\\)\n\n\\(\\beta_{1} \\rightarrow \\Delta Y = (\\beta_{1} \\div 100) \\cdot \\% \\Delta X \\rightarrow\\) A one-percent increase in \\(X\\) leads to a \\(\\beta_{1} \\div\\)-unit increase in \\(Y\\)"
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/072-logarithms.html#so-can-we-do-better",
    "href": "lectures/07-Non-Linear-Models/072-logarithms.html#so-can-we-do-better",
    "title": "EC 320 - Intro. Econometrics",
    "section": "So … Can we Do Better?",
    "text": "So … Can we Do Better?\n\\[\n(\\widehat{\\text{Life Expectancy})_i} = 53.96 + 8 \\times 10^{-4} \\cdot \\text{GDP}_i \\quad\\quad R^2 = 0.34\n\\]"
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/072-logarithms.html#so-can-we-do-better-1",
    "href": "lectures/07-Non-Linear-Models/072-logarithms.html#so-can-we-do-better-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "So … Can we Do Better?",
    "text": "So … Can we Do Better?\n\\[\nlog(\\widehat{\\text{Life Expectancy})_i} = 3.97 + 1.3 \\times 10^{-5} \\cdot \\text{GDP}_i \\quad\\quad R^2 = 0.3\n\\]"
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/072-logarithms.html#so-can-we-do-better-2",
    "href": "lectures/07-Non-Linear-Models/072-logarithms.html#so-can-we-do-better-2",
    "title": "EC 320 - Intro. Econometrics",
    "section": "So … Can we Do Better?",
    "text": "So … Can we Do Better?\n\\[\nlog(\\widehat{\\text{Life Expectancy})_i} = 2.86 + 0.15 \\cdot log(\\text{GDP}_i) \\quad\\quad R^2 = 0.61\n\\]"
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/072-logarithms.html#so-can-we-do-better-3",
    "href": "lectures/07-Non-Linear-Models/072-logarithms.html#so-can-we-do-better-3",
    "title": "EC 320 - Intro. Econometrics",
    "section": "So … Can we Do Better?",
    "text": "So … Can we Do Better?\n\\[\n(\\widehat{\\text{Life Expectancy})_i} = -9.1 + 8.41 \\cdot log(\\text{GDP}_i) \\quad\\quad R^2 = 0.65\n\\]"
  },
  {
    "objectID": "lectures/07-Non-Linear-Models/072-logarithms.html#practical-considerations",
    "href": "lectures/07-Non-Linear-Models/072-logarithms.html#practical-considerations",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Practical Considerations",
    "text": "Practical Considerations\nConsideration 01 Does your data take negative numbers or zeros as values?\n\n\\(log(0) = \\infty\\)\n\nConsideration 02 What coefficient intepretation do you want?\n\nUnit change? Unit-free percentage change?\n\nConsideration 03 Are your data skewed?"
  },
  {
    "objectID": "lectures/08-Categorical-Variables/080-compile.html#categorical-variables-1",
    "href": "lectures/08-Categorical-Variables/080-compile.html#categorical-variables-1",
    "title": "Categorical Variables and Interactions",
    "section": "Categorical Variables",
    "text": "Categorical Variables\nGoal Make quantitative statements about qualitative information.\n\ne.g., race, gender, being employed, living in Oregon, etc.\n\n\nApproach. Construct binary variables.\n\na.k.a. dummy variables or indicator variables.\nValue equals 1 if observation is in the category or 0 if otherwise.\n\n\n\nRegression implications.\n\nChange the interpretation of the intercept.\nChange the interpretations of the slope parameters."
  },
  {
    "objectID": "lectures/08-Categorical-Variables/080-compile.html#continuous-variables",
    "href": "lectures/08-Categorical-Variables/080-compile.html#continuous-variables",
    "title": "Categorical Variables and Interactions",
    "section": "Continuous Variables",
    "text": "Continuous Variables\nConsider the relationship\n\\[\n\\text{Pay}_i = \\beta_0 + \\beta_1 \\text{School}_i + u_i\n\\]\nwhere\n\n\\(\\text{Pay}_i\\) is a continuous variable measuring an individual’s pay\n\\(\\text{School}_i\\) is a continuous variable that measures years of education\n\n\nInterpretation\n\n\\(\\beta_0\\): \\(y\\)-intercept, i.e., \\(\\text{Pay}\\) when \\(\\text{School} = 0\\)\n\\(\\beta_1\\): expected increase in \\(\\text{Pay}\\) for a one-unit increase in \\(\\text{School}\\)"
  },
  {
    "objectID": "lectures/08-Categorical-Variables/080-compile.html#continuous-variables-1",
    "href": "lectures/08-Categorical-Variables/080-compile.html#continuous-variables-1",
    "title": "Categorical Variables and Interactions",
    "section": "Continuous Variables",
    "text": "Continuous Variables\nConsider the relationship\n\\[\n\\text{Pay}_i = \\beta_0 + \\beta_1 \\text{School}_i + u_i\n\\]\nAlternative derivation:\nDifferentiate the model with respect to schooling:\n\\[\n\\dfrac{\\partial \\text{Pay}}{\\partial \\text{School}} = \\beta_1\n\\]\nExpected increase in pay for an additional year of schooling"
  },
  {
    "objectID": "lectures/08-Categorical-Variables/080-compile.html#continuous-variables-2",
    "href": "lectures/08-Categorical-Variables/080-compile.html#continuous-variables-2",
    "title": "Categorical Variables and Interactions",
    "section": "Continuous Variables",
    "text": "Continuous Variables\nIf we have multiple explanatory variables, e.g.,\n\\[\n\\text{Pay}_i = \\beta_0 + \\beta_1 \\text{School}_i + \\beta_2 \\text{Ability}_i + u_i\n\\]\nthen the interpretation changes slightly.\n\nAlternative derivation\nDifferentiate the model with respect to schooling:\n\\[\n\\dfrac{\\partial\\text{Pay}}{\\partial\\text{School}} = \\beta_1\n\\]\nThe slope gives the expected increase in pay for an additional year of schooling, holding ability constant."
  },
  {
    "objectID": "lectures/08-Categorical-Variables/080-compile.html#categorical-variables-2",
    "href": "lectures/08-Categorical-Variables/080-compile.html#categorical-variables-2",
    "title": "Categorical Variables and Interactions",
    "section": "Categorical Variables",
    "text": "Categorical Variables\nConsider the relationship\n\\[\n\\text{Pay}_i = \\beta_0 + \\beta_1 \\text{Female}_i + u_i\n\\]\nwhere \\(\\text{Pay}_i\\) is a continuous variable measuring an individual’s pay and \\(\\text{Female}_i\\) is a binary variable equal to \\(1\\) when \\(i\\) is female.\n\nInterpretation of \\(\\beta_0\\)\n\\(\\beta_0\\) is the expected \\(\\text{Pay}\\) for males (i.e., when \\(\\text{Female} = 0\\)):\n\n\n\\[\n\\mathop{\\mathbb{E}}\\left[ \\text{Pay} | \\text{Male} \\right] = \\mathop{\\mathbb{E}}\\left[ \\beta_0 + \\beta_1\\times 0 + u_i \\right] = \\mathop{\\mathbb{E}}\\left[ \\beta_0 + 0 + u_i \\right] = \\beta_0\n\\]"
  },
  {
    "objectID": "lectures/08-Categorical-Variables/080-compile.html#categorical-variables-3",
    "href": "lectures/08-Categorical-Variables/080-compile.html#categorical-variables-3",
    "title": "Categorical Variables and Interactions",
    "section": "Categorical Variables",
    "text": "Categorical Variables\nConsider the relationship\n\\[\n\\text{Pay}_i = \\beta_0 + \\beta_1 \\text{Female}_i + u_i\n\\]\nwhere \\(\\text{Pay}_i\\) is a continuous variable measuring an individual’s pay and \\(\\text{Female}_i\\) is a binary variable equal to \\(1\\) when \\(i\\) is female.\n\nInterpretation of \\(\\beta_1\\)\n\\(\\beta_1\\) is the expected difference in \\(\\text{Pay}\\) between females and males:\n\n\n\\(\\mathop{\\mathbb{E}}\\left[ \\text{Pay} | \\text{Female} \\right] - \\mathop{\\mathbb{E}}\\left[ \\text{Pay} | \\text{Male} \\right]\\)\n\n\n\\(\\quad = \\mathop{\\mathbb{E}}\\left[ \\beta_0 + \\beta_1\\times 1 + u_i \\right] - \\mathop{\\mathbb{E}}\\left[ \\beta_0 + \\beta_1\\times 0 + u_i \\right]\\)\n\n\n\\(\\quad = \\mathop{\\mathbb{E}}\\left[ \\beta_0 + \\beta_1 + u_i \\right] - \\mathop{\\mathbb{E}}\\left[ \\beta_0 + 0 + u_i \\right]\\)\n\n\n\\(\\quad = \\beta_0 + \\beta_1 - \\beta_0\\) \\(\\quad = \\beta_1\\)"
  },
  {
    "objectID": "lectures/08-Categorical-Variables/080-compile.html#categorical-variables-4",
    "href": "lectures/08-Categorical-Variables/080-compile.html#categorical-variables-4",
    "title": "Categorical Variables and Interactions",
    "section": "Categorical Variables",
    "text": "Categorical Variables\nConsider the relationship\n\\[\n\\text{Pay}_i = \\beta_0 + \\beta_1 \\text{Female}_i + u_i\n\\]\nwhere \\(\\text{Pay}_i\\) is a continuous variable measuring an individual’s pay and \\(\\text{Female}_i\\) is a binary variable equal to \\(1\\) when \\(i\\) is female.\nInterpretation\n\\(\\beta_0 + \\beta_1\\): is the expected \\(\\text{Pay}\\) for females:\n\\(\\mathop{\\mathbb{E}}\\left[ \\text{Pay} | \\text{Female} \\right]\\)\n\n\\(\\quad = \\mathop{\\mathbb{E}}\\left[ \\beta_0 + \\beta_1\\times 1 + u_i \\right]\\)\n\n\n\\(\\quad = \\mathop{\\mathbb{E}}\\left[ \\beta_0 + \\beta_1 + u_i \\right]\\)\n\n\n\\(\\quad = \\beta_0 + \\beta_1\\)"
  },
  {
    "objectID": "lectures/08-Categorical-Variables/080-compile.html#categorical-variables-5",
    "href": "lectures/08-Categorical-Variables/080-compile.html#categorical-variables-5",
    "title": "Categorical Variables and Interactions",
    "section": "Categorical Variables",
    "text": "Categorical Variables\nConsider the relationship\n\\[\n\\text{Pay}_i = \\beta_0 + \\beta_1 \\text{Female}_i + u_i\n\\]\nInterpretation\n\n\\(\\beta_0\\): expected \\(\\text{Pay}\\) for males (i.e., when \\(\\text{Female} = 0\\))\n\\(\\beta_1\\): expected difference in \\(\\text{Pay}\\) between females and males\n\\(\\beta_0 + \\beta_1\\): expected \\(\\text{Pay}\\) for females\nMales are the reference group"
  },
  {
    "objectID": "lectures/08-Categorical-Variables/080-compile.html#categorical-variables-6",
    "href": "lectures/08-Categorical-Variables/080-compile.html#categorical-variables-6",
    "title": "Categorical Variables and Interactions",
    "section": "Categorical Variables",
    "text": "Categorical Variables\nConsider the relationship\n\\[\n\\text{Pay}_i = \\beta_0 + \\beta_1 \\text{Female}_i + u_i\n\\]\nNote. If there are no other variables to condition on, then \\(\\hat{\\beta}_1\\) equals the difference in group means, e.g., \\(\\bar{X}_\\text{Female} - \\bar{X}_\\text{Male}\\).\n\n\nNote2. The holding all other variables constant interpretation also applies for categorical variables in multiple regression settings."
  },
  {
    "objectID": "lectures/08-Categorical-Variables/080-compile.html#categorical-variables-7",
    "href": "lectures/08-Categorical-Variables/080-compile.html#categorical-variables-7",
    "title": "Categorical Variables and Interactions",
    "section": "Categorical Variables",
    "text": "Categorical Variables\n\\(Y_i = \\beta_0 + \\beta_1 X_i + u_i\\) for binary variable \\(X_i = \\{\\color{#434C5E}{0}, \\, {\\color{#B48EAD}{1}}\\}\\)"
  },
  {
    "objectID": "lectures/08-Categorical-Variables/080-compile.html#categorical-variables-8",
    "href": "lectures/08-Categorical-Variables/080-compile.html#categorical-variables-8",
    "title": "Categorical Variables and Interactions",
    "section": "Categorical Variables",
    "text": "Categorical Variables\n\\(Y_i = \\beta_0 + \\beta_1 X_i + u_i\\) for binary variable \\(X_i = \\{\\color{#434C5E}{0}, \\, {\\color{#B48EAD}{1}}\\}\\)"
  },
  {
    "objectID": "lectures/08-Categorical-Variables/080-compile.html#categorical-variables-9",
    "href": "lectures/08-Categorical-Variables/080-compile.html#categorical-variables-9",
    "title": "Categorical Variables and Interactions",
    "section": "Categorical Variables",
    "text": "Categorical Variables\n\\(Y_i = \\beta_0 + \\beta_1 X_i + u_i\\) for binary variable \\(X_i = \\{\\color{#434C5E}{0}, \\, {\\color{#B48EAD}{1}}\\}\\)"
  },
  {
    "objectID": "lectures/08-Categorical-Variables/080-compile.html#multiple-regression",
    "href": "lectures/08-Categorical-Variables/080-compile.html#multiple-regression",
    "title": "Categorical Variables and Interactions",
    "section": "Multiple Regression",
    "text": "Multiple Regression\n\\(Y_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + u_i \\quad\\) \\(X_1\\) is continuous \\(\\quad X_2\\) is categorical"
  },
  {
    "objectID": "lectures/08-Categorical-Variables/080-compile.html#multiple-regression-1",
    "href": "lectures/08-Categorical-Variables/080-compile.html#multiple-regression-1",
    "title": "Categorical Variables and Interactions",
    "section": "Multiple Regression",
    "text": "Multiple Regression\nThe intercept and categorical variable \\(X_2\\) control for the groups’ means."
  },
  {
    "objectID": "lectures/08-Categorical-Variables/080-compile.html#multiple-regression-2",
    "href": "lectures/08-Categorical-Variables/080-compile.html#multiple-regression-2",
    "title": "Categorical Variables and Interactions",
    "section": "Multiple Regression",
    "text": "Multiple Regression\nWith groups’ means removed"
  },
  {
    "objectID": "lectures/08-Categorical-Variables/080-compile.html#multiple-regression-3",
    "href": "lectures/08-Categorical-Variables/080-compile.html#multiple-regression-3",
    "title": "Categorical Variables and Interactions",
    "section": "Multiple Regression",
    "text": "Multiple Regression\n\\(\\hat{\\beta}_1\\) estimates the relationship between \\(Y\\) and \\(X_1\\) after controlling for \\(X_2\\)."
  },
  {
    "objectID": "lectures/08-Categorical-Variables/080-compile.html#multiple-regression-4",
    "href": "lectures/08-Categorical-Variables/080-compile.html#multiple-regression-4",
    "title": "Categorical Variables and Interactions",
    "section": "Multiple Regression",
    "text": "Multiple Regression\nAnother way to think about it: Regression by group"
  },
  {
    "objectID": "lectures/08-Categorical-Variables/080-compile.html#omitted-variable-bias",
    "href": "lectures/08-Categorical-Variables/080-compile.html#omitted-variable-bias",
    "title": "Categorical Variables and Interactions",
    "section": "Omitted variable bias",
    "text": "Omitted variable bias\nEx. Imagine a population model for the amount individual \\(i\\) gets paid\n\\[\n\\text{Pay}_i = \\beta_0 + \\beta_1 \\text{School}_i + \\beta_2 \\text{Male}_i + u_i\n\\]\nwhere \\(\\text{School}_i\\) gives \\(i\\)’s years of schooling and \\(\\text{Male}_i\\) denotes an indicator variable for whether individual \\(i\\) is male.\nInterpretation\n\n\\(\\beta_1\\): returns to an additional year of schooling (ceteris paribus)\n\\(\\beta_2\\): premium for being male (ceteris paribus)\n\n\n\nIf \\(\\beta_2 &gt; 0\\), then there is discrimination against women."
  },
  {
    "objectID": "lectures/08-Categorical-Variables/080-compile.html#omitted-variable-bias-1",
    "href": "lectures/08-Categorical-Variables/080-compile.html#omitted-variable-bias-1",
    "title": "Categorical Variables and Interactions",
    "section": "Omitted variable bias",
    "text": "Omitted variable bias\nEx. From the population model\n\\[\n\\text{Pay}_i = \\beta_0 + \\beta_1 \\text{School}_i + \\beta_2 \\text{Male}_i + u_i\n\\]\nAn analyst focuses on the relationship between pay and schooling, i.e.,\n\\[\n\\text{Pay}_i = \\beta_0 + \\beta_1 \\text{School}_i + \\left(\\beta_2 \\text{Male}_i + u_i\\right)\n\\] \\[\n\\text{Pay}_i = \\beta_0 + \\beta_1 \\text{School}_i + \\varepsilon_i\n\\]\nwhere \\(\\varepsilon_i = \\beta_2 \\text{Male}_i + u_i\\)."
  },
  {
    "objectID": "lectures/08-Categorical-Variables/080-compile.html#omitted-variable-bias-2",
    "href": "lectures/08-Categorical-Variables/080-compile.html#omitted-variable-bias-2",
    "title": "Categorical Variables and Interactions",
    "section": "Omitted variable bias",
    "text": "Omitted variable bias\nWe assumed exogeniety to show that OLS is unbiased.\nEven if \\(\\mathop{\\mathbb{E}}\\left[ u | X \\right] = 0\\), it is not necessarily true that \\(\\mathop{\\mathbb{E}}\\left[ \\varepsilon | X \\right] = 0\\)\n\n\nIf \\(\\beta_2 \\neq 0\\), then it is false\n\n\n\nSpecifically, if\n\\[\n\\mathop{\\mathbb{E}}\\left[ \\varepsilon | \\text{Male} = 1 \\right] = \\beta_2 + \\mathop{\\mathbb{E}}\\left[ u | \\text{Male} = 1 \\right] \\neq 0\n\\]\n\n\nThen, OLS is biased"
  },
  {
    "objectID": "lectures/08-Categorical-Variables/080-compile.html#omitted-variable-bias-3",
    "href": "lectures/08-Categorical-Variables/080-compile.html#omitted-variable-bias-3",
    "title": "Categorical Variables and Interactions",
    "section": "Omitted Variable Bias",
    "text": "Omitted Variable Bias\nLet’s try to see this result graphically.\nThe true population model:\n\\[\n\\text{Pay}_i = 20 + 0.5 \\times \\text{School}_i + 10 \\times \\text{Male}_i + u_i\n\\]\nThe regression model that suffers from omitted-variable bias:\n\\[\n\\text{Pay}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\times \\text{School}_i + e_i\n\\]\nSuppose that women, on average, receive more schooling than men."
  },
  {
    "objectID": "lectures/08-Categorical-Variables/080-compile.html#omitted-variable-bias-4",
    "href": "lectures/08-Categorical-Variables/080-compile.html#omitted-variable-bias-4",
    "title": "Categorical Variables and Interactions",
    "section": "Omitted Variable Bias",
    "text": "Omitted Variable Bias\nTrue model: \\(\\text{Pay}_i = 20 + 0.5 \\times \\text{School}_i + 10 \\times \\text{Male}_i + u_i\\)"
  },
  {
    "objectID": "lectures/08-Categorical-Variables/080-compile.html#omitted-variable-bias-5",
    "href": "lectures/08-Categorical-Variables/080-compile.html#omitted-variable-bias-5",
    "title": "Categorical Variables and Interactions",
    "section": "Omitted Variable Bias",
    "text": "Omitted Variable Bias\nBiased regression: \\(\\widehat{\\text{Pay}}_{i} = 31.3 - 0.9 \\times \\text{School}_{i}\\)"
  },
  {
    "objectID": "lectures/08-Categorical-Variables/080-compile.html#omitted-variable-bias-6",
    "href": "lectures/08-Categorical-Variables/080-compile.html#omitted-variable-bias-6",
    "title": "Categorical Variables and Interactions",
    "section": "Omitted Variable Bias",
    "text": "Omitted Variable Bias\nRecalling the omitted variable: Sex (female vs male)"
  },
  {
    "objectID": "lectures/08-Categorical-Variables/080-compile.html#omitted-variable-bias-7",
    "href": "lectures/08-Categorical-Variables/080-compile.html#omitted-variable-bias-7",
    "title": "Categorical Variables and Interactions",
    "section": "Omitted Variable Bias",
    "text": "Omitted Variable Bias\nRecalling the omitted variable: Sex (female vs male)"
  },
  {
    "objectID": "lectures/08-Categorical-Variables/080-compile.html#omitted-variable-bias-8",
    "href": "lectures/08-Categorical-Variables/080-compile.html#omitted-variable-bias-8",
    "title": "Categorical Variables and Interactions",
    "section": "Omitted Variable Bias",
    "text": "Omitted Variable Bias\nUnbiased Regression:\n\\[\n    \\widehat{\\text{Pay}}_{i} = 20.9 + 0.4 \\times \\text{School}_{i} + 9.1 \\times \\text{Male}_{i}\n\\]"
  },
  {
    "objectID": "lectures/08-Categorical-Variables/080-compile.html#motivation",
    "href": "lectures/08-Categorical-Variables/080-compile.html#motivation",
    "title": "Categorical Variables and Interactions",
    "section": "Motivation",
    "text": "Motivation\nRegression coefficients describe average effects. But for whom does on average mean?\n\n\nAverages can mask heterogeneous effects that differ by group or by the level of another variable.\n\n\n\nWe can use interaction terms to model heterogeneous effects, accommodating complexity and nuance by going beyond “the effect of \\(X\\) on \\(Y\\) is \\(\\beta_1\\).”"
  },
  {
    "objectID": "lectures/08-Categorical-Variables/080-compile.html#interaction-terms",
    "href": "lectures/08-Categorical-Variables/080-compile.html#interaction-terms",
    "title": "Categorical Variables and Interactions",
    "section": "Interaction Terms",
    "text": "Interaction Terms\nStarting point: \\(Y_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + u_i\\)\n\n\n\n\\(X_{1i}\\) is the variable of interest\n\n\n\n\\(X_{2i}\\) is a control variable\n\n\n\nA richer model: Interactions test whether \\(X_{2i}\\) moderates the effect of \\(X_{1i}\\)\n\\[\nY_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{1i} \\cdot X_{2i} + u_i\n\\]\n\n\nInterpretation: The partial derivative of \\(Y_i\\) with respect to \\(X_{1i}\\) is the marginal effect of \\(X_1\\) on \\(Y_i\\):\n\\[\n\\color{#81A1C1}{\\dfrac{\\partial Y}{\\partial X_1} = \\beta_1 + \\beta_3 X_{2i}}\n\\]\n\n\nThe effect of \\(X_1\\) depends on the level of \\(X_2\\) 🤯"
  },
  {
    "objectID": "lectures/08-Categorical-Variables/082-ovb.html",
    "href": "lectures/08-Categorical-Variables/082-ovb.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Ex. Imagine a population model for the amount individual \\(i\\) gets paid\n\\[\n\\text{Pay}_i = \\beta_0 + \\beta_1 \\text{School}_i + \\beta_2 \\text{Male}_i + u_i\n\\]\nwhere \\(\\text{School}_i\\) gives \\(i\\)’s years of schooling and \\(\\text{Male}_i\\) denotes an indicator variable for whether individual \\(i\\) is male.\nInterpretation\n\n\\(\\beta_1\\): returns to an additional year of schooling (ceteris paribus)\n\\(\\beta_2\\): premium for being male (ceteris paribus)\n\n. . .\n\nIf \\(\\beta_2 &gt; 0\\), then there is discrimination against women."
  },
  {
    "objectID": "lectures/08-Categorical-Variables/082-ovb.html#omitted-variable-bias",
    "href": "lectures/08-Categorical-Variables/082-ovb.html#omitted-variable-bias",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Ex. Imagine a population model for the amount individual \\(i\\) gets paid\n\\[\n\\text{Pay}_i = \\beta_0 + \\beta_1 \\text{School}_i + \\beta_2 \\text{Male}_i + u_i\n\\]\nwhere \\(\\text{School}_i\\) gives \\(i\\)’s years of schooling and \\(\\text{Male}_i\\) denotes an indicator variable for whether individual \\(i\\) is male.\nInterpretation\n\n\\(\\beta_1\\): returns to an additional year of schooling (ceteris paribus)\n\\(\\beta_2\\): premium for being male (ceteris paribus)\n\n. . .\n\nIf \\(\\beta_2 &gt; 0\\), then there is discrimination against women."
  },
  {
    "objectID": "lectures/08-Categorical-Variables/082-ovb.html#omitted-variable-bias-1",
    "href": "lectures/08-Categorical-Variables/082-ovb.html#omitted-variable-bias-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Omitted variable bias",
    "text": "Omitted variable bias\nEx. From the population model\n\\[\n\\text{Pay}_i = \\beta_0 + \\beta_1 \\text{School}_i + \\beta_2 \\text{Male}_i + u_i\n\\]\nAn analyst focuses on the relationship between pay and schooling, i.e.,\n\\[\n\\text{Pay}_i = \\beta_0 + \\beta_1 \\text{School}_i + \\left(\\beta_2 \\text{Male}_i + u_i\\right)\n\\] \\[\n\\text{Pay}_i = \\beta_0 + \\beta_1 \\text{School}_i + \\varepsilon_i\n\\]\nwhere \\(\\varepsilon_i = \\beta_2 \\text{Male}_i + u_i\\)."
  },
  {
    "objectID": "lectures/08-Categorical-Variables/082-ovb.html#omitted-variable-bias-2",
    "href": "lectures/08-Categorical-Variables/082-ovb.html#omitted-variable-bias-2",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Omitted variable bias",
    "text": "Omitted variable bias\nWe assumed exogeniety to show that OLS is unbiased.\nEven if \\(\\mathop{\\mathbb{E}}\\left[ u | X \\right] = 0\\), it is not necessarily true that \\(\\mathop{\\mathbb{E}}\\left[ \\varepsilon | X \\right] = 0\\)\n. . .\n\nIf \\(\\beta_2 \\neq 0\\), then it is false\n\n. . .\nSpecifically, if\n\\[\n\\mathop{\\mathbb{E}}\\left[ \\varepsilon | \\text{Male} = 1 \\right] = \\beta_2 + \\mathop{\\mathbb{E}}\\left[ u | \\text{Male} = 1 \\right] \\neq 0\n\\]\n. . .\nThen, OLS is biased"
  },
  {
    "objectID": "lectures/08-Categorical-Variables/082-ovb.html#omitted-variable-bias-3",
    "href": "lectures/08-Categorical-Variables/082-ovb.html#omitted-variable-bias-3",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Omitted Variable Bias",
    "text": "Omitted Variable Bias\nLet’s try to see this result graphically.\nThe true population model:\n\\[\n\\text{Pay}_i = 20 + 0.5 \\times \\text{School}_i + 10 \\times \\text{Male}_i + u_i\n\\]\nThe regression model that suffers from omitted-variable bias:\n\\[\n\\text{Pay}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\times \\text{School}_i + e_i\n\\]\nSuppose that women, on average, receive more schooling than men."
  },
  {
    "objectID": "lectures/08-Categorical-Variables/082-ovb.html#omitted-variable-bias-4",
    "href": "lectures/08-Categorical-Variables/082-ovb.html#omitted-variable-bias-4",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Omitted Variable Bias",
    "text": "Omitted Variable Bias\nTrue model: \\(\\text{Pay}_i = 20 + 0.5 \\times \\text{School}_i + 10 \\times \\text{Male}_i + u_i\\)"
  },
  {
    "objectID": "lectures/08-Categorical-Variables/082-ovb.html#omitted-variable-bias-5",
    "href": "lectures/08-Categorical-Variables/082-ovb.html#omitted-variable-bias-5",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Omitted Variable Bias",
    "text": "Omitted Variable Bias\nBiased regression: \\(\\widehat{\\text{Pay}}_{i} = 31.3 - 0.9 \\times \\text{School}_{i}\\)"
  },
  {
    "objectID": "lectures/08-Categorical-Variables/082-ovb.html#omitted-variable-bias-6",
    "href": "lectures/08-Categorical-Variables/082-ovb.html#omitted-variable-bias-6",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Omitted Variable Bias",
    "text": "Omitted Variable Bias\nRecalling the omitted variable: Sex (female vs male)"
  },
  {
    "objectID": "lectures/08-Categorical-Variables/082-ovb.html#omitted-variable-bias-7",
    "href": "lectures/08-Categorical-Variables/082-ovb.html#omitted-variable-bias-7",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Omitted Variable Bias",
    "text": "Omitted Variable Bias\nRecalling the omitted variable: Sex (female vs male)"
  },
  {
    "objectID": "lectures/08-Categorical-Variables/082-ovb.html#omitted-variable-bias-8",
    "href": "lectures/08-Categorical-Variables/082-ovb.html#omitted-variable-bias-8",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Omitted Variable Bias",
    "text": "Omitted Variable Bias\nUnbiased Regression:\n\\[\n    \\widehat{\\text{Pay}}_{i} = 20.9 + 0.4 \\times \\text{School}_{i} + 9.1 \\times \\text{Male}_{i}\n\\]"
  },
  {
    "objectID": "lectures/09-Fundamental-Prob-Causal-Inf/090-compile.html#statistics-inform-policy",
    "href": "lectures/09-Fundamental-Prob-Causal-Inf/090-compile.html#statistics-inform-policy",
    "title": "Fundamental Problem of Causal Inference",
    "section": "Statistics Inform Policy",
    "text": "Statistics Inform Policy\nPolicy: In 2017, the University of Oregon started requiring first-year students to live on campus.\n\nRationale: First-year students who live on campus fare better:\n\n80 percent more likely to graduate in four years.\nSecond-year retention rate 5 percentage points higher.\nGPAs 0.13 points higher, on average.\n\n\n\nDo these comparisons suggest the policy improves student outcomes?\n\n\nDo they describe the effect of living on campus?\n\n\nDo they describe something else?"
  },
  {
    "objectID": "lectures/09-Fundamental-Prob-Causal-Inf/090-compile.html#other-things-equal",
    "href": "lectures/09-Fundamental-Prob-Causal-Inf/090-compile.html#other-things-equal",
    "title": "Fundamental Problem of Causal Inference",
    "section": "Other Things Equal",
    "text": "Other Things Equal\nThe UO’s interpretation of those comparisons warrants skepticism.\n\nThe decision to live on campus is probably related to family wealth and interest in school.\nFamily wealth and interest in school are related to academic achievement.\n\n\n\nWhy might I be worried?\n\nThe difference in outcomes between those on and off campus is not an all else equal comparison.\n\n\nUpshot: Can’t attribute the difference in outcomes solely to living on campus."
  },
  {
    "objectID": "lectures/09-Fundamental-Prob-Causal-Inf/090-compile.html#all-else-equal",
    "href": "lectures/09-Fundamental-Prob-Causal-Inf/090-compile.html#all-else-equal",
    "title": "Fundamental Problem of Causal Inference",
    "section": "All else equal",
    "text": "All else equal\n\nCeteris paribus, all else held constant, etc.\n\n\n\nA high bar\nWhen all else equal, statistical comparisons detect causal relationships.\n(Micro)economics has developed a comparative advantage in understanding where all else equal comparisons can and cannot be made.\n\nAnyone can retort “correlation doesn’t necessarily imply causation.”\nUnderstanding why is difficult, but useful for learning from data."
  },
  {
    "objectID": "lectures/09-Fundamental-Prob-Causal-Inf/090-compile.html#causal-identification",
    "href": "lectures/09-Fundamental-Prob-Causal-Inf/090-compile.html#causal-identification",
    "title": "Fundamental Problem of Causal Inference",
    "section": "Causal identification",
    "text": "Causal identification\n\nGoal:\n\nIdentify the effect of a treatment, \\(D_i\\), on individual \\(i\\)’s outcome, \\(Y_{D,i}\\).\n\n\n\n\nIdeally, we could calculate the treatment effect for each individual \\(i\\) as\n\\[Y_{1,i} - Y_{0,i}\\]\n\n\\(Y_{1,i}\\): the outcome for person \\(i\\) when she receives the treatment.\n\\(Y_{0,i}\\): the outcome for person \\(i\\) when she doesn’t receive the treatment.\n\n\n\nReferred to as potential outcomes"
  },
  {
    "objectID": "lectures/09-Fundamental-Prob-Causal-Inf/090-compile.html#ideal-data",
    "href": "lectures/09-Fundamental-Prob-Causal-Inf/090-compile.html#ideal-data",
    "title": "Fundamental Problem of Causal Inference",
    "section": "Ideal data",
    "text": "Ideal data\n\n\nThe ideal data for 10 people\n\n\n\n\n\n\nCausal effect of treatment.\n\\[\n\\begin{align}\n  \\tau_i = Y_{1,i} -  Y_{0,i}\n\\end{align}\n\\] for each individual \\(i\\)."
  },
  {
    "objectID": "lectures/09-Fundamental-Prob-Causal-Inf/090-compile.html#ideal-data-1",
    "href": "lectures/09-Fundamental-Prob-Causal-Inf/090-compile.html#ideal-data-1",
    "title": "Fundamental Problem of Causal Inference",
    "section": "Ideal data",
    "text": "Ideal data\n\n\nThe ideal data for 10 people\n\n\n\n\n\n\nCausal effect of treatment.\n\\[\n\\begin{align}\n  \\tau_i = Y_{1,i} -  Y_{0,i}\n\\end{align}\n\\] for each individual \\(i\\).\nDefine the mean of \\(\\tau_i\\) as the average treatment effect (ATE)\n\\[\n\\color{#81A1C1}{\\overline{\\tau} = 3.82}\n\\]\n\n\nNotice the assignment of treatment is irrelevant in this setting."
  },
  {
    "objectID": "lectures/09-Fundamental-Prob-Causal-Inf/090-compile.html#ideal-comparison",
    "href": "lectures/09-Fundamental-Prob-Causal-Inf/090-compile.html#ideal-comparison",
    "title": "Fundamental Problem of Causal Inference",
    "section": "Ideal comparison",
    "text": "Ideal comparison\n\\[\n\\begin{align}\n  \\tau_i = \\color{#81A1C1}{Y_{1,i}} &- \\color{#B48EAD}{Y_{0,i}}\n\\end{align}\n\\]\nHighlights the fundamental problem of econometrics, much like when a traveler assesses options down two separate roads.\n\nThe problem\n\nIf we observe \\(\\color{#81A1C1}{Y_{1,i}}\\), then we cannot observe \\(\\color{#B48EAD}{Y_{0,i}}\\).\nIf we observe \\(\\color{#B48EAD}{Y_{0,i}}\\), then we cannot observe \\(\\color{#81A1C1}{Y_{1,i}}\\).\nWe do not observe the counterfactual."
  },
  {
    "objectID": "lectures/09-Fundamental-Prob-Causal-Inf/090-compile.html#counterfactual",
    "href": "lectures/09-Fundamental-Prob-Causal-Inf/090-compile.html#counterfactual",
    "title": "Fundamental Problem of Causal Inference",
    "section": "Counterfactual",
    "text": "Counterfactual\n\nHypothetical scenario representing the unobserved outcome for an individual or unit if they had experienced the alternative treatment or condition\n\n\nThe traveler’s alternative outcome is forever unknown to them."
  },
  {
    "objectID": "lectures/09-Fundamental-Prob-Causal-Inf/090-compile.html#fundamental-problem-of-causal-inference-1",
    "href": "lectures/09-Fundamental-Prob-Causal-Inf/090-compile.html#fundamental-problem-of-causal-inference-1",
    "title": "Fundamental Problem of Causal Inference",
    "section": "Fundamental problem of causal inference",
    "text": "Fundamental problem of causal inference\nA dataset that we can observe for 10 people looks something like\n\n\n\n\n\n\n\n\nWe can’t observe \\(\\color{#81A1C1}{Y_{1,i}}\\) and \\(\\color{#B48EAD}{Y_{0,i}}\\).\nBut, we do observe\n\n\\(\\color{#81A1C1}{Y_{1,i}}\\) for \\(i\\) in 1, 2, 3, 4, 5\n\\(\\color{#B48EAD}{Y_{0,i}}\\) for \\(i\\) in 6, 7, 8, 9, 10\n\n\n\nQ: How do we “fill in” the NAs and estimate \\(\\overline{\\tau}\\)? Or.\n\n\nQ: What is a good counterfactual for the missing data?"
  },
  {
    "objectID": "lectures/09-Fundamental-Prob-Causal-Inf/090-compile.html#estimating-causal-effects",
    "href": "lectures/09-Fundamental-Prob-Causal-Inf/090-compile.html#estimating-causal-effects",
    "title": "Fundamental Problem of Causal Inference",
    "section": "Estimating causal effects",
    "text": "Estimating causal effects\nNotation: \\(D_i\\) is a binary indicator variable such that\n\n\\(\\color{#81A1C1}{D_i=1}\\) if individual \\(\\color{#81A1C1}{i}\\) is treated.\n\\(\\color{#B48EAD}{D_i=0}\\) if individual \\(\\color{#B48EAD}{i}\\) is not treated (control group).\n\n\nThen, rephrasing the previous slide,\n\nWe only observe \\(\\color{#81A1C1}{Y_{1,i}}\\) when \\(\\color{#81A1C1}{D_{i}=1}\\).\nWe only observe \\(\\color{#B48EAD}{Y_{0,i}}\\) when \\(\\color{#B48EAD}{D_{i}=0}\\).\n\n\n\nQ: How can we estimate \\(\\overline{\\tau}\\) using only \\(\\left(\\color{#81A1C1}{Y_{1,i}|D_i=1}\\right)\\) and \\(\\left(\\color{#B48EAD}{Y_{0,i}|D_i=0}\\right)\\)?"
  },
  {
    "objectID": "lectures/09-Fundamental-Prob-Causal-Inf/090-compile.html#estimating-causal-effects-1",
    "href": "lectures/09-Fundamental-Prob-Causal-Inf/090-compile.html#estimating-causal-effects-1",
    "title": "Fundamental Problem of Causal Inference",
    "section": "Estimating causal effects",
    "text": "Estimating causal effects\nQ: How can we estimate \\(\\overline{\\tau}\\) using only \\(\\left(\\color{#81A1C1}{Y_{1,i}|D_i=1}\\right)\\) and \\(\\left(\\color{#B48EAD}{Y_{0,i}|D_i=0}\\right)\\)?\n\nIdea: What if we compare the group of \\(n\\) peoples’ means? I.e.,\n\\[\n\\begin{aligned}\n  =&\\color{#81A1C1}{E\\left( Y_i\\mid D_i = 1 \\right)} - \\color{#B48EAD}{E\\left( Y_i\\mid D_i =0 \\right)}\\\\\n  =&\\color{#81A1C1}{E\\left( Y_{1i}\\mid D_i = 1 \\right)} - \\color{#B48EAD}{E\\left( Y_{0i}\\mid D_i =0 \\right)}\n\\end{aligned}\n\\]\n\n\nQ: When does a simple difference-in-means provide information on the causal effect of the treatment?\n\n\nQ: Is \\(\\color{#81A1C1}{E\\left( Y_i\\mid D_i = 1 \\right)} - \\color{#B48EAD}{E\\left( Y_i\\mid D_i =0 \\right)}\\) a good estimator for \\(\\overline{\\tau}\\)?"
  },
  {
    "objectID": "lectures/09-Fundamental-Prob-Causal-Inf/090-compile.html#estimating-causal-effects-2",
    "href": "lectures/09-Fundamental-Prob-Causal-Inf/090-compile.html#estimating-causal-effects-2",
    "title": "Fundamental Problem of Causal Inference",
    "section": "Estimating causal effects",
    "text": "Estimating causal effects\nAssumption: Let \\(\\tau_i = \\tau\\) for all \\(i\\).\n\nThe treatment effect is equal (constant) across all individuals \\(i\\).\n\n\nNote: We defined\n\\[\n  \\tau_i = \\tau = \\color{#81A1C1}{Y_{1,i}} - \\color{#B48EAD}{Y_{0,i}}\n\\]\nwhich implies\n\\[\n   \\color{#81A1C1}{Y_{1,i}} = \\color{#B48EAD}{Y_{0,i}} + \\tau\n\\]"
  },
  {
    "objectID": "lectures/09-Fundamental-Prob-Causal-Inf/090-compile.html#estimating-causal-effects-3",
    "href": "lectures/09-Fundamental-Prob-Causal-Inf/090-compile.html#estimating-causal-effects-3",
    "title": "Fundamental Problem of Causal Inference",
    "section": "Estimating causal effects",
    "text": "Estimating causal effects\nOur proposed difference-in-means estimator\n\\[\n\\color{#434C5E}{\\text{Average causal effect}} + \\color{#B48EAD}{\\text{Selection bias}}\n\\]\ngives us the sum of\n\n\\(\\tau\\), the causal, average treatment effect that we want.\nSelection bias: How much treatment and control groups differ, on average."
  },
  {
    "objectID": "lectures/09-Fundamental-Prob-Causal-Inf/090-compile.html#mastering-metrics-video-on-selection-bias",
    "href": "lectures/09-Fundamental-Prob-Causal-Inf/090-compile.html#mastering-metrics-video-on-selection-bias",
    "title": "Fundamental Problem of Causal Inference",
    "section": "Mastering Metrics Video on Selection Bias",
    "text": "Mastering Metrics Video on Selection Bias"
  },
  {
    "objectID": "lectures/09-Fundamental-Prob-Causal-Inf/090-compile.html#selection-bias",
    "href": "lectures/09-Fundamental-Prob-Causal-Inf/090-compile.html#selection-bias",
    "title": "Fundamental Problem of Causal Inference",
    "section": "Selection Bias",
    "text": "Selection Bias\nProblem: Selection bias precludes all else equal comparisons.\n\nTo make causal statements, we need to shut down the bias term.\n\n\nPotential solution: Conduct an experiment.\n\nHow? Random assignment of treatment\nHence the name, randomized control trial (RCT).\n\n\n\nGroups will need to be large enough\n\nFollowing the LLN, as we increase \\(n\\) of both groups, our randomly assigned treatment estimate is more likely to be representative of the population mean."
  },
  {
    "objectID": "lectures/09-Fundamental-Prob-Causal-Inf/090-compile.html#randomized-control-trials",
    "href": "lectures/09-Fundamental-Prob-Causal-Inf/090-compile.html#randomized-control-trials",
    "title": "Fundamental Problem of Causal Inference",
    "section": "Randomized control trials",
    "text": "Randomized control trials\nEx. Effect of de-worming on attendance\n\nMotivation: Intestinal worms are common among children in less-developed countries. The parasitic symptoms disrupt human capital acquisition by keeping children home.\n\nPolicy Question: Do school-based de-worming interventions provide a cost-effective way to increase school attendance?"
  },
  {
    "objectID": "lectures/09-Fundamental-Prob-Causal-Inf/090-compile.html#randomized-control-trials-1",
    "href": "lectures/09-Fundamental-Prob-Causal-Inf/090-compile.html#randomized-control-trials-1",
    "title": "Fundamental Problem of Causal Inference",
    "section": "Randomized control trials",
    "text": "Randomized control trials\nEx. Effect of de-worming on attendance\n\nResearch Question: How much do de-worming interventions increase school attendance?\nQ: Could we simply compare average attendance among children with and without access to de-worming medication?\n\nA: If we’re after the causal effect, probably not."
  },
  {
    "objectID": "lectures/09-Fundamental-Prob-Causal-Inf/090-compile.html#randomized-control-trials-2",
    "href": "lectures/09-Fundamental-Prob-Causal-Inf/090-compile.html#randomized-control-trials-2",
    "title": "Fundamental Problem of Causal Inference",
    "section": "Randomized control trials",
    "text": "Randomized control trials\nEx. Effect of de-worming on attendance\n\nResearch Question: How much do de-worming interventions increase school attendance?\nQ: Why not?\n\nA: Selection bias – Families with access to de-worming medication probably have healthier children for other reasons (e.g. wealth, access to clean drinking water).1\n\nCan’t make an all else equal comparison. Biased and/or spurious results."
  },
  {
    "objectID": "lectures/09-Fundamental-Prob-Causal-Inf/090-compile.html#randomized-control-trials-3",
    "href": "lectures/09-Fundamental-Prob-Causal-Inf/090-compile.html#randomized-control-trials-3",
    "title": "Fundamental Problem of Causal Inference",
    "section": "Randomized Control Trials",
    "text": "Randomized Control Trials\nEx. Effect of de-worming on attendance\n\nImagine an RCT where we have two groups of villages:\n\nTreatment: Where children get de-worming medication in school.\nControl: Where children don’t get de-worming medication in school (status quo).\n\n\n\nBy randomizing, we will, on average, include all kinds of villages in both groups\n\npoor vs. less poor\naccess to clean water vs. contaminated water\nhospital vs. no hospital"
  },
  {
    "objectID": "lectures/09-Fundamental-Prob-Causal-Inf/090-compile.html#section",
    "href": "lectures/09-Fundamental-Prob-Causal-Inf/090-compile.html#section",
    "title": "Fundamental Problem of Causal Inference",
    "section": "",
    "text": "54 villages"
  },
  {
    "objectID": "lectures/09-Fundamental-Prob-Causal-Inf/090-compile.html#section-1",
    "href": "lectures/09-Fundamental-Prob-Causal-Inf/090-compile.html#section-1",
    "title": "Fundamental Problem of Causal Inference",
    "section": "",
    "text": "54 villages of varying levels of development"
  },
  {
    "objectID": "lectures/09-Fundamental-Prob-Causal-Inf/090-compile.html#section-2",
    "href": "lectures/09-Fundamental-Prob-Causal-Inf/090-compile.html#section-2",
    "title": "Fundamental Problem of Causal Inference",
    "section": "",
    "text": "54 villages of varying levels of development plus randomly assigned treatment"
  },
  {
    "objectID": "lectures/09-Fundamental-Prob-Causal-Inf/090-compile.html#section-3",
    "href": "lectures/09-Fundamental-Prob-Causal-Inf/090-compile.html#section-3",
    "title": "Fundamental Problem of Causal Inference",
    "section": "",
    "text": "54 villages of varying levels of development plus randomly assigned treatment"
  },
  {
    "objectID": "lectures/09-Fundamental-Prob-Causal-Inf/090-compile.html#section-4",
    "href": "lectures/09-Fundamental-Prob-Causal-Inf/090-compile.html#section-4",
    "title": "Fundamental Problem of Causal Inference",
    "section": "",
    "text": "54 villages of varying levels of development plus randomly assigned treatment"
  },
  {
    "objectID": "lectures/09-Fundamental-Prob-Causal-Inf/090-compile.html#section-5",
    "href": "lectures/09-Fundamental-Prob-Causal-Inf/090-compile.html#section-5",
    "title": "Fundamental Problem of Causal Inference",
    "section": "",
    "text": "54 villages of varying levels of development plus randomly assigned treatment"
  },
  {
    "objectID": "lectures/09-Fundamental-Prob-Causal-Inf/090-compile.html#section-6",
    "href": "lectures/09-Fundamental-Prob-Causal-Inf/090-compile.html#section-6",
    "title": "Fundamental Problem of Causal Inference",
    "section": "",
    "text": "54 villages of varying levels of development plus randomly assigned treatment"
  },
  {
    "objectID": "lectures/09-Fundamental-Prob-Causal-Inf/090-compile.html#randomized-control-trials-4",
    "href": "lectures/09-Fundamental-Prob-Causal-Inf/090-compile.html#randomized-control-trials-4",
    "title": "Fundamental Problem of Causal Inference",
    "section": "Randomized Control Trials",
    "text": "Randomized Control Trials\nEx. Effect of de-worming on attendance\nWe can estimate the causal effect of de-worming on school attendance by comparing the average attendance rates in the treatment group (💊) with those in the control group (no 💊).\n\\[\n\\begin{align}\n  \\overline{\\text{Attendance}}_\\text{Treatment} - \\overline{\\text{Attendance}}_\\text{Control}\n\\end{align}\n\\]\n\nAlternatively, we can use the regression\n\n\n\\[\n\\begin{align}\n  \\text{Attendance}_i = \\beta_0 + \\beta_1 \\text{Treatment}_i + u_i \\tag{1}\n\\end{align}\n\\]\nwhere \\(\\text{Treatment}_i\\) is a binary variable (=1 if village \\(i\\) received the de-worming treatment)."
  },
  {
    "objectID": "lectures/09-Fundamental-Prob-Causal-Inf/090-compile.html#randomized-control-trials-5",
    "href": "lectures/09-Fundamental-Prob-Causal-Inf/090-compile.html#randomized-control-trials-5",
    "title": "Fundamental Problem of Causal Inference",
    "section": "Randomized Control Trials",
    "text": "Randomized Control Trials\nEx. Effect of de-worming on attendance\n\\[\n\\begin{align}\n  \\text{Attendance}_i = \\beta_0 + \\beta_1 \\text{Treatment}_i + u_i \\tag{1}\n\\end{align}\n\\]\nwhere \\(\\text{Treatment}_i\\) is a binary variable (=1 if village \\(i\\) received the de-worming treatment).\nQ: Should trust the results of Eq. \\((1)\\)? Why?\n\nA: On average, randomly assigning treatment should balance treatment and control across the other dimensions that affect school attendance.\nBut we must always be cautious"
  },
  {
    "objectID": "lectures/09-Fundamental-Prob-Causal-Inf/092-fundamental.html",
    "href": "lectures/09-Fundamental-Prob-Causal-Inf/092-fundamental.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Goal:\n\nIdentify the effect of a treatment, \\(D_i\\), on individual \\(i\\)’s outcome, \\(Y_{D,i}\\).\n\n\n. . .\n\nIdeally, we could calculate the treatment effect for each individual \\(i\\) as\n\\[Y_{1,i} - Y_{0,i}\\]\n\n\\(Y_{1,i}\\): the outcome for person \\(i\\) when she receives the treatment.\n\\(Y_{0,i}\\): the outcome for person \\(i\\) when she doesn’t receive the treatment.\n\n. . .\nReferred to as potential outcomes"
  },
  {
    "objectID": "lectures/09-Fundamental-Prob-Causal-Inf/092-fundamental.html#causal-identification",
    "href": "lectures/09-Fundamental-Prob-Causal-Inf/092-fundamental.html#causal-identification",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Goal:\n\nIdentify the effect of a treatment, \\(D_i\\), on individual \\(i\\)’s outcome, \\(Y_{D,i}\\).\n\n\n. . .\n\nIdeally, we could calculate the treatment effect for each individual \\(i\\) as\n\\[Y_{1,i} - Y_{0,i}\\]\n\n\\(Y_{1,i}\\): the outcome for person \\(i\\) when she receives the treatment.\n\\(Y_{0,i}\\): the outcome for person \\(i\\) when she doesn’t receive the treatment.\n\n. . .\nReferred to as potential outcomes"
  },
  {
    "objectID": "lectures/09-Fundamental-Prob-Causal-Inf/092-fundamental.html#ideal-data",
    "href": "lectures/09-Fundamental-Prob-Causal-Inf/092-fundamental.html#ideal-data",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Ideal data",
    "text": "Ideal data\n\n\nThe ideal data for 10 people\n\n\n\n\n\n\nCausal effect of treatment.\n\\[\n\\begin{align}\n  \\tau_i = Y_{1,i} -  Y_{0,i}\n\\end{align}\n\\] for each individual \\(i\\)."
  },
  {
    "objectID": "lectures/09-Fundamental-Prob-Causal-Inf/092-fundamental.html#ideal-data-1",
    "href": "lectures/09-Fundamental-Prob-Causal-Inf/092-fundamental.html#ideal-data-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Ideal data",
    "text": "Ideal data\n\n\nThe ideal data for 10 people\n\n\n\n\n\n\nCausal effect of treatment.\n\\[\n\\begin{align}\n  \\tau_i = Y_{1,i} -  Y_{0,i}\n\\end{align}\n\\] for each individual \\(i\\).\nDefine the mean of \\(\\tau_i\\) as the average treatment effect (ATE)\n\\[\n\\color{#81A1C1}{\\overline{\\tau} = 3.82}\n\\]\n\n\n. . .\nNotice the assignment of treatment is irrelevant in this setting."
  },
  {
    "objectID": "lectures/09-Fundamental-Prob-Causal-Inf/092-fundamental.html#ideal-comparison",
    "href": "lectures/09-Fundamental-Prob-Causal-Inf/092-fundamental.html#ideal-comparison",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Ideal comparison",
    "text": "Ideal comparison\n\\[\n\\begin{align}\n  \\tau_i = \\color{#81A1C1}{Y_{1,i}} &- \\color{#B48EAD}{Y_{0,i}}\n\\end{align}\n\\]\nHighlights the fundamental problem of econometrics, much like when a traveler assesses options down two separate roads.\n. . .\nThe problem\n\nIf we observe \\(\\color{#81A1C1}{Y_{1,i}}\\), then we cannot observe \\(\\color{#B48EAD}{Y_{0,i}}\\).\nIf we observe \\(\\color{#B48EAD}{Y_{0,i}}\\), then we cannot observe \\(\\color{#81A1C1}{Y_{1,i}}\\).\nWe do not observe the counterfactual."
  },
  {
    "objectID": "lectures/09-Fundamental-Prob-Causal-Inf/092-fundamental.html#counterfactual",
    "href": "lectures/09-Fundamental-Prob-Causal-Inf/092-fundamental.html#counterfactual",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Counterfactual",
    "text": "Counterfactual\n\nHypothetical scenario representing the unobserved outcome for an individual or unit if they had experienced the alternative treatment or condition\n\n. . .\nThe traveler’s alternative outcome is forever unknown to them."
  },
  {
    "objectID": "lectures/09-Fundamental-Prob-Causal-Inf/092-fundamental.html#fundamental-problem-of-causal-inference",
    "href": "lectures/09-Fundamental-Prob-Causal-Inf/092-fundamental.html#fundamental-problem-of-causal-inference",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Fundamental problem of causal inference",
    "text": "Fundamental problem of causal inference\nA dataset that we can observe for 10 people looks something like\n\n\n\n\n\n\n\n\nWe can’t observe \\(\\color{#81A1C1}{Y_{1,i}}\\) and \\(\\color{#B48EAD}{Y_{0,i}}\\).\nBut, we do observe\n\n\\(\\color{#81A1C1}{Y_{1,i}}\\) for \\(i\\) in 1, 2, 3, 4, 5\n\\(\\color{#B48EAD}{Y_{0,i}}\\) for \\(i\\) in 6, 7, 8, 9, 10\n\n\n\n. . .\nQ: How do we “fill in” the NAs and estimate \\(\\overline{\\tau}\\)? Or.\n. . .\nQ: What is a good counterfactual for the missing data?"
  },
  {
    "objectID": "lectures/09-Fundamental-Prob-Causal-Inf/092-fundamental.html#estimating-causal-effects",
    "href": "lectures/09-Fundamental-Prob-Causal-Inf/092-fundamental.html#estimating-causal-effects",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Estimating causal effects",
    "text": "Estimating causal effects\nNotation: \\(D_i\\) is a binary indicator variable such that\n\n\\(\\color{#81A1C1}{D_i=1}\\) if individual \\(\\color{#81A1C1}{i}\\) is treated.\n\\(\\color{#B48EAD}{D_i=0}\\) if individual \\(\\color{#B48EAD}{i}\\) is not treated (control group).\n\n. . .\nThen, rephrasing the previous slide,\n\nWe only observe \\(\\color{#81A1C1}{Y_{1,i}}\\) when \\(\\color{#81A1C1}{D_{i}=1}\\).\nWe only observe \\(\\color{#B48EAD}{Y_{0,i}}\\) when \\(\\color{#B48EAD}{D_{i}=0}\\).\n\n. . .\nQ: How can we estimate \\(\\overline{\\tau}\\) using only \\(\\left(\\color{#81A1C1}{Y_{1,i}|D_i=1}\\right)\\) and \\(\\left(\\color{#B48EAD}{Y_{0,i}|D_i=0}\\right)\\)?"
  },
  {
    "objectID": "lectures/09-Fundamental-Prob-Causal-Inf/092-fundamental.html#estimating-causal-effects-1",
    "href": "lectures/09-Fundamental-Prob-Causal-Inf/092-fundamental.html#estimating-causal-effects-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Estimating causal effects",
    "text": "Estimating causal effects\nQ: How can we estimate \\(\\overline{\\tau}\\) using only \\(\\left(\\color{#81A1C1}{Y_{1,i}|D_i=1}\\right)\\) and \\(\\left(\\color{#B48EAD}{Y_{0,i}|D_i=0}\\right)\\)?\n. . .\nIdea: What if we compare the group of \\(n\\) peoples’ means? I.e.,\n\\[\n\\begin{aligned}\n  =&\\color{#81A1C1}{E\\left( Y_i\\mid D_i = 1 \\right)} - \\color{#B48EAD}{E\\left( Y_i\\mid D_i =0 \\right)}\\\\\n  =&\\color{#81A1C1}{E\\left( Y_{1i}\\mid D_i = 1 \\right)} - \\color{#B48EAD}{E\\left( Y_{0i}\\mid D_i =0 \\right)}\n\\end{aligned}\n\\]\n. . .\nQ: When does a simple difference-in-means provide information on the causal effect of the treatment?\n. . .\nQ: Is \\(\\color{#81A1C1}{E\\left( Y_i\\mid D_i = 1 \\right)} - \\color{#B48EAD}{E\\left( Y_i\\mid D_i =0 \\right)}\\) a good estimator for \\(\\overline{\\tau}\\)?"
  },
  {
    "objectID": "lectures/09-Fundamental-Prob-Causal-Inf/092-fundamental.html#estimating-causal-effects-2",
    "href": "lectures/09-Fundamental-Prob-Causal-Inf/092-fundamental.html#estimating-causal-effects-2",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Estimating causal effects",
    "text": "Estimating causal effects\nAssumption: Let \\(\\tau_i = \\tau\\) for all \\(i\\).\n\nThe treatment effect is equal (constant) across all individuals \\(i\\).\n\n. . .\nNote: We defined\n\\[\n  \\tau_i = \\tau = \\color{#81A1C1}{Y_{1,i}} - \\color{#B48EAD}{Y_{0,i}}\n\\]\nwhich implies\n\\[\n   \\color{#81A1C1}{Y_{1,i}} = \\color{#B48EAD}{Y_{0,i}} + \\tau\n\\]\n\nQ: Is \\(\\color{#81A1C1}{E\\left( Y_i\\mid D_i = 1 \\right)} \\color{#434C5E}{-} \\color{#B48EAD}{E\\left( Y_i\\mid D_i =0 \\right)}\\) a good estimator for \\(\\tau\\)?\n\n. . .\n\\(\\quad \\color{#ffffff}{\\Bigg|}=\\color{#81A1C1}{E\\left( Y_i\\mid D_i = 1 \\right)} \\color{#434C5E}{-} \\color{#B48EAD}{E\\left( Y_i\\mid D_i =0 \\right)}\\)\n. . .\n\\(\\quad \\color{#ffffff}{\\Bigg|}=\\color{#81A1C1}{E\\left( Y_{1,i}\\mid D_i = 1 \\right)} \\color{#434C5E}{-} \\color{#B48EAD}{E\\left( Y_{0,i}\\mid D_i =0 \\right)}\\)\n. . .\n\\(\\quad \\color{#ffffff}{\\Bigg|}=\\color{#B48EAD}{E\\left( \\color{#434C5E}{\\tau \\: +} \\: \\color{#B48EAD}{Y_{0,i}} \\mid D_i = 1 \\right)} \\color{#434C5E}{-} \\color{#B48EAD}{E\\left( Y_{0,i}\\mid D_i =0 \\right)}\\)\n. . .\n\\(\\quad \\color{#ffffff}{\\Bigg|}= \\color{#434C5E}{\\tau} \\color{#434C5E}{+} \\color{#B48EAD}{E\\left(\\color{#B48EAD}{Y_{0,i}} \\mid D_i = 1 \\right)} \\color{#434C5E}{-} \\color{#B48EAD}{E\\left( Y_{0,i}\\mid D_i =0 \\right)}\\)\n. . .\n\\(\\quad \\color{#ffffff}{\\Bigg|}= \\color{#434C5E}{\\text{Average causal effect}} + \\color{#B48EAD}{\\text{Selection bias}}\\)"
  },
  {
    "objectID": "lectures/09-Fundamental-Prob-Causal-Inf/092-fundamental.html#estimating-causal-effects-3",
    "href": "lectures/09-Fundamental-Prob-Causal-Inf/092-fundamental.html#estimating-causal-effects-3",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Estimating causal effects",
    "text": "Estimating causal effects\nOur proposed difference-in-means estimator\n\\[\n\\color{#434C5E}{\\text{Average causal effect}} + \\color{#B48EAD}{\\text{Selection bias}}\n\\]\ngives us the sum of\n\n\\(\\tau\\), the causal, average treatment effect that we want.\nSelection bias: How much treatment and control groups differ, on average."
  },
  {
    "objectID": "lectures/09-Fundamental-Prob-Causal-Inf/092-fundamental.html#mastering-metrics-video-on-selection-bias",
    "href": "lectures/09-Fundamental-Prob-Causal-Inf/092-fundamental.html#mastering-metrics-video-on-selection-bias",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Mastering Metrics Video on Selection Bias",
    "text": "Mastering Metrics Video on Selection Bias"
  },
  {
    "objectID": "lectures/10-Tibbles-and-lm/100-compile.html#preview",
    "href": "lectures/10-Tibbles-and-lm/100-compile.html#preview",
    "title": "Learning R: Tibbles and lm()",
    "section": "Preview",
    "text": "Preview\n\nIn this lecture, you will:\n\nLearn about holding data in tibbles (tidy tables)\nUse those tibbles to run regressions using the function lm()"
  },
  {
    "objectID": "lectures/10-Tibbles-and-lm/100-compile.html#what-is-a-tibble",
    "href": "lectures/10-Tibbles-and-lm/100-compile.html#what-is-a-tibble",
    "title": "Learning R: Tibbles and lm()",
    "section": "What is a tibble?",
    "text": "What is a tibble?\nThey are tidyverse spreadsheets\nData is still being held in vectors (column vectors specifically), but the rows of a tibble also hold meaning.\nThe rows are the observations while the columns are the variables\nLet’s look at an example"
  },
  {
    "objectID": "lectures/10-Tibbles-and-lm/100-compile.html#ex-daily-weather",
    "href": "lectures/10-Tibbles-and-lm/100-compile.html#ex-daily-weather",
    "title": "Learning R: Tibbles and lm()",
    "section": "Ex Daily Weather",
    "text": "Ex Daily Weather\nLet’s write down each day’s high temp, low temp, and rainfall. In words the data is:\n\nJan 01, 2023: We had a high of \\(46^{\\circ}\\), a low of \\(37^{\\circ}\\) and \\(0.07\\) in. of rain\nJan 02, 2023: We had a high of \\(46^{\\circ}\\), a low of \\(35^{\\circ}\\) and \\(0.00\\) in. of rain\nJan 03, 2023: We had a high of \\(47^{\\circ}\\), a low of \\(34^{\\circ}\\) and \\(0.08\\) in. of rain\n\n\n\nWhat should the observations (rows) be?\n\nEach day we went outside and observed the weather. So each day should have its own row.\n\nWhat are the variables (columns) we observe?\n\nThe date, the high temp, the low temp, rainfall"
  },
  {
    "objectID": "lectures/10-Tibbles-and-lm/100-compile.html#ex.-daily-weather",
    "href": "lectures/10-Tibbles-and-lm/100-compile.html#ex.-daily-weather",
    "title": "Learning R: Tibbles and lm()",
    "section": "Ex. Daily Weather",
    "text": "Ex. Daily Weather\nSo we want our tibble to look like:\n\n\n\nDate\nHigh Temp\nLow Temp\nRainfall\n\n\n\n\n1/1/23\n46\n37\n0.07\n\n\n1/2/23\n46\n35\n0.00\n\n\n1/3/23\n47\n34\n0.08\n\n\n\n\nThe mantra “observations as rows, variables as columns” is what we call the tidied data format\nThere are tons of ways you could format your data, but the tidyverse is compatible with only this way.\nLuckily, it turns out to be very experssible"
  },
  {
    "objectID": "lectures/10-Tibbles-and-lm/100-compile.html#ex.-daily-weather-1",
    "href": "lectures/10-Tibbles-and-lm/100-compile.html#ex.-daily-weather-1",
    "title": "Learning R: Tibbles and lm()",
    "section": "Ex. Daily Weather",
    "text": "Ex. Daily Weather\nHere’s the code to construct our tibble:\n\ntibble(\n    date = as.Date(c(\"2023-01-01\", \"2023-01-02\", \"2023-01-03\")),\n    high_temp = c(46, 46, 47),\n    low_temp = c(37, 35, 36),\n    rainfall = c(0.07, 0.00, 0.08)\n)\n\n# A tibble: 3 × 4\n  date       high_temp low_temp rainfall\n  &lt;date&gt;         &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 2023-01-01        46       37     0.07\n2 2023-01-02        46       35     0   \n3 2023-01-03        47       36     0.08\n\n\nHow it works:\n\nUse the function tibble()\ntibble() takes a list of vectors created with c() that become variable columns\nEach varible column has a name"
  },
  {
    "objectID": "lectures/10-Tibbles-and-lm/100-compile.html#rules-for-tibbles",
    "href": "lectures/10-Tibbles-and-lm/100-compile.html#rules-for-tibbles",
    "title": "Learning R: Tibbles and lm()",
    "section": "2 Rules for Tibbles",
    "text": "2 Rules for Tibbles\n1. Each Column Must be Named\n\nIf you try to define a column without giving it a name, tibble() generatses one for you (try it out yourself)\nWhen naming variables, avoid spaces between words\n\n2. Each Column Must Have the Same Number of Rows\n\nIf you try to define a column that is shorter than the others, tibble() will throw an error.\n\nException: If you define a column with only one element, tibble() will repeat it to make it the same length as the other columns."
  },
  {
    "objectID": "lectures/10-Tibbles-and-lm/100-compile.html#data-types",
    "href": "lectures/10-Tibbles-and-lm/100-compile.html#data-types",
    "title": "Learning R: Tibbles and lm()",
    "section": "Data Types",
    "text": "Data Types\n\nThere are 3 types of data that you will come across\n\nCross-sectional\nTime Series\nPanel\n\nImportantly, we cannot model different data types the same way.\nIn this class we build models of exclusively cross-sectional data."
  },
  {
    "objectID": "lectures/10-Tibbles-and-lm/100-compile.html#cross-sectional-data",
    "href": "lectures/10-Tibbles-and-lm/100-compile.html#cross-sectional-data",
    "title": "Learning R: Tibbles and lm()",
    "section": "Cross-Sectional Data",
    "text": "Cross-Sectional Data\n\nCross-sectional Data is data about many individuals at (around) the same time. Here, “individuals” might refer to people, or households, companies, cities, states, countries, etc.\n\n\ntibble(\n    name = c(\"Kris\", \"Kourtney\", \"Kim\"),\n    study_time = c(\"&lt; 2hrs\", \"2-5hrs\", \"&lt; 2hrs\"),\n    final_grade = c(69.4, 89.7, 66.3)\n)\n\n# A tibble: 3 × 3\n  name     study_time final_grade\n  &lt;chr&gt;    &lt;chr&gt;            &lt;dbl&gt;\n1 Kris     &lt; 2hrs            69.4\n2 Kourtney 2-5hrs            89.7\n3 Kim      &lt; 2hrs            66.3\n\n\nThe name “cross-sectional” comes from the fact that the data is a cross-section of some population. It is a snapshot in time of a smaple of individuals."
  },
  {
    "objectID": "lectures/10-Tibbles-and-lm/100-compile.html#time-series-data",
    "href": "lectures/10-Tibbles-and-lm/100-compile.html#time-series-data",
    "title": "Learning R: Tibbles and lm()",
    "section": "Time Series Data",
    "text": "Time Series Data\n\nTime Series Data is data that follows one specific individual (or household, company, city, state, country, etc.) over time. So this dataset would be a time series if it reports quiz grades over the course of a term for one student:\n\n\ntibble(\n    assignment = c(\"Quiz 01\", \"Quiz 02\", \"Quiz 03\", \"Quiz 04\", \"Quiz 05\"),\n    study_hours = c(4,3,2,3,8),\n    grade = c(75, 74, 69, 77, 89)\n)\n\n# A tibble: 5 × 3\n  assignment study_hours grade\n  &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt;\n1 Quiz 01              4    75\n2 Quiz 02              3    74\n3 Quiz 03              2    69\n4 Quiz 04              3    77\n5 Quiz 05              8    89"
  },
  {
    "objectID": "lectures/10-Tibbles-and-lm/100-compile.html#panel-data",
    "href": "lectures/10-Tibbles-and-lm/100-compile.html#panel-data",
    "title": "Learning R: Tibbles and lm()",
    "section": "Panel Data",
    "text": "Panel Data\nPanel Data describes many individuals over many periods of time. For example, many students’ scores over the course of a term would be panel data\n\ntibble(\n    name = rep(c(\"Kris\", \"Kourtney\", \"Kim\"), each = 3),\n    assignment = rep(c(\"Quiz 01\", \"Quiz 02\", \"Quiz 03\"), times = 3),\n    grade = c(75, 78, 66, 95, 97, 90, 62, 66, 54)\n)\n\n# A tibble: 9 × 3\n  name     assignment grade\n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt;\n1 Kris     Quiz 01       75\n2 Kris     Quiz 02       78\n3 Kris     Quiz 03       66\n4 Kourtney Quiz 01       95\n5 Kourtney Quiz 02       97\n6 Kourtney Quiz 03       90\n7 Kim      Quiz 01       62\n8 Kim      Quiz 02       66\n9 Kim      Quiz 03       54"
  },
  {
    "objectID": "lectures/10-Tibbles-and-lm/100-compile.html#lm",
    "href": "lectures/10-Tibbles-and-lm/100-compile.html#lm",
    "title": "Learning R: Tibbles and lm()",
    "section": "lm()",
    "text": "lm()\nNow we will use the function lm() to estimate a linear model\nlm() takes 2 important arguments:\n\nA formula created using the tilde symbol ~\nData (a tibble)\n\nlm() outputs an lm object:\n\nA bunch of information about the regression it just ran"
  },
  {
    "objectID": "lectures/10-Tibbles-and-lm/100-compile.html#ex-simple-regression",
    "href": "lectures/10-Tibbles-and-lm/100-compile.html#ex-simple-regression",
    "title": "Learning R: Tibbles and lm()",
    "section": "Ex Simple Regression",
    "text": "Ex Simple Regression\nWe create the following dataset and pipe it into the lm() to run the regression\n\ntibble(\n    x = 1:3,\n    y = c(4, 2, 1)\n) %&gt;%\n    lm(y ~ x, data = .)\n\n\nCall:\nlm(formula = y ~ x, data = .)\n\nCoefficients:\n(Intercept)            x  \n      5.333       -1.500  \n\n\nNote that we get our standard regression coefficients \\(\\hat{\\beta}_{0}\\) and \\(\\hat{\\beta}_{1}\\)\n\nWe can also unpack the lm object in lots of different ways."
  },
  {
    "objectID": "lectures/10-Tibbles-and-lm/100-compile.html#ex-simple-regression-1",
    "href": "lectures/10-Tibbles-and-lm/100-compile.html#ex-simple-regression-1",
    "title": "Learning R: Tibbles and lm()",
    "section": "Ex Simple Regression",
    "text": "Ex Simple Regression\nLet’s get the residuals using the residuals() function\n\ntibble(\n    x = 1:3,\n    y = c(4, 2, 1)\n) %&gt;% \n    lm(y ~ x, data = .) %&gt;%\n    residuals()\n\n         1          2          3 \n 0.1666667 -0.3333333  0.1666667 \n\n\n\nWe can also get the fitted values using fitted.values()\n\ntibble(\n    x = 1:3,\n    y = c(4, 2, 1)\n) %&gt;%\n    lm(y ~ x, data = .) %&gt;%\n    fitted.values()\n\n        1         2         3 \n3.8333333 2.3333333 0.8333333"
  },
  {
    "objectID": "lectures/10-Tibbles-and-lm/100-compile.html#practice-download-worksheet-02-from-the-site",
    "href": "lectures/10-Tibbles-and-lm/100-compile.html#practice-download-worksheet-02-from-the-site",
    "title": "Learning R: Tibbles and lm()",
    "section": "Practice: Download “Worksheet 02” From the Site",
    "text": "Practice: Download “Worksheet 02” From the Site\nThis worksheet will help you learn coding by doing. You will:\n\nConstruct your own tibble\n&lt;- Assign it to a variable name in your environment\nview() it in a separate tab\nFind dimensions of the data\nFind variable names\nAdd new observations\nRun a regression and practice interpreting results"
  },
  {
    "objectID": "lectures/10-Tibbles-and-lm/102-lm.html",
    "href": "lectures/10-Tibbles-and-lm/102-lm.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Now we will use the function lm() to estimate a linear model\nlm() takes 2 important arguments:\n\nA formula created using the tilde symbol ~\nData (a tibble)\n\nlm() outputs an lm object:\n\nA bunch of information about the regression it just ran"
  },
  {
    "objectID": "lectures/10-Tibbles-and-lm/102-lm.html#lm",
    "href": "lectures/10-Tibbles-and-lm/102-lm.html#lm",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "Now we will use the function lm() to estimate a linear model\nlm() takes 2 important arguments:\n\nA formula created using the tilde symbol ~\nData (a tibble)\n\nlm() outputs an lm object:\n\nA bunch of information about the regression it just ran"
  },
  {
    "objectID": "lectures/10-Tibbles-and-lm/102-lm.html#ex-simple-regression",
    "href": "lectures/10-Tibbles-and-lm/102-lm.html#ex-simple-regression",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Ex Simple Regression",
    "text": "Ex Simple Regression\nWe create the following dataset and pipe it into the lm() to run the regression\n\ntibble(\n    x = 1:3,\n    y = c(4, 2, 1)\n) %&gt;%\n    lm(y ~ x, data = .)\n\n\nCall:\nlm(formula = y ~ x, data = .)\n\nCoefficients:\n(Intercept)            x  \n      5.333       -1.500  \n\n\nNote that we get our standard regression coefficients \\(\\hat{\\beta}_{0}\\) and \\(\\hat{\\beta}_{1}\\)\n\nWe can also unpack the lm object in lots of different ways."
  },
  {
    "objectID": "lectures/10-Tibbles-and-lm/102-lm.html#ex-simple-regression-1",
    "href": "lectures/10-Tibbles-and-lm/102-lm.html#ex-simple-regression-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Ex Simple Regression",
    "text": "Ex Simple Regression\nLet’s get the residuals using the residuals() function\n\ntibble(\n    x = 1:3,\n    y = c(4, 2, 1)\n) %&gt;% \n    lm(y ~ x, data = .) %&gt;%\n    residuals()\n\n         1          2          3 \n 0.1666667 -0.3333333  0.1666667 \n\n\n\nWe can also get the fitted values using fitted.values()\n\ntibble(\n    x = 1:3,\n    y = c(4, 2, 1)\n) %&gt;%\n    lm(y ~ x, data = .) %&gt;%\n    fitted.values()\n\n        1         2         3 \n3.8333333 2.3333333 0.8333333"
  },
  {
    "objectID": "lectures/11-Dplyr/111-functions.html",
    "href": "lectures/11-Dplyr/111-functions.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "dplyr is an amazing tool because, with just 7 main functions, you can answer just about any question you could possibly have about your data\n\n\nIf you have any data science experience, you will quickly realize that dplyr is nothing new.\nIt is actually just SQL, a language for querying databases that has been around since the ’70s.\nWe will not be write any SQL but you should know that by learning dplyr, you are also learning SQL.\nThe graphics for the functions were created by Colleen O’Briant"
  },
  {
    "objectID": "lectures/11-Dplyr/111-functions.html#dplyr-functions",
    "href": "lectures/11-Dplyr/111-functions.html#dplyr-functions",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "dplyr is an amazing tool because, with just 7 main functions, you can answer just about any question you could possibly have about your data\n\n\nIf you have any data science experience, you will quickly realize that dplyr is nothing new.\nIt is actually just SQL, a language for querying databases that has been around since the ’70s.\nWe will not be write any SQL but you should know that by learning dplyr, you are also learning SQL.\nThe graphics for the functions were created by Colleen O’Briant"
  },
  {
    "objectID": "lectures/12-ggplot/121-recipes.html",
    "href": "lectures/12-ggplot/121-recipes.html",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "We need some data.\nAs you’ll see, the type of plot you will want to draw depends crucially on the type of data that you have.\n. . .\nSuppose we have a tibble called students that looks like this:\n\n\n\nsex\nstudy_time\ngrade1\nfinal_grade\n\n\n\n\nfemale\n5-10 hrs\n94.9\n95.4\n\n\nmale\n2-5 hrs\n79.6\n73.7\n\n\nfemale\n5-10 hrs\n64.2\n49.1\n\n\n. . .\n. . .\n. . .\n. . .\n\n\n\nWhere study_time is hours per week studying math, grade1 is their first-semester math grade, and final_grade is their final grade in the math course"
  },
  {
    "objectID": "lectures/12-ggplot/121-recipes.html#but-first",
    "href": "lectures/12-ggplot/121-recipes.html#but-first",
    "title": "EC 320 - Intro. Econometrics",
    "section": "",
    "text": "We need some data.\nAs you’ll see, the type of plot you will want to draw depends crucially on the type of data that you have.\n. . .\nSuppose we have a tibble called students that looks like this:\n\n\n\nsex\nstudy_time\ngrade1\nfinal_grade\n\n\n\n\nfemale\n5-10 hrs\n94.9\n95.4\n\n\nmale\n2-5 hrs\n79.6\n73.7\n\n\nfemale\n5-10 hrs\n64.2\n49.1\n\n\n. . .\n. . .\n. . .\n. . .\n\n\n\nWhere study_time is hours per week studying math, grade1 is their first-semester math grade, and final_grade is their final grade in the math course"
  },
  {
    "objectID": "lectures/12-ggplot/121-recipes.html#but-first-1",
    "href": "lectures/12-ggplot/121-recipes.html#but-first-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "But First…",
    "text": "But First…\n\n\n\nsex\nstudy_time\ngrade1\nfinal_grade\n\n\n\n\nfemale\n5-10 hrs\n94.9\n95.4\n\n\nmale\n2-5 hrs\n79.6\n73.7\n\n\nfemale\n5-10 hrs\n64.2\n49.1\n\n\n. . .\n. . .\n. . .\n. . .\n\n\n\nWe have some variables which are categorical (sex takes either “male” or “female” and study_time takes on 0-2 hrs, 2-5 hrs, etc.).\nWe have other variables which are numeric, and in particular, continuous (grade1 and final_grade can take any value between 0 and 100)\nThese being different “type” of variables means we use different “recipes” to visualize them"
  },
  {
    "objectID": "lectures/12-ggplot/121-recipes.html#data-and-quick-ggplot-syntax",
    "href": "lectures/12-ggplot/121-recipes.html#data-and-quick-ggplot-syntax",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Data and quick ggplot() syntax",
    "text": "Data and quick ggplot() syntax\nI will be using the dataset you can find below the lecture, named students\nDownload it, open RStudio and in your terminal load it using:\n\nload(\"./students-data.Rdata\")\n\nThe exact path will change depending on where it is stored on your device. I recommend right-clicking on the file itself and copying the path shown in the properties.\n\nNote: This is not necessary for you to do. It’s just if you want to follow along"
  },
  {
    "objectID": "lectures/12-ggplot/121-recipes.html#recipe-1-bar-plots",
    "href": "lectures/12-ggplot/121-recipes.html#recipe-1-bar-plots",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Recipe 1: Bar Plots",
    "text": "Recipe 1: Bar Plots\ngeom_bar()\nQ: How many hours do students spend studying?\n\nDistrubtion of a single discrete variable \\(\\rightarrow\\) Use a Bar Plot\n\n\nstudents %&gt;% \n    ggplot(aes(x = study_time)) + \n    geom_bar()\n\n\nHere I pipe the tibble into the function ggplot()\nggplot() needs an aesthetic mapping. You need to tell it which variables in your dataset map to which visual aesthetic in the plot.\nAfter the ggplot() call, add any extra layer with +\ngeom_bar() draws the bar plot using the previous instructions\n\n\n\n\nstudents %&gt;% \n    ggplot(aes(x = study_time)) + \n    geom_bar()\n\n\n\n\n\n\n\n\n\nWhich admittedly looks ugly but we can spice it up later"
  },
  {
    "objectID": "lectures/12-ggplot/121-recipes.html#recipe-2-histograms",
    "href": "lectures/12-ggplot/121-recipes.html#recipe-2-histograms",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Recipe 2: Histograms",
    "text": "Recipe 2: Histograms\ngeom_histogram()\nQ: What is the grade distribution?\n\nDistribution of a single continuous variable \\(\\rightarrow\\) use a histogram\n\n. . .\nThe recipe remains largely the same: Pipe the data into ggplot(), then ggplot() needs an aesthetic mapping which is wrapped in aes().\nThen use the specific geom_histogram() to draw the histogram.\n\nstudents %&gt;%\n    ggplot(aes(x = final_grade)) + \n    geom_histogram()\n\n\n\n\nstudents %&gt;%\n    ggplot(aes(x = final_grade)) + \n    geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "lectures/12-ggplot/121-recipes.html#recipe-3-box-plots",
    "href": "lectures/12-ggplot/121-recipes.html#recipe-3-box-plots",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Recipe 3: Box Plots",
    "text": "Recipe 3: Box Plots\ngeom_boxplot()\nQ. Do students who study more (discrete) earn higher grades (continuous)?\n\nBegin by piping the data into ggplot()\nProvide ggplot() the aesthetic wrapped in aes()\nThis time we have 2 variables: one for each axis\n\n\nstudents %&gt;%\n    ggplot(aes(x = study_time, y = final_grade)) + \n    geom_boxplot()"
  },
  {
    "objectID": "lectures/12-ggplot/121-recipes.html#recipe-4-scatterplots",
    "href": "lectures/12-ggplot/121-recipes.html#recipe-4-scatterplots",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Recipe 4: Scatterplots",
    "text": "Recipe 4: Scatterplots\ngeom_point()\nQ. How well does a student’s first-semester grade predict their final grade in a (high school) class?\n\nstudents %&gt;%\n    ggplot(aes(x = grade1, y = final_grade)) + \n    geom_point()\n\n\n\nstudents %&gt;%\n    ggplot(aes(x = grade1, y = final_grade)) + \n    geom_point()\n\n\n\n\n\n\n\n\nWhich looks fine, but what about adding an additional layer: a best fit line\n\n\nstudents %&gt;%\n    ggplot(aes(x = grade1, y = final_grade)) + \n    geom_point() + \n    geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nWe can further modify our geom_smooth line to make it a linear model using method = \"lm\" and also remove the predicted standard errors se=FALSE\n\n\nstudents %&gt;%\n    ggplot(aes(x = grade1, y = final_grade)) + \n    geom_point() + \n    geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "lectures/12-ggplot/121-recipes.html#recipe-5-bar-plots",
    "href": "lectures/12-ggplot/121-recipes.html#recipe-5-bar-plots",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Recipe 5: Bar Plots",
    "text": "Recipe 5: Bar Plots\ngeom_bar()\nQ. Do females report studying for longer than males?\nRelationship between time studied and sex\nWe can do this in two different ways:\n\n\nMake a bar plot for each category (sex) which creates separate plots\n\n\nstudents %&gt;% \n    ggplot(aes(x = study_time)) +\n    geom_bar() +\n    facet_wrap(~ sex) # ~ indicates a formula is coming"
  },
  {
    "objectID": "lectures/12-ggplot/121-recipes.html#recipe-5-bar-plots-1",
    "href": "lectures/12-ggplot/121-recipes.html#recipe-5-bar-plots-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Recipe 5: Bar Plots",
    "text": "Recipe 5: Bar Plots\ngeom_bar()\nQ. Do females report studying for longer than males?\nRelationship between time studied and sex\nWe can do this in two different ways:\n\n\nTry a fill aesthetic mapping to color in the bars using seperate colors for males and females\n\n\nstudents %&gt;%\n    ggplot(aes(x = study_time, fill = sex)) + # Be sure to include the fill inside the aes()\n    geom_bar(position = 'dodge') # Use position = \"dodge\" to set bars next to each other. The default is to stack them on top of each other"
  },
  {
    "objectID": "lectures/12-ggplot/121-recipes.html#a-few-more-tools",
    "href": "lectures/12-ggplot/121-recipes.html#a-few-more-tools",
    "title": "EC 320 - Intro. Econometrics",
    "section": "A Few More Tools",
    "text": "A Few More Tools\nSay we want to show a scatterplot and have it differentiate amongst categories using different colors. It is very similar to fill =, but instead we use color =.\n\nlibrary(gapminder)\n\ngapminder %&gt;%\n    ggplot(aes(x = gdpPercap, y = lifeExp, color = continent)) + \n    geom_point()"
  },
  {
    "objectID": "lectures/12-ggplot/121-recipes.html#a-few-more-tools-1",
    "href": "lectures/12-ggplot/121-recipes.html#a-few-more-tools-1",
    "title": "EC 320 - Intro. Econometrics",
    "section": "A Few More Tools",
    "text": "A Few More Tools"
  },
  {
    "objectID": "lectures/12-ggplot/121-recipes.html#summary",
    "href": "lectures/12-ggplot/121-recipes.html#summary",
    "title": "EC 320 - Intro. Econometrics",
    "section": "Summary",
    "text": "Summary\n\nAesthetic mappings get wrapped in aes() and map variables in your tibble to aesthetics in your plot like which variable gets drawn on the x-axis, which goes on the y-axis, and which variable is represented in color\nGeoms are added to the plot using + as layers"
  }
]