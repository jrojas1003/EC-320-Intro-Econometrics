---
name: linear model
---

## Sidebar: Summation Rules {.tiny}

Before we continue, let's cover some important rules we will need to derive some OLS things in the near future:

Summations $(\sum)$ have certain rules that we cannot violate and are important to hold in mind:

<br>

1. $\sum_{i=1}^{n} x_{i} = x_{1} + x_{2} + \cdots + x_{n}$

<br>

2. $\sum_{i} x_{i} + y_{i} = \sum_{i} x_{i} + \sum_{i} y_{i}$

<br>

3. $\sum_{i} x_{i} y_{i} \neq \sum_{i} x_{i} \sum_{i} y_{i}$

<br>

---

## Summation Rules

$$\sum_{i=1}^{n} x_{i} = x_{1} + x_{2} + \cdots + x_{n}$$

Let $x$ be the set of ${1,5,2}$ $(x: \{1,5,2\})$ then using our summation rule we have:

$$
    \sum_{i} x_{i} = 1 + 5 + 2 = 8.
$$

---

## Summation Rules

$$\sum_{i} x_{i} + y_{i} = \sum_{i} x_{i} + \sum_{i} y_{i}$$

Let $x: \{1,5,2\}$ and $y: \{1,2,1\}$, then using our summation rule we have:

\begin{align*}
    \sum_{i} x_{i} + y_{i} &= x_{1} + y_{1} + x_{2} + y_{2} + x_{3} + y_{3} \\
                           &= x_{1} + x_{2} + x_{3} + y_{1} + y_{2} + y_{3} \\
                           &= 1 + 5 + 2 + 1 + 2 + 1 \\
                           &= 12
\end{align*}

---

## Summation Rules

$$\sum_{i} x_{i} y_{i} \neq \sum_{i} x_{i} \sum_{i} y_{i}$$

If we expand $\sum_{i} x_{i} y_{i} \neq \sum_{i} x_{i} \sum_{i} y_{i}$, we get:

<br>

\begin{align*}
    x_{1}y_{1} + x_{2}y_{2} + x_{3}y_{3} \neq (x_{1} + x_{2} + x_{3})(y_{1} + y_{2} + y_{3})
\end{align*}

<br><br>

I'll leave it to you to use the above numbers to show this holds

---

## Linear Model Estimators

We will spend the rest of the course exploring how to use [Ordinary Least Squares (OLS)]{.hi} to fit a linear model like:

$$
    y_{i} = \beta_{0} + \beta_{1}x_{i} + u_{i},
$$

That is, if we wanted to hypothesize that some random variable $Y$ depends on another random variable $X$ and that there is a [linear relationship between then]{.hi}, $\beta_{0}$ and $\beta_{1}$ are the parameters which describe the nature of that relationship.

Given a sample of $X$ and $Y$, we will derive [unbiased estimators]{.h} for the intercept $\beta_{0}$ and slope $\beta_{1}$.
Those estimators help us [combine observations]{.h} of $X$ and $Y$ to estimate underlying relationships between them. 

---

<!--
## Linear Model Notation {.tiny}

|   Symbol        |             Meaning                    |                    Example                  |
|:---------------:|:--------------------------------------:|:-------------------------------------------:|
|  $\beta_{0}$    | Intercept parameter in a linear model  | $y_{i} = \beta_{0} + \beta_{1}x_{i} + u_{i}$|
|  $\beta_{1}$    | Slope parameter in a linear model      |                    See above                |
|    $y_{i}$      | Dependent variable or outcome variable |                    See above                |
|    $x_{i}$      | Explanatory variable                   |                    See above                |
|    $u_{i}$      | Unobservable term, disturbance, shock  |                    See above                |
|$\hat{\beta}_{0}$| Estimate of the intercept              |$y_{i} = \hat{\beta}_{0} + \hat{\beta}_{1}x_{i} + e_{i}$|
|$\hat{\beta}_{1}$| Estimate of the slope                  |                    See above                |
|    $e_{i}$      | Residuals                              |                    See above                |

--->

## The Linear Regression Model

We can estimate the effect of $X$ on $Y$ by estimating the model:

$$
    y_{i} = \beta_{0} + \beta_{1}x_{i} + u_{i},
$$

- $y_i$ is the _dependent_ variable 

- $x_i$ is the _independent_ variable (continuous)

- $\beta_0$ is the _intercept_ parameter. $E\left[ {y_i | x_i=0} \right] = \beta_0$

- $\beta_1$ is the _slope_ parameter, which under the correct causal setting represents marginal change in $x_i$'s effect on $y_i$. $\frac{\partial y_i}{\partial x_i} = \beta_1$

- $u_i$ is an Error Term including all other (omitted) factors affecting $y_i$.

---

## The Error Term $u_{i}$

$u_{i}$ is quite special

Consider the [data generating process]{.hi .note} of variable $y_{i}$,

- $u_{i}$ captures [all unobserved variables]{.h} that explain variation in $y_{i}$

<br>

Some error will exist in all models, no model is perfect.

- Our aim is to minimize error under a set of constraints

> Error is the price we are willing to accept for a simplified model

---

## The Error Term

[Five]{.hi} items contribute to the existence of the disturbance term:

. . .

[1]{.hi}. [_Omission of independent variables_]{.hii}


. . .

- Our description (model) of the relationship between $Y$ and $X$ is a simplification
- Other variables have been left out (omitted)

---

## The Error Term {data-visibility="uncounted"}

[Five]{.hi} items contribute to the existence of the disturbance term:

[1]{.hi}. Omission of independent variables

[2]{.hi}. [_Aggregation of Variables_]{.hii}

. . .

- Microeconomic relationships are often summarized
- [Ex.]{.ex} Housing prices ($X$) are described by county-level median home value data

---

## The Error Term {data-visibility="uncounted"}

[Five]{.hi} items contribute to the existence of the disturbance term:

[1]{.hi}. Omission of independent variables

[2]{.hi}. Aggregation of Variables

[3]{.hi}. [_Model misspecificiation_]{.hii}

. . .

- Model structure is incorrectly specified
- [Ex.]{.ex} $Y$ depends on the anticipated value of $X$ in the previous period, not $X$

---

## The Error Term {data-visibility="uncounted"}

[Five]{.hi} items contribute to the existence of the disturbance term:

[1]{.hi}. Omission of independent variables

[2]{.hi}. Aggregation of Variables

[3]{.hi}. Model misspecificiation

[4]{.hi}. [_Functional misspecificiation_]{.hii}

. . .

- The functional relationship is specified incorrectly
- True relationship is nonlinear, not linear

---

## The Error Term {data-visibility="uncounted"}

[Five]{.hi} items contribute to the existence of the disturbance term:

[1]{.hi}. Omission of independent variables

[2]{.hi}. Aggregation of Variables

[3]{.hi}. Model misspecificiation

[4]{.hi}. Functional misspecificiation

[5]{.hi}. [_Measurement error_]{.hii}

. . .

- Measurement of the variables in the data is just wrong
- $Y$ or $X$ 

---

## The Error Term {data-visibility="uncounted"}

[Five]{.hi} items contribute to the existence of the disturbance term:

[1]{.hi}. Omission of independent variables

[2]{.hi}. Aggregation of Variables

[3]{.hi}. Model misspecificiation

[4]{.hi}. Functional misspecificiation

[5]{.hi}. Measurement error

---

## Running a Regression Model

Using an estimator with data on $x_{i}$ and $y_{i}$, we can estimate a [fitted regression line]{.h}:

$$
    \hat{y}_{i} = \hat{\beta}_{0} + \hat{\beta}_{1}x_{i}
$$

- $\hat{y}_{i}$ is the [fitted value]{.hi} of $y_{i}$
- $\hat{\beta}_{0}$ is the [estimated intercept]{.hi}
- $\hat{\beta}_{1}$ is the [estimated slope]{.hi}

This procedure produces misses, known as [residuals]{.hi} $y_{i} - \hat{y_{i}}$

Let's look at an example of how this works