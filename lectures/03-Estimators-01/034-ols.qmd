---
name: old
---

## OLS

The [OLS Estimator]{.h} chooses the parameters $\hat{\beta}_{0}$ and $\hat{\beta}_{1}$ that [minimize]{.hi-red} the [Residual Sum of Squares (RSS)]{.hii}

$$
    \min_{\hat{\beta}_{0},\hat{\beta}_{1}} \sum_{i=1}^{n} \hat{u}_{i}^{2}
$$

This is why we call the estimator ordinary [least squares]{.h}

Recall that residuals are given by $y_{i} - \hat{y}_{i}$ and that:

$$
    \hat{y}_{i} = \hat{\beta}_{0} + \hat{\beta}_{1} x_{i}
$$

Then

$$
    u_{i} = y_{i} - \hat{\beta}_{0} + \hat{\beta}_{1} x_{i}
$$

---

## OLS & Calculus {.small}

We can find our choices $\hat{\beta}_{0}$ and $\hat{\beta}_{1}$ to minimize our residuals using calculus

A minimization problem is essentially the same as an optimization problem where we find the point at which our choices have a slope of zero

To begin, let's properly write out our minimization problem:

$$
    \min_{\hat{\beta}_{0}, \hat{\beta}_{1}} \;\; \sum_{i} u_{i}^{2}
$$

$$
    \min_{\hat{\beta}_{0}, \hat{\beta}_{1}} \; \sum_{i} (y_{i} - \hat{y}_{i})^{2}
$$

$$
    \min_{\hat{\beta}_{0}, \hat{\beta}_{1}} \; \sum_{i} (y_{i} - \hat{\beta}_{0} - \hat{\beta}_{1}x_{i}) (y_{i} - \hat{\beta}_{0} - \hat{\beta}_{1}x_{i})
$$

---

## OLS & Calculus

The calculus we'll use is by finding the derivatives of the function with respect to $\hat{\beta}_{0}$ and $\hat{\beta}_{1}$.

It's a lot of algebra but it is simple math, just a lot of it:

::: {.small}
\begin{align*}
    \min_{\hat{\beta}_{0}, \hat{\beta}_{1}} &\; 
    \sum_{i} y_{i}^{2} - \hat{\beta}_{0}y_{i} - \hat{\beta}_{1}x_{i}y_{i} - \hat{\beta}_{0}y_{i} + \hat{\beta}_{0}^{2} + \hat{\beta}_{0}\hat{\beta}_{1}x_{i} - \hat{\beta}_{1}x_{i}y_{i} + \hat{\beta}_{0}\hat{\beta}_{1}x_{i} + \hat{\beta}_{1}^{2}x_{i}^{2} \\
    \min_{\hat{\beta}_{0}, \hat{\beta}_{1}} &\; 
    \sum_{i} y_{i}^{2} - 2 \hat{\beta}_{0}y_{i} + \hat{\beta}_{0}^{2} - 2 \hat{\beta}_{1}x_{i}y_{i} + 2\hat{\beta}_{0}\hat{\beta}_{1}x_{i} + \hat{\beta}_{1}^{2}x_{i}^{2}
\end{align*}
:::

Then, we take partial derivatives over our choices $\hat{\beta}_{0}$ and $\hat{\beta}_{1}$ to figure the best choices.

These are called First Order Conditions (FOCs)

---

## OLS & Calculus

To find our choices, we find the partial derivative and set it equal to 0

For our intercept $\hat{\beta}_{0}$:

::: {.small}
\begin{align*}
    &\dfrac{\partial u_{i}}{\partial \hat{\beta}_{0}} = 0 \\
    \sum_{i} -2y_{i} + &2\hat{\beta}_{0} + 2\hat{\beta}_{1}x_{i} = 0
\end{align*}
:::

For our slope $\hat{\beta}_{1}$:

::: {.small}
\begin{align*}
    &\dfrac{\partial u_{i}}{\partial \hat{\beta}_{1}} = 0 \\
    \sum_{i} -2x_{i}y_{i} + &2\hat{\beta}_{0}x_{i} + 2\hat{\beta}_{1}x_{i}^{2} = 0
\end{align*}
:::

---

## $\hat{\beta}_{0}$ Derivation

$$
    \sum_{i} -2y_{i} + 2\hat{\beta}_{0} + 2\hat{\beta}_{1}x_{i} = 0
$$

Our task is to find solve the above for $\hat{\beta}_{0}$:

::: {.align-center}
{{< video https://www.youtube.com/watch?v=6TgQgqUHasI&list=PLLcoqzdOKX9rq5n44iiYqtv40X3KxlQ0M width="700" height="375" >}}
:::

---

## $\hat{\beta}_{1}$ Derivation

$$
    \sum_{i} -2x_{i}y_{i} + 2\hat{\beta}_{0}x_{i} + 2\hat{\beta}_{1}x_{i}^{2} = 0
$$

Our task is to find solve the above for $\hat{\beta}_{1}$:

::: {.align-center}
{{< video https://www.youtube.com/watch?v=jJbFjyRVxMY&list=PLLcoqzdOKX9rq5n44iiYqtv40X3KxlQ0M&index=2 width="700" height="375" >}}
:::

---

## OLS Formulas

<br>

[Intercept]{.hi .align-center}

$$
    \hat{\beta}_{0} = \bar{y} - \hat{\beta}_{1}\bar{x}
$$

<br>

[Slope Coefficient]{.hi .align-center}

$$
    \hat{\beta}_{1} = 
    \dfrac{
        \sum_{i=1}^{n} (y_{i} - \bar{y})(x_{i} - \bar{x})
    }{
        \sum_{i=1}^{n} (x_{i} - \bar{x})^{2}
    }
$$

These may look slightly different to my derivation. 
Part of your assignments is to bridge the gap. 