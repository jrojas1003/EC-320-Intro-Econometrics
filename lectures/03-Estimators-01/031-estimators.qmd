---
name: estimators
---

## Estimators

Let's define some concepts first:

[Estimand]{.h}

> Quantity that is to be estimated in a statistical analysis

[Estimator]{.h}

> A rule (or formula) for estimating an unknown population parameter given a sample of data

[Estimate]{.h}

> A specific numerical value that we obtain from the smaple data by applying the estimator

---

## Estimators Example

Suppose we want to know the average height of the population in the US 

- We have a [sample]{.hi} of 1 million Americans

So then we can identify our [Estimand]{.h}, [Estimator]{.h}, and [Estimate]{.h}

- [Estimand]{.h}: The population mean $(\mu)$

- [Estimator]{.h}: The sample mean $(\bar{X})$

$$
    \bar{X} = \dfrac{1}{n} \sum_{i=1}^{n} X_{i}
$$

- [Estimate]{.h}: The sample mean $(\hat{\mu} = 5'6'')$

---

## Properties of Estimators

There are many ways to estimate things and they all have their benefits and costs.

Imagine we want to estimate an unknown parameter $\mu$, and we know the distributions of three competing estimators.

[Which one should we use?]{.hii .align-center}

```{R}
#| label: "competing pdfs"
#| echo : FALSE
#| fig.height: 4.5
#| fig-align: center
#| message: FALSE
#| warning: FALSE

library(pacman)
p_load(dplyr, ggplot2)

# Generate data for densities' polygons
d1 <- tibble(
  x = seq(-7.5, 7.5, 0.01),
  y = dnorm(x, mean = 1, sd = 1)
  ) %>%
  rbind(.,
    tibble(x = seq(7.5, -7.5, -0.01), y = 0)
    )

d2 <- tibble(
  x = seq(-7.5, 7.5, 0.01),
  y = dunif(x, min = -2.5, max = 1.5)
  ) %>%
  rbind(.,
    tibble(x = seq(7.5, -7.5, -0.01), y = 0)
    )

d3 <- tibble(
  x = seq(-7.5, 7.5, 0.01),
  y = dnorm(x, mean = 0, sd = 2.5)
  ) %>%
  rbind(.,
    tibble(x = seq(7.5, -7.5, -0.01), y = 0)
    )

# Plot them
ggplot() +
  geom_polygon(data = d1, aes(x, y), size = 0.25, alpha = 0.6, color = 'white', fill = "#4EACB1") +
  geom_polygon(data = d2, aes(x, y), size = 0.25, alpha = 0.6, color = 'white', fill = "#B14EAC") +
  geom_polygon(data = d3, aes(x, y), size = 0.25, alpha = 0.6, color = 'white', fill = "#ACB14E") +
  geom_hline(yintercept = 0, color = "black") +
  geom_vline(xintercept = 0, size = 1, linetype = "dashed") +
  scale_x_continuous(breaks = 0) +
  scale_y_continuous(expand = c(0, 0), limits = c(0, NA)) +
  theme_void()
```

---

## Properties of Estimators - Unbiasedness

We ask: What [properties]{.hi} make an estimator [reliable]{.h}?

Answer (1): [Unbiasedness]{.h}

On average, does the estimator tend toward the correct value?

<br>

[Formally]{.note}: Does the mean of the estimator's distribution equal the parameter it estimates?

$$
    \text{Bias}_{\mu} (\hat{\mu}) = E[\hat{\mu}] - \mu 
$$

---

## Properties of estimators

[Question]{.note} What properties make an estimator reliable?

[A01]{.note}: [Unbiasedness]{.hi}

:::: {.columns}

::: {.column width="50%"}

[Unbiased estimator]{.hi}: $E\left[ \hat{\mu} \right] = \mu$

```{r}
#| label: "unbiased pdf"
#| echo: FALSE

tmp <- tibble(x = seq(-4, 4, 0.01), y = dnorm(x))
tmp <- rbind(tmp, tibble(x = seq(4, -4, -0.01), y = 0))

ggplot(data = tmp, aes(x, y)) +
  geom_polygon(fill = "violet", alpha = 0.49) +
  geom_hline(yintercept = 0, color = "black") +
  geom_vline(xintercept = 0, size = 1, linetype = "dashed") +
  labs(
    x = "", y = ""
  ) +
  scale_x_continuous(breaks = 0, limits = c(-6,6)) +
  scale_y_continuous(expand = c(0, 0), limits = c(0, NA))+
  theme_void()
```

:::

::: {.column width="50%"}

[Biased estimator]{.hi} $E\left[ \hat{\mu} \right] \neq \mu$

```{r} 
#| label: "biased pdf"
#| echo: FALSE

tmp <- tibble(x = seq(-4, 4, 0.01), y = dnorm(x))
tmp <- rbind(tmp, tibble(x = seq(4, -4, -0.01), y = 0))

ggplot(data = tmp, aes(x, y)) +
  geom_polygon(aes(x = x + 2), fill = "lightblue", alpha = 0.49) +
  geom_hline(yintercept = 0, color = "black") +
  geom_vline(xintercept = 0, size = 1, linetype = "dashed") +
  labs(
    x = "", y = ""
  ) +
  scale_x_continuous(breaks = 0, limits = c(-6,6)) +
  scale_y_continuous(expand = c(0, 0), limits = c(0, NA))+
  theme_void()
```

:::

::::

---

## Properties of Estimators - Efficiency

We ask: What [properties]{.hi} make an estimator [reliable]{.h}?

Answer (1): [Efficiency]{.h} (Low Variance)

The central tendencies (means) of competing distribution are not the only things that matter. 
We also care about the [variance]{.h} of an estimator.

$$
    Var(\hat{\mu}) = E \left[ (\hat{\mu} - E[\hat{\mu}])^{2} \right]
$$

[Lower variance]{.hi} estimators estimate closer to the mean in each sample

---

## Properties of Estimators - Efficiency

Imagine low variance to be similar to accuracy $\rightarrow$ tighter estimates

```{r}
#| label: "variance pdf"
#| echo: FALSE
#| fig.height: 4.5

d4 <- tibble(
  x = seq(-7.5, 7.5, 0.01),
  y = dnorm(x, mean = 0, sd = 1)
  ) %>%
  rbind(.,
    tibble(
      x = seq(7.5, -7.5, -0.01),
      y = 0
    )
  )

d5 <- tibble(
  x = seq(-7.5, 7.5, 0.01),
  y = dnorm(x, mean = 0, sd = 2)
  ) %>%
  rbind(.,
    tibble(
      x = seq(7.5, -7.5, -0.01),
      y = 0
    )
  )

ggplot() +
  geom_polygon(data = d4, aes(x, y), fill = "#4383BC", alpha = 0.49) +
  geom_polygon(data = d5, aes(x, y), fill = "#BC7C43", alpha = 0.48) +
  geom_hline(yintercept = 0, color = "black") +
  geom_vline(xintercept = 0, size = 1, linetype = "dashed") +
  scale_x_continuous(breaks = 0) +
  scale_y_continuous(expand = c(0, 0), limits = c(0, NA))+
  theme_void()
```

---

## The Bias-Variance Tradeoff

Much like everything, there are tradeoffs from gaining one thing over another.

Should we be willing to take a bit of bias to reduce the variance?

In economics/causal inference, we emphasize unbiasedness

```{r}
#| label: "variance bias"
#| echo: FALSE
#| fig.height: 4.5
#| fig-align: center

d4 <- tibble(
  x = seq(-7.5, 7.5, 0.01),
  y = dnorm(x, mean = 0.3, sd = 1)
  ) %>%
  rbind(.,
   tibble(
    x = seq(7.5, -7.5, -0.01),
    y = 0
    )
  )

d5 <- tibble(
  x = seq(-7.5, 7.5, 0.01),
  y = dnorm(x, mean = 0, sd = 2)
  ) %>%
  rbind(.,
   tibble(
    x = seq(7.5, -7.5, -0.01),
    y = 0
    )
  )

ggplot() +
geom_polygon(data = d4, aes(x, y), fill = "#4383BC", alpha = 0.49) +
geom_polygon(data = d5, aes(x, y), fill = "#BC7C43", alpha = 0.48) +
geom_hline(yintercept = 0, color = "black") +
geom_vline(xintercept = 0, size = 1, linetype = "dashed") +
scale_x_continuous(breaks = 0) +
scale_y_continuous(expand = c(0, 0), limits = c(0, NA))+
theme_void()
```

---

## Unbiased estimators

In addition to the sample mean, there are other unbiased estimators we will often use

<br>

- [Sample variance]{.hi} estimates the variance $\sigma^{2}$

<br>

- [Sample covariance]{.hi} setimates the covariance $\sigma_{XY}$

<br>

- [Sample correlation]{.hi} estimates the pop. correlation coefficient $\rho_{XY}$

---

## Sample Variance

The sample variance, $S_{X}^{2}$, is an unbiased estimator of the population variance

<br>

$$
    S_{X}^{2} = \dfrac{1}{n - 1} \sum_{i=1}^{n} (X_{i} - \bar{X})^{2}.
$$

---

## Sample Covariance

The sample covariance, $S_{XY}$, is an unbiaed estimator of the population covariance

<br>

$$
    S_{XY} = \dfrac{1}{n-1} \sum_{i=1}^{n} (X_{i} - \bar{X})(Y_{i} - \bar{Y}).
$$

---

## Sample Correlation 
 
Sample correlation, $r_{XY}$, is an unbiased estimator of the population correlation coefficient

<br>

$$
    r_{XY} = \dfrac{S_{XY}}{\sqrt{S_{X}^{2}}\sqrt{S_{Y}^{2}}}.
$$