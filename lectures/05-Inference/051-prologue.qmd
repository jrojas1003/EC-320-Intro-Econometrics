---
name: prologue
---


## OLS 

Up to now, we have been focusing on OLS considering:

- How we model regressions with this estimator
- How the estimator is derived and what properties it demonstrates
- How the classical assumptions make the estimator [BLUE]{.hii}

. . .

We have [mostly ignored]{.h} drawing conclusions about the true population parameters from the estimates of the sample data

> This is [inference]{.hi-red}

---

## OLS 

Thus far we have fit an OLS model to find an answer to the following questions:

- How much does an additional year of schooling increase earnings?
- Does the number of police officers affect campus crime rates?

. . .

Up to now, we have not discussed our [confidence]{.hi} in our fitted relationship

Even if all [6 Assumptions]{.hi} hold, sample selection might generate the [incorrect conclusions]{.hi-red} in a completely unbiased, [coincidental]{.hi} fashion.

---

[_Previously_]{.note} we used the first [3]{.hi} assumptions to show that OLS is unbiased: 

$$
\mathop{\mathbb{E}}\left[ \hat{\beta} \right] = \beta
$$

<br>

We used the first [5]{.hi} assumptions to derive a formula for the [_variance_]{.note} of the OLS estimator: 

$$
\mathop{\text{Var}}(\hat{\beta}) = \frac{\sigma^2}{\sum_{i=1}^n (X_i - \bar{X})^2}
$$

. . .

By using the [_variance_]{.note} of the OLS estimator, we can infer confidence from the [sampling distribution]{.hi}

---

## Sampling distribution

> The probability distribution of the OLS estimators obtained from repeatedly drawing random samples of the same size from a population and fitting [_point estimates_]{.note} each time.

Provides information about their variability, accuracy, and precision across different samples.

. . .

<br>

[_Point estimates_]{.note}

> The fitted values of the OLS estimator (e.g., $\hat{\beta}_0, \hat{\beta}_1$)

---

## Sampling distribution properties

[1.]{.note} [Unbiasedness:]{.hi} If the Gauss-Markov assumptions hold, the OLS estimators are unbiased (i.e., $E(\hat{\beta}_0) = \beta_0\) and \(E(\hat{\beta}_1) = \beta_1$)

. . .

[2.]{.note} [Variance:]{.hi} The variance of the OLS estimators describes their dispersion around the true population parameters.

. . .

[3.]{.note} [Normality:]{.hi} If the errors are normally distributed or the sample size is large enough, by the [Central Limit Theorem]{.hi}, the sampling distribution of the OLS estimators will be approximately normal.^[Useful for making inferences, constructing confidence intervals, and performing hypothesis tests using the t-distribution.]

---

## Sampling distribution

The sampling distribution of $\hat{\beta}$ to conduct hypothesis tests.

Use all [6]{.hi} classical assumptions to show that OLS is [normally distributed]{.note}:

$$
\hat{\beta} \sim \mathop{N}\left( \beta, \frac{\sigma^2}{\sum_{i=1}^n (X_i - \bar{X})^2} \right)
$$

. . .

Let's look at a simulation

---

## 

![](images/simulation-data.png){fig-align="center"}

---

Plotting the distributions of the [point estimates]{.note} in a histogram

<br>
<br>

![](images/sim-1000.png){fig-align="center"}

Simulating 1,000 draws

---

Plotting the distributions of the [point estimates]{.note} in a histogram

<br>
<br>

![](images/sim-10000.png){fig-align="center"}

Simulating 10,000 draws